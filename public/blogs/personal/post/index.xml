<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Max Woolf's Blog</title><link>https://minimaxir.com/post/</link><atom:link href="https://minimaxir.com/post/index.xml" rel="self" type="application/rss+xml"/><description>Posts</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Copyright Max Woolf Â© 2022</copyright><lastBuildDate>Mon, 19 Sep 2022 08:45:00 -0700</lastBuildDate><image><url>https://minimaxir.com/media/icon_hu8e97f1fc1d6468da1325468be42ca7f8_10286_512x512_fill_gaussian_center_3.png</url><title>Posts</title><link>https://minimaxir.com/post/</link></image><item><title>I Resurrected "Ugly Sonic" with Stable Diffusion Textual Inversion</title><link>https://minimaxir.com/2022/09/stable-diffusion-ugly-sonic/</link><pubDate>Mon, 19 Sep 2022 08:45:00 -0700</pubDate><guid>https://minimaxir.com/2022/09/stable-diffusion-ugly-sonic/</guid><description>&lt;p>So there&amp;rsquo;s a new popular AI image generation tool named &lt;a href="https://github.com/CompVis/stable-diffusion" target="_blank" rel="noopener">Stable Diffusion&lt;/a>. But first, let&amp;rsquo;s discuss why you really clicked on the link to this article: &lt;a href="https://knowyourmeme.com/memes/ugly-sonic" target="_blank" rel="noopener">Ugly Sonic&lt;/a>.&lt;/p>
&lt;figure id="figure-hes-sonic-but-ugly-via-paramount-pictures">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="He&amp;#39;s Sonic, but Ugly. via Paramount Pictures" srcset="
/2022/09/stable-diffusion-ugly-sonic/ugly_sonic_hu89d926edd03c5878f62fcf553068403b_720336_deb8b72e8833663e7c3d0579cc2dcfc7.png 400w,
/2022/09/stable-diffusion-ugly-sonic/ugly_sonic_hu89d926edd03c5878f62fcf553068403b_720336_e969418e91092f9eee702de55086dda2.png 760w,
/2022/09/stable-diffusion-ugly-sonic/ugly_sonic_hu89d926edd03c5878f62fcf553068403b_720336_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/09/stable-diffusion-ugly-sonic/ugly_sonic_hu89d926edd03c5878f62fcf553068403b_720336_deb8b72e8833663e7c3d0579cc2dcfc7.png"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
He&amp;rsquo;s Sonic, but Ugly. via Paramount Pictures
&lt;/figcaption>&lt;/figure>
&lt;p>A short background: &lt;a href="https://www.sonicthehedgehog.com" target="_blank" rel="noopener">Sonic the Hedgehog&lt;/a> is one of the most iconic video game characters of all time.&lt;/p>
&lt;figure id="figure-key-art-of-modern-sonic-from-super-smash-brothers-ultimate-via-nintendohttpswwwsmashbroscomen_usfighter38html">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Key art of &amp;#34;Modern&amp;#34; Sonic from Super Smash Brothers Ultimate. [via Nintendo](https://www.smashbros.com/en_US/fighter/38.html)" srcset="
/2022/09/stable-diffusion-ugly-sonic/Sonic_-_Super_Smash_Bros._Ultimate%20copy_hue5431530e04e38f7c819acceb01cdf11_418029_fe2a6308dd27b602e60dbf24a2843f15.png 400w,
/2022/09/stable-diffusion-ugly-sonic/Sonic_-_Super_Smash_Bros._Ultimate%20copy_hue5431530e04e38f7c819acceb01cdf11_418029_e8f8ebc8b1932110ee9956b22dd9a472.png 760w,
/2022/09/stable-diffusion-ugly-sonic/Sonic_-_Super_Smash_Bros._Ultimate%20copy_hue5431530e04e38f7c819acceb01cdf11_418029_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/09/stable-diffusion-ugly-sonic/Sonic_-_Super_Smash_Bros._Ultimate%20copy_hue5431530e04e38f7c819acceb01cdf11_418029_fe2a6308dd27b602e60dbf24a2843f15.png"
width="600"
height="743"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Key art of &amp;ldquo;Modern&amp;rdquo; Sonic from Super Smash Brothers Ultimate. &lt;a href="https://www.smashbros.com/en_US/fighter/38.html" target="_blank" rel="noopener">via Nintendo&lt;/a>
&lt;/figcaption>&lt;/figure>
&lt;p>The initial movie trailer released in 2019 for the Sonic the Hedgehog movie included a peculiar &lt;em>general-audience-friendly&lt;/em> design for Sonic.&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/4mW9FE5ILJs" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>This was a more humanoid Sonic, with small eyes, blue furry arms, and &lt;em>human teeth&lt;/em>. After backlash, Sonic was redesign to be closer to his modern game incarnation:&lt;/p>
&lt;figure id="figure-comparison-between-the-two-designs-via-newsweekhttpswwwnewsweekcomsonic-hedgehog-redesign-movie-conspiracy-old-vs-new-comparison-1471620">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Comparison between the two designs. [via Newsweek](https://www.newsweek.com/sonic-hedgehog-redesign-movie-conspiracy-old-vs-new-comparison-1471620)" srcset="
/2022/09/stable-diffusion-ugly-sonic/new-vs-old-sonic-hedgehog_hu07a07131e2baa41544ae7decb667647b_24100_5ed841305a07a70ce55d8d8f52df08cc.webp 400w,
/2022/09/stable-diffusion-ugly-sonic/new-vs-old-sonic-hedgehog_hu07a07131e2baa41544ae7decb667647b_24100_92656c8de3d3f9054795dd076b8a80e4.webp 760w,
/2022/09/stable-diffusion-ugly-sonic/new-vs-old-sonic-hedgehog_hu07a07131e2baa41544ae7decb667647b_24100_1200x1200_fit_q75_h2_gaussian_2.webp 1200w"
src="https://minimaxir.com/2022/09/stable-diffusion-ugly-sonic/new-vs-old-sonic-hedgehog_hu07a07131e2baa41544ae7decb667647b_24100_5ed841305a07a70ce55d8d8f52df08cc.webp"
width="760"
height="427"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Comparison between the two designs. &lt;a href="https://www.newsweek.com/sonic-hedgehog-redesign-movie-conspiracy-old-vs-new-comparison-1471620" target="_blank" rel="noopener">via Newsweek&lt;/a>
&lt;/figcaption>&lt;/figure>
&lt;p>The movie itself turned out to be the best video-game movie ever, which sounds &lt;a href="https://tvtropes.org/pmwiki/pmwiki.php/Main/DamnedByFaintPraise" target="_blank" rel="noopener">damning by faint praise&lt;/a> but it was an accurate assessment. Years later, a gag in the straight-to-Disney+ movie &lt;a href="https://www.imdb.com/title/tt3513500/" target="_blank" rel="noopener">Chip N&amp;rsquo; Dale: Rescue Rangers&lt;/a> reintroduced this design as a gag, officially called Ugly Sonic.&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/uZzl3Y1HDAQ" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>So why not see if AI can resurrect this Ugly Sonic? (that&amp;rsquo;s a rhetorical question, please don&amp;rsquo;t answer it)&lt;/p>
&lt;p>I decided to use Ugly Sonic to test Stable Diffusion for three reasons: one, because he&amp;rsquo;s a computer-generated character so it seems thematically appropriate; two, because there aren&amp;rsquo;t many images of him in the training dataset so generated output should be truly unique; and three, because if Paramount wants to send me a cease and desist for besmirching the the Ugly Sonic brand, that would be objectively hilarious.&lt;/p>
&lt;h2 id="stable-diffusion-is-a-crazy-gadget">Stable Diffusion is a Crazy Gadget&lt;/h2>
&lt;div class="alert alert-">
&lt;div>
All images generated by Stable Diffusion v1.4 in this post are generated with a classifier guidance of 7.5 with 50 denoising steps. Images are cherrypicked from 16 total generations from the prompt, as occasionally the prompt is misinterpreted by Stable Diffusion, or the generations aren&amp;rsquo;t funny enough. Additionally, the NSFW filter was disabled during generation due to frequent false positives: none of the images used in this post are NSFW, although some may argue that Ugly Sonic himself is NSFL.
&lt;/div>
&lt;/div>
&lt;p>I&amp;rsquo;ve always had difficulty generating a normal Sonic the Hedgehog image with AI image generation. &lt;a href="https://openai.com/dall-e-2/" target="_blank" rel="noopener">DALL-E 2&lt;/a>, for example, just flat-out can&amp;rsquo;t do it.&lt;/p>
&lt;figure id="figure-a-portrait-of-sonic-the-hedgehog-via-dall-e-2">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`a portrait of Sonic the Hedgehog`, via DALL-E 2" srcset="
/2022/09/stable-diffusion-ugly-sonic/a071601b4209bcd2_hu33b829fd75505c91bcbc60b925fae7fc_1073451_0c59908b823f98d321421112470c21e1.png 400w,
/2022/09/stable-diffusion-ugly-sonic/a071601b4209bcd2_hu33b829fd75505c91bcbc60b925fae7fc_1073451_ab8b7be4b1cf5244bf036c37ab19ac63.png 760w,
/2022/09/stable-diffusion-ugly-sonic/a071601b4209bcd2_hu33b829fd75505c91bcbc60b925fae7fc_1073451_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/09/stable-diffusion-ugly-sonic/a071601b4209bcd2_hu33b829fd75505c91bcbc60b925fae7fc_1073451_0c59908b823f98d321421112470c21e1.png"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>a portrait of Sonic the Hedgehog&lt;/code>, via DALL-E 2
&lt;/figcaption>&lt;/figure>
&lt;p>Stable Diffusion does a tad better, capturing Sonic with a variety of styles and eras.&lt;/p>
&lt;figure id="figure-a-portrait-of-sonic-the-hedgehog-via-stable-diffusion">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`a portrait of Sonic the Hedgehog`, via Stable Diffusion" srcset="
/2022/09/stable-diffusion-ugly-sonic/d3bc427a63dad734_hud2e13215184b505334e5b0683eb02a49_923356_15dd9863f9a18fe1b9e23ed43f8c3b73.png 400w,
/2022/09/stable-diffusion-ugly-sonic/d3bc427a63dad734_hud2e13215184b505334e5b0683eb02a49_923356_6c9c314bdca65304410a0bcb875d3069.png 760w,
/2022/09/stable-diffusion-ugly-sonic/d3bc427a63dad734_hud2e13215184b505334e5b0683eb02a49_923356_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/09/stable-diffusion-ugly-sonic/d3bc427a63dad734_hud2e13215184b505334e5b0683eb02a49_923356_15dd9863f9a18fe1b9e23ed43f8c3b73.png"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>a portrait of Sonic the Hedgehog&lt;/code>, via Stable Diffusion
&lt;/figcaption>&lt;/figure>
&lt;p>Indeed, there are &lt;a href="https://haveibeentrained.com/?search_text=sonic%20the%20hedgehog" target="_blank" rel="noopener">many images of Sonic&lt;/a> in the training dataset, however the generated images do not verbatim reproduce or otherwise plagiarize results from the training set above (I checked each one).&lt;/p>
&lt;p>By now, you probably already know that Stable Diffusion takes in text and generates an image from random latent noise. The text encoding is done through a large pretrained CLIP model. However, a new technique called &lt;a href="https://textual-inversion.github.io" target="_blank" rel="noopener">textual inversion&lt;/a> can reverse engineer the 768D &amp;ldquo;encoding&amp;rdquo; of a concept with the CLIP encoding space given a few example images and without modifying the underlying image generation model, which can then be used with the model to generate more specific images.&lt;/p>
&lt;figure id="figure-demo-of-textual-inversion-via-the-official-project-repohttpstextual-inversiongithubio">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Demo of textual inversion, via [the official project repo](https://textual-inversion.github.io)" srcset="
/2022/09/stable-diffusion-ugly-sonic/teaser_huc481276552365ce4ad34e266c5e05428_168778_25b13e1d93f671b01b88fd150b73f074.jpg 400w,
/2022/09/stable-diffusion-ugly-sonic/teaser_huc481276552365ce4ad34e266c5e05428_168778_bb795c8c7af0f31fdb90237d4d1d3a02.jpg 760w,
/2022/09/stable-diffusion-ugly-sonic/teaser_huc481276552365ce4ad34e266c5e05428_168778_1200x1200_fit_q75_gaussian.jpg 1200w"
src="https://minimaxir.com/2022/09/stable-diffusion-ugly-sonic/teaser_huc481276552365ce4ad34e266c5e05428_168778_25b13e1d93f671b01b88fd150b73f074.jpg"
width="760"
height="358"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Demo of textual inversion, via &lt;a href="https://textual-inversion.github.io" target="_blank" rel="noopener">the official project repo&lt;/a>
&lt;/figcaption>&lt;/figure>
&lt;p>Soon after, Hugging Face &lt;a href="https://twitter.com/psuraj28/status/1567212122970685442" target="_blank" rel="noopener">released a Colab Notebook&lt;/a> that makes training the model to obtain the concept straightforward. From that, I trained an &lt;a href="https://huggingface.co/sd-concepts-library/ugly-sonic" target="_blank" rel="noopener">Ugly Sonic object concept&lt;/a> on 5 image crops from the movie trailer, with 6,000 steps and 1 gradient accumulation step (on a T4 GPU, this took about 1.5 hours and cost about $0.21 on a GCP Spot instance). I then &lt;a href="https://colab.research.google.com/drive/1-Go3l9HpSIkjvDfR0gm8kWLPRnsaUIYd?usp=sharing" target="_blank" rel="noopener">customized the inference Colab notebook&lt;/a> to more easily generate images from a new textual inversion.&lt;/p>
&lt;p>The Ugly Sonic object concept, once loaded into the text encoder, can be invoked by including &lt;code>&amp;lt;ugly-sonic&amp;gt;&lt;/code> in the prompt where you&amp;rsquo;d normally include an object. Let&amp;rsquo;s test it out with a simple &lt;a href="https://minimaxir.com/2021/08/vqgan-clip/" target="_blank" rel="noopener">VQGAN + CLIP-esque&lt;/a> prompt such as &lt;code>a beautiful portrait of &amp;lt;ugly-sonic&amp;gt; by Leonardo Da Vinci&lt;/code> which should have a more expected output:&lt;/p>
&lt;figure id="figure-a-beautiful-portrait-of--by-leonardo-da-vinci-via-stable-diffusion">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`a beautiful portrait of by Leonardo Da Vinci`, via Stable Diffusion" srcset="
/2022/09/stable-diffusion-ugly-sonic/8ed7ee0d3e25a187_hu6cf38f6b2853fde87c418a49bda66382_1087050_a84773be898d7f181b3df052c3501ef1.png 400w,
/2022/09/stable-diffusion-ugly-sonic/8ed7ee0d3e25a187_hu6cf38f6b2853fde87c418a49bda66382_1087050_b1b07dea9a284d7d06a23801ac21ab48.png 760w,
/2022/09/stable-diffusion-ugly-sonic/8ed7ee0d3e25a187_hu6cf38f6b2853fde87c418a49bda66382_1087050_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/09/stable-diffusion-ugly-sonic/8ed7ee0d3e25a187_hu6cf38f6b2853fde87c418a49bda66382_1087050_a84773be898d7f181b3df052c3501ef1.png"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>a beautiful portrait of &amp;lt;ugly-sonic&amp;gt; by Leonardo Da Vinci&lt;/code>, via Stable Diffusion
&lt;/figcaption>&lt;/figure>
&lt;p>ðŸ˜µâ€ðŸ’«&lt;/p>
&lt;p>Apparently the textual inversion tokens can have an unexpectedly strong effect on the resulting output. Fortunately, there&amp;rsquo;s a Stable Diffusion prompt hacking trick I &lt;a href="https://www.reddit.com/r/StableDiffusion/comments/xd1ze4/increases_attention_to_enclosed_words_decreases/" target="_blank" rel="noopener">saw on Reddit&lt;/a>: wrapping terms you want to emphasize with &lt;code>()&lt;/code> increases their &amp;ldquo;weight&amp;rdquo; in the generation, while &lt;code>[]&lt;/code> decreases the weight. Modifying the prompt to also include deemphasis on Ugly Sonic and emphasis on the medium of &lt;code>painting, oil on canvas&lt;/code> gives better results.&lt;/p>
&lt;figure id="figure-a-beautiful-portrait-of--by-leonardo-da-vinci-painting-oil-on-canvas-via-stable-diffusion">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`a beautiful portrait of [[[]]] by Leonardo Da Vinci, (((painting, oil on canvas)))`, via Stable Diffusion" srcset="
/2022/09/stable-diffusion-ugly-sonic/e04ddaa8da5edbf5_hu2eed26573be14b9ceb8c55e11db4f88e_1127984_65d6a9fbeeeecaaf6892f20003363d41.png 400w,
/2022/09/stable-diffusion-ugly-sonic/e04ddaa8da5edbf5_hu2eed26573be14b9ceb8c55e11db4f88e_1127984_5284d44088c05e208d8404fdb155bfff.png 760w,
/2022/09/stable-diffusion-ugly-sonic/e04ddaa8da5edbf5_hu2eed26573be14b9ceb8c55e11db4f88e_1127984_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/09/stable-diffusion-ugly-sonic/e04ddaa8da5edbf5_hu2eed26573be14b9ceb8c55e11db4f88e_1127984_65d6a9fbeeeecaaf6892f20003363d41.png"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>a beautiful portrait of [[[&amp;lt;ugly-sonic&amp;gt;]]] by Leonardo Da Vinci, (((painting, oil on canvas)))&lt;/code>, via Stable Diffusion
&lt;/figcaption>&lt;/figure>
&lt;p>Close enough!&lt;/p>
&lt;p>There is a lot of trial and error, but fortunately Stable Diffusion generation is fast enough and cheap enough that you can brute force it. And this is just the beginning.&lt;/p>
&lt;h2 id="mad-latent-space">Mad Latent Space&lt;/h2>
&lt;p>Now that we have a working Ugly Sonic inversion, let&amp;rsquo;s get dangerous. The standard modifiers added to AI-generate image prompts work here to increase realism.&lt;/p>
&lt;figure id="figure-hyperrealistic--unreal-engine-4k-via-stable-diffusion">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`hyperrealistic , unreal engine, 4k`, via Stable Diffusion" srcset="
/2022/09/stable-diffusion-ugly-sonic/eae2c80e866d45b5_hu3abe668baa93671f4539db66d41884dc_955192_d5190b36259f044995a6235c528b1758.png 400w,
/2022/09/stable-diffusion-ugly-sonic/eae2c80e866d45b5_hu3abe668baa93671f4539db66d41884dc_955192_6ccd32b5fac149fd74308489a8c6d990.png 760w,
/2022/09/stable-diffusion-ugly-sonic/eae2c80e866d45b5_hu3abe668baa93671f4539db66d41884dc_955192_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/09/stable-diffusion-ugly-sonic/eae2c80e866d45b5_hu3abe668baa93671f4539db66d41884dc_955192_d5190b36259f044995a6235c528b1758.png"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>hyperrealistic &amp;lt;ugly-sonic&amp;gt;, unreal engine, 4k&lt;/code>, via Stable Diffusion
&lt;/figcaption>&lt;/figure>
&lt;p>Ugly Sonic is better rendered here than in the movie trailer.&lt;/p>
&lt;p>It&amp;rsquo;s noticeable here, but in some cases the generated figure is closer to Modern Sonic than Ugly Sonic. It&amp;rsquo;s possible the trained concept and the encoded &lt;code>Sonic the Hedgehog&lt;/code> text are similarly embedded in the latent space. Hence we need to curate the generated images so we try not to include the &lt;em>boring&lt;/em> Modern Sonic that no one likes.&lt;/p>
&lt;p>Ugly Sonic must be hungry, let&amp;rsquo;s get him his favorite food: a chili dog.&lt;/p>
&lt;figure id="figure--sitting-and-eating-a-chili-dog-stock-photo-via-stable-diffusion">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="` sitting and eating a ((chili dog)), stock photo`, via Stable Diffusion" srcset="
/2022/09/stable-diffusion-ugly-sonic/d8bc24adb9f679fe_hue067378eb998cd15d1a0cb43ae3caecb_967979_4f3154b5a89dd6a901cf80638457f686.png 400w,
/2022/09/stable-diffusion-ugly-sonic/d8bc24adb9f679fe_hue067378eb998cd15d1a0cb43ae3caecb_967979_45c6a677e7e909672c789b2fa3b21e33.png 760w,
/2022/09/stable-diffusion-ugly-sonic/d8bc24adb9f679fe_hue067378eb998cd15d1a0cb43ae3caecb_967979_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/09/stable-diffusion-ugly-sonic/d8bc24adb9f679fe_hue067378eb998cd15d1a0cb43ae3caecb_967979_4f3154b5a89dd6a901cf80638457f686.png"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>&amp;lt;ugly-sonic&amp;gt; sitting and eating a ((chili dog)), stock photo&lt;/code>, via Stable Diffusion
&lt;/figcaption>&lt;/figure>
&lt;p>Now that he&amp;rsquo;s had lunch, Ugly Sonic can now spend time with the former president of the United States, Barack Obama!&lt;/p>
&lt;figure id="figure-hyperrealistic--shakes-hands-with-barack-obama-via-stable-diffusion">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`hyperrealistic shakes hands with Barack Obama`, via Stable Diffusion" srcset="
/2022/09/stable-diffusion-ugly-sonic/59aec00fb3f1e797_hu731981668ce30b6bfb54d1bdec7a1085_1030833_10874042e4d573db4fa293498e3c0def.png 400w,
/2022/09/stable-diffusion-ugly-sonic/59aec00fb3f1e797_hu731981668ce30b6bfb54d1bdec7a1085_1030833_9223036be2ac3ed20199513c1d27cda0.png 760w,
/2022/09/stable-diffusion-ugly-sonic/59aec00fb3f1e797_hu731981668ce30b6bfb54d1bdec7a1085_1030833_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/09/stable-diffusion-ugly-sonic/59aec00fb3f1e797_hu731981668ce30b6bfb54d1bdec7a1085_1030833_10874042e4d573db4fa293498e3c0def.png"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>hyperrealistic &amp;lt;ugly-sonic&amp;gt; shakes hands with Barack Obama&lt;/code>, via Stable Diffusion
&lt;/figcaption>&lt;/figure>
&lt;p>Let&amp;rsquo;s go full circle and put Ugly Sonic back into a video game!&lt;/p>
&lt;figure id="figure--as-a-character-in-a-genesis-video-game-16-bit-pixel-art-via-stable-diffusion">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`[[[[]]]] as a character in a ((Genesis)) video game, ((((16-bit pixel art))))`, via Stable Diffusion" srcset="
/2022/09/stable-diffusion-ugly-sonic/d2dbb4ada2fc87cc_hueee669375451135ce1e05b1e1f6e22e9_1075915_83f60089357fdc40f12ae7eed1c72f34.png 400w,
/2022/09/stable-diffusion-ugly-sonic/d2dbb4ada2fc87cc_hueee669375451135ce1e05b1e1f6e22e9_1075915_d3665a534808419acba5396c7c538ed1.png 760w,
/2022/09/stable-diffusion-ugly-sonic/d2dbb4ada2fc87cc_hueee669375451135ce1e05b1e1f6e22e9_1075915_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/09/stable-diffusion-ugly-sonic/d2dbb4ada2fc87cc_hueee669375451135ce1e05b1e1f6e22e9_1075915_83f60089357fdc40f12ae7eed1c72f34.png"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>[[[[&amp;lt;ugly-sonic&amp;gt;]]]] as a character in a ((Genesis)) video game, ((((16-bit pixel art))))&lt;/code>, via Stable Diffusion
&lt;/figcaption>&lt;/figure>
&lt;p>It&amp;rsquo;s indeed possible to use more than one textual inversion at a time in a prompt, and the &lt;a href="https://huggingface.co/sd-concepts-library" target="_blank" rel="noopener">Concepts gallery&lt;/a> is a good repository of trained concepts. What about giving Ugly Sonic a psychedelic aspect by combining a &lt;a href="https://huggingface.co/sd-concepts-library/liquid-light" target="_blank" rel="noopener">liquid light style concept&lt;/a> and a &lt;a href="https://huggingface.co/sd-concepts-library/nebula" target="_blank" rel="noopener">nebula style concept&lt;/a>?&lt;/p>
&lt;figure id="figure-a-hyperrealistic-portrait-of--in-the-style-of--and-the-style-of--trending-on-artstation-via-stable-diffusion">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`a ((((hyperrealistic portrait)))) of [] in the style of and the style of , trending on artstation`, via Stable Diffusion" srcset="
/2022/09/stable-diffusion-ugly-sonic/d8bf6343c03dde31_hubd35abc600b88a9640c493ba1f715fb0_1151591_15cdde0aa0240304481ce877732478b1.png 400w,
/2022/09/stable-diffusion-ugly-sonic/d8bf6343c03dde31_hubd35abc600b88a9640c493ba1f715fb0_1151591_2ae3a6d5f6971bf5d4098914e3021377.png 760w,
/2022/09/stable-diffusion-ugly-sonic/d8bf6343c03dde31_hubd35abc600b88a9640c493ba1f715fb0_1151591_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/09/stable-diffusion-ugly-sonic/d8bf6343c03dde31_hubd35abc600b88a9640c493ba1f715fb0_1151591_15cdde0aa0240304481ce877732478b1.png"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>a ((((hyperrealistic portrait)))) of [&amp;lt;ugly-sonic&amp;gt;] in the style of &amp;lt;lls&amp;gt; and the style of &amp;lt;nebula&amp;gt;, trending on artstation&lt;/code>, via Stable Diffusion
&lt;/figcaption>&lt;/figure>
&lt;p>Lastly, Stable Diffusion experts on &lt;a href="https://www.reddit.com/r/StableDiffusion/" target="_blank" rel="noopener">/r/StableDiffusion&lt;/a> have gotten prompt engineering down to a science, with massive prompts even longer than the ones above. Let&amp;rsquo;s just YOLO Ugly Sonic into one.&lt;/p>
&lt;figure id="figure--dynamic-comic-hero-pose-detailed-city-at-night-background-aesthetic-captivating-concept-art-anime-hyper-detailed-and-intricate-realistic-shaded-fine-detail-realistic-proportions-symmetrical-sharp-focus-8k-resolution-with-lineart-flat-ink-trending-on-pixiv-fanbox-via-stable-diffusion-prompt-adapted-from-herehttpswwwredditcomrstablediffusioncommentsxemaq3making_someone_dreams_comes_true_but_in_waifu">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="` dynamic comic hero pose, detailed city at night background, aesthetic, captivating, (((concept art, anime, hyper-detailed and intricate, realistic shaded, fine detail, realistic proportions, symmetrical, sharp focus, 8K resolution, with lineart flat ink, trending on pixiv fanbox)))`, via Stable Diffusion. Prompt adapted [from here](https://www.reddit.com/r/StableDiffusion/comments/xemaq3/making_someone_dreams_comes_true_but_in_waifu/)." srcset="
/2022/09/stable-diffusion-ugly-sonic/7762adbe6895884c_hu765bfde0c05ace0b134a4c39cdd5510f_1121042_8dcbbacb3ad520cb50908c8d5655ba50.png 400w,
/2022/09/stable-diffusion-ugly-sonic/7762adbe6895884c_hu765bfde0c05ace0b134a4c39cdd5510f_1121042_c7d7d18103a655093783784cd0e73b94.png 760w,
/2022/09/stable-diffusion-ugly-sonic/7762adbe6895884c_hu765bfde0c05ace0b134a4c39cdd5510f_1121042_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/09/stable-diffusion-ugly-sonic/7762adbe6895884c_hu765bfde0c05ace0b134a4c39cdd5510f_1121042_8dcbbacb3ad520cb50908c8d5655ba50.png"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>&amp;lt;ugly-sonic&amp;gt; dynamic comic hero pose, detailed city at night background, aesthetic, captivating, (((concept art, anime, hyper-detailed and intricate, realistic shaded, fine detail, realistic proportions, symmetrical, sharp focus, 8K resolution, with lineart flat ink, trending on pixiv fanbox)))&lt;/code>, via Stable Diffusion. Prompt adapted &lt;a href="https://www.reddit.com/r/StableDiffusion/comments/xemaq3/making_someone_dreams_comes_true_but_in_waifu/" target="_blank" rel="noopener">from here&lt;/a>.
&lt;/figcaption>&lt;/figure>
&lt;p>The funny thing about textual inversion is that each of these concepts are only 4KB on disk. Although a given textual inversion concept may not work with future versions of Stable Diffusions or other diffusion models using the CLIP encoder, it&amp;rsquo;s a good demo of how well trained concepts can be used to get more specific outputs, even if the concept isn&amp;rsquo;t in the original dataset the model was trained upon.&lt;/p>
&lt;p>Again, you can use the &lt;a href="https://huggingface.co/sd-concepts-library/ugly-sonic" target="_blank" rel="noopener">Ugly Sonic concept&lt;/a> yourself with a &lt;a href="https://colab.research.google.com/drive/1-Go3l9HpSIkjvDfR0gm8kWLPRnsaUIYd?usp=sharing" target="_blank" rel="noopener">textual inversion inference notebook&lt;/a> or &lt;a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui" target="_blank" rel="noopener">another Stable Diffusion user interface&lt;/a> that supports textual inversion to generate your own Ugly Sonics with Stable Diffusion!&lt;/p>
&lt;p>There were a few AI-generated images of Ugly Sonic with his human teeth, but I opted not to include them because I have &lt;em>standards&lt;/em>, believe it or not.&lt;/p>
&lt;hr>
&lt;p>&lt;em>Disclosure: I am neither an artist nor an expert in art theory. All my comments on what are &amp;ldquo;good&amp;rdquo; AI art generations are my own (likely bad) opinions.&lt;/em>&lt;/p>
&lt;p>&lt;em>Also, I am not a furry. Even though my name is Max Woolf.&lt;/em>&lt;/p></description></item><item><title>How to Create a Blog Post Title Optimizer with GPT-3 and Hacker News Data</title><link>https://minimaxir.com/2022/08/gpt3-blog-title-optimizer/</link><pubDate>Mon, 15 Aug 2022 08:30:00 -0700</pubDate><guid>https://minimaxir.com/2022/08/gpt3-blog-title-optimizer/</guid><description>&lt;p>&lt;span>&lt;style type="text/css">
pre code {
white-space: pre-wrap !important;
}
&lt;/style>&lt;/span>&lt;/p>
&lt;p>I am objectively terrible at writing attractive titles for my blog posts. Which is a problem, as nowadays it&amp;rsquo;s a commonly understood truth that a good headline can be the sole factor whether a blog post goes viral or gets completely ignored, especially in the data science/machine learning fields I typically write about.&lt;/p>
&lt;p>So, why not use said data science/machine learning to create an optimized title for me?&lt;/p>
&lt;p>Many know &lt;a href="https://openai.com/api/" target="_blank" rel="noopener">GPT-3&lt;/a> as a tool for robust text generation. But a newer, lesser discussed feature that &lt;a href="https://openai.com" target="_blank" rel="noopener">OpenAI&lt;/a> allows is finetuning GPT-3 on data you provide. If I provide GPT-3 with a large dataset of good titles, can I use that to tell me if one of my blog post titles are good? Let&amp;rsquo;s give it a try.&lt;/p>
&lt;h2 id="getting-the-good-blog-post-data-from-hacker-news">Getting The Good Blog Post Data from Hacker News&lt;/h2>
&lt;div class="alert alert-">
&lt;div>
All code and tools used in this blog post are available &lt;a href="https://github.com/minimaxir/gpt3-blog-title-optimizer" target="_blank" rel="noopener">open-source on GitHub&lt;/a>.
&lt;/div>
&lt;/div>
&lt;p>The AI classifier I will create will be a &lt;a href="https://en.wikipedia.org/wiki/Binary_classification" target="_blank" rel="noopener">binary classifier&lt;/a>, which returns the probability that an input blog post title is good, and from that I can provide alternate blog post titles and see roughly which is best from those probabilities.&lt;/p>
&lt;p>In order to finetune GPT-3 for this use case, I need to obtain a decently large amount of post titles with &lt;code>good&lt;/code> and &lt;code>bad&lt;/code> labels. For this experiment, I&amp;rsquo;ll use submission data from &lt;a href="https://news.ycombinator.com" target="_blank" rel="noopener">Hacker News&lt;/a>.&lt;/p>
&lt;figure id="figure-hacker-news-frontpage-on-august-14th-2022">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Hacker News frontpage on August 14th, 2022." srcset="
/2022/08/gpt3-blog-title-optimizer/hn_front_page_hu42e63d32a0cf25a962db4c01e09cdfd0_175424_ec78c6a527289f8895c4a3f1955321a2.png 400w,
/2022/08/gpt3-blog-title-optimizer/hn_front_page_hu42e63d32a0cf25a962db4c01e09cdfd0_175424_883be035c01d6e815aa8b9ed5d91fd0d.png 760w,
/2022/08/gpt3-blog-title-optimizer/hn_front_page_hu42e63d32a0cf25a962db4c01e09cdfd0_175424_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/08/gpt3-blog-title-optimizer/hn_front_page_hu42e63d32a0cf25a962db4c01e09cdfd0_175424_ec78c6a527289f8895c4a3f1955321a2.png"
width="760"
height="312"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Hacker News frontpage on August 14th, 2022.
&lt;/figcaption>&lt;/figure>
&lt;p>Hacker News data is good for a few reasons: each submission has community validation by a large number of people, submission titles cover a wide variety of idiosyncratic styles, and most of all, it&amp;rsquo;s easy to get Hacker News submission data in bulk &lt;a href="https://console.cloud.google.com/marketplace/details/y-combinator/hacker-news" target="_blank" rel="noopener">from BigQuery&lt;/a>. For example, if I wanted to get all submissions between August 2020 and 2022 with atleast a score of 10 (the rough minimum to get on the front page and to filter out some spam), plus some light filters to remove things that are definitely not blog posts or articles (such as &lt;a href="https://news.ycombinator.com/show" target="_blank" rel="noopener">Show HNs&lt;/a> and social media), I&amp;rsquo;d input a SQL query something like this:&lt;/p>
&lt;pre>&lt;code class="language-sql">SELECT
title,
score
FROM
`bigquery-public-data.hacker_news.full`
WHERE
type = &amp;quot;story&amp;quot;
AND score &amp;gt;= 10
AND url IS NOT NULL
AND timestamp BETWEEN &amp;quot;2020-08-01&amp;quot; AND &amp;quot;2022-08-01&amp;quot;
AND NOT REGEXP_CONTAINS(title, r&amp;quot;^Show HN&amp;quot;)
AND NOT REGEXP_CONTAINS(url, r&amp;quot;(?:github|youtube|twitter)\.com&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>This query returns roughly 90k submission titles total. For &lt;code>good&lt;/code> titles, let&amp;rsquo;s say we consider posts with atleast 100 points as &amp;ldquo;good&amp;rdquo;, because it&amp;rsquo;s a nice number which is sometimes all that&amp;rsquo;s necessary in the world of data science. There are about 27k posts with more than 100 points in that subset, which is more than sufficient. The harder part is selecting the &lt;code>bad&lt;/code> titles: since there are 63k titles fewer than 100 points, the data set as-is is unbalanced ~1:3 and will lead to flawed training results.&lt;/p>
&lt;p>There are two solutions: either repeat the &lt;code>good&lt;/code> posts to roughly equal the &lt;code>bad&lt;/code> posts, or take a subset of &lt;code>bad&lt;/code> posts to roughly equal the amount of &lt;code>good&lt;/code> posts. We&amp;rsquo;ll do the latter since the sample size of &lt;code>good&lt;/code> posts is large enough. Most people would download all 90k rows into something like Python to handle that sampling, but with SQL shenanigans you can do it entirely in BigQuery. (the annotated query &lt;a href="https://github.com/minimaxir/gpt3-blog-title-optimizer/blob/main/gpt3_input_data.sql" target="_blank" rel="noopener">is here&lt;/a> and out of scope for this post, but may be interesting for data science hiring managers who want to annoy candidates in screening interviews)&lt;/p>
&lt;p>This results in a ~55k title dataset: 27k &lt;code>good&lt;/code>, 27k &lt;code>bad&lt;/code>, perfectly balanced, as all datasets should be.&lt;/p>
&lt;p>OpenAI&amp;rsquo;s &lt;a href="https://beta.openai.com/docs/guides/fine-tuning" target="_blank" rel="noopener">finetuning API&lt;/a> takes in a JSONL file where each line is a JSON object with two fields: &lt;code>prompt&lt;/code> and &lt;code>completion&lt;/code> (no, I am not sure why it can&amp;rsquo;t just be a CSV). In this case, the &lt;code>prompt&lt;/code> is the title, prepended with &lt;code>Title: &lt;/code> and with a &lt;code> -&amp;gt;&lt;/code> suffix per their documentation suggestions to &amp;ldquo;align&amp;rdquo; it better to GPT-3, and the &lt;code>completion&lt;/code> is the good/bad label, prepended with a space because GPT-3 is weird like that. An example of the final dataset:&lt;/p>
&lt;pre>&lt;code class="language-json">{&amp;quot;prompt&amp;quot;:&amp;quot;Title: How to slightly improve your life without trying -&amp;gt;&amp;quot;,&amp;quot;completion&amp;quot;:&amp;quot; bad&amp;quot;}
{&amp;quot;prompt&amp;quot;:&amp;quot;Title: SixtyFPS Becomes Slint -&amp;gt;&amp;quot;,&amp;quot;completion&amp;quot;:&amp;quot; bad&amp;quot;}
{&amp;quot;prompt&amp;quot;:&amp;quot;Title: Family estrangement: Why adults are cutting off their parents -&amp;gt;&amp;quot;,&amp;quot;completion&amp;quot;:&amp;quot; bad&amp;quot;}
&lt;/code>&lt;/pre>
&lt;p>Their CLI cleans and can extract a validation set out of the inputs, which you should always do. Fortunately, BigQuery now offers JSONL export, so downloading the resulting dataset requires no further preprocessing. Once that&amp;rsquo;s done, the CLI allows you finetune, with special options for binary classification. (the exact CLI command I used is &lt;a href="https://github.com/minimaxir/gpt3-blog-title-optimizer/blob/main/finetune_command.txt" target="_blank" rel="noopener">here&lt;/a>)&lt;/p>
&lt;p>Another understated aspect of GPT-3 is that there are weaker models that are faster and much cheaper than the default &lt;code>davinci&lt;/code> model that is what people use when they generally use &amp;ldquo;GPT-3&amp;rdquo;. For text generation they tend to have less coherent outputs, but for a simplified use case like binary classification they are more than sufficient. I&amp;rsquo;ll use the &lt;code>babbage&lt;/code> model, the second weakest.&lt;/p>
&lt;p>The &lt;a href="https://github.com/minimaxir/gpt3-blog-title-optimizer/blob/main/finetune_results.csv" target="_blank" rel="noopener">final results&lt;/a> of the finetuning are about 63% accuracy on both the training and validation sets: not too much better than the default 50% accuracy of a balanced dataset for a binary classification problem, but given the problem difficulty it&amp;rsquo;s better than &lt;a href="https://minimaxir.com/2018/09/modeling-link-aggregators/" target="_blank" rel="noopener">most approaches I&amp;rsquo;ve done&lt;/a> for Hacker News data.&lt;/p>
&lt;p>Once the finetuning is complete, you can query it, and ask it to return the probability of the returned token. Let&amp;rsquo;s pass in the title for my last blog post: &lt;strong>&lt;a href="https://minimaxir.com/2022/07/food-photography-ai/" target="_blank" rel="noopener">Absurd AI-Generated Professional Food Photography with DALL-E 2&lt;/a>&lt;/strong>&lt;/p>
&lt;pre>&lt;code class="language-json">&amp;quot;top_logprobs&amp;quot;: [
{
&amp;quot; bad&amp;quot;: -0.34654787
}
&lt;/code>&lt;/pre>
&lt;p>Well, that&amp;rsquo;s not promising.&lt;/p>
&lt;p>For some &lt;em>really&lt;/em> weird reason, the API returns a log-probability instead of the actual probability that you&amp;rsquo;d want, so taking the &lt;a href="https://en.wikipedia.org/wiki/Exponential_function" target="_blank" rel="noopener">exponent&lt;/a> of that value results in a 70.7% probability it&amp;rsquo;s bad, which means there&amp;rsquo;s a 29.3% chance it&amp;rsquo;s good.&lt;/p>
&lt;p>And &lt;em>that&lt;/em>, is why I need a title optimizer.&lt;/p>
&lt;h2 id="using-instructgpt-to-create-alternate-titles">Using InstructGPT To Create Alternate Titles&lt;/h2>
&lt;p>Since we now have a tool to determine the quality of blog post titles, how do we generate alternate titles that maintain the same meaning? I could think of tweaks to titles, but that takes &lt;em>effort&lt;/em> and I am lazy. What if GPT-3 could create the candidate titles for me? It turns out, GPT-3 latest Instruct model can.&lt;/p>
&lt;p>InstructGPT, &lt;a href="https://openai.com/blog/instruction-following/" target="_blank" rel="noopener">released in January&lt;/a> without much fanfare, is a version of &lt;code>davinci&lt;/code> OpenAI finetuned themselves to better respond to instructions. It worked so well that it&amp;rsquo;s now the default GPT-3 model (noted as &lt;code>text-davinci-002&lt;/code> in the backend UI).&lt;/p>
&lt;p>InstructGPT is surprisingly robust with the right prompt engineering. You can tell it to create detailed product descriptions of &lt;a href="https://twitter.com/minimaxir/status/1551609670237708288" target="_blank" rel="noopener">nonexistent video games&lt;/a>, or write &lt;a href="https://twitter.com/minimaxir/status/1536824548376465409" target="_blank" rel="noopener">4chan-style greentexts&lt;/a> for any domain which maintain both the style and twist endings of the format.&lt;/p>
&lt;figure id="figure-via-openais-gpt-3-playground-all-nonhighlighted-text-is-the-prompt">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="via OpenAI&amp;#39;s GPT-3 Playground; all nonhighlighted text is the prompt." srcset="
/2022/08/gpt3-blog-title-optimizer/greentext_hu6b8810fb3b91cff2277dcc8f3fe509ea_64954_b4bff31f96a9037f8f2f9bfcc1cf67b5.png 400w,
/2022/08/gpt3-blog-title-optimizer/greentext_hu6b8810fb3b91cff2277dcc8f3fe509ea_64954_117773c4bcdd8f5fdcc616901af9b019.png 760w,
/2022/08/gpt3-blog-title-optimizer/greentext_hu6b8810fb3b91cff2277dcc8f3fe509ea_64954_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/08/gpt3-blog-title-optimizer/greentext_hu6b8810fb3b91cff2277dcc8f3fe509ea_64954_b4bff31f96a9037f8f2f9bfcc1cf67b5.png"
width="684"
height="378"
loading="lazy" data-zoomable class=" img-light" />&lt;/div>
&lt;/div>&lt;figcaption>
via OpenAI&amp;rsquo;s GPT-3 Playground; all nonhighlighted text is the prompt.
&lt;/figcaption>&lt;/figure>
&lt;p>After a bit of testing, the prompt I found worked best for this use case was:&lt;/p>
&lt;pre>&lt;code class="language-txt">Rewrite the following blog post title into six different titles but optimized for social media virality: &amp;lt;FILL IN TITLE&amp;gt;
-
&lt;/code>&lt;/pre>
&lt;p>It&amp;rsquo;s verbose, but that&amp;rsquo;s prompt engineering for you. The &lt;code>-&lt;/code> at the end informs GPT-3 that the output should be a list with dash-bullets, which will make it easier to programmatically split the final output into distinct titles.&lt;/p>
&lt;p>You can test it on the &lt;a href="https://beta.openai.com/playground" target="_blank" rel="noopener">GPT-3 Playground&lt;/a>; if the &lt;code>temperature&lt;/code> parameter is &lt;code>0&lt;/code>, then the output will be deterministic.&lt;/p>
&lt;p>Again putting in my last blog post &lt;strong>&lt;a href="https://minimaxir.com/2022/07/food-photography-ai/" target="_blank" rel="noopener">Absurd AI-Generated Professional Food Photography with DALL-E 2&lt;/a>&lt;/strong> into InstructGPT:&lt;/p>
&lt;figure id="figure-via-openais-gpt-3-playground-all-nonhighlighted-text-is-the-prompt">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="via OpenAI&amp;#39;s GPT-3 Playground; all nonhighlighted text is the prompt." srcset="
/2022/08/gpt3-blog-title-optimizer/gpt3_demo_hu0d11cdf6fd066973b4bcded43fe12a68_139195_55bad9aac651b90e8445764dc94c311c.png 400w,
/2022/08/gpt3-blog-title-optimizer/gpt3_demo_hu0d11cdf6fd066973b4bcded43fe12a68_139195_f5153536eef071f8c0a7ee3424e16812.png 760w,
/2022/08/gpt3-blog-title-optimizer/gpt3_demo_hu0d11cdf6fd066973b4bcded43fe12a68_139195_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/08/gpt3-blog-title-optimizer/gpt3_demo_hu0d11cdf6fd066973b4bcded43fe12a68_139195_55bad9aac651b90e8445764dc94c311c.png"
width="760"
height="272"
loading="lazy" data-zoomable class=" img-light" />&lt;/div>
&lt;/div>&lt;figcaption>
via OpenAI&amp;rsquo;s GPT-3 Playground; all nonhighlighted text is the prompt.
&lt;/figcaption>&lt;/figure>
&lt;p>All six of those titles are definitely an improvement, and all the text in green is what the programmatic API returns. Notably, despite the terseness of the input title and recency of DALL-E 2, InstructGPT is able to infer that the AI &lt;em>creates&lt;/em> something and work from that, which is impressive.&lt;/p>
&lt;h2 id="put-the-title-optimizer-into-action">Put The Title Optimizer Into Action!&lt;/h2>
&lt;div class="alert alert-">
&lt;div>
A walkthrough of the code used to interact with the GPT-3 API and make the optimizer is available &lt;a href="https://github.com/minimaxir/gpt3-blog-title-optimizer/blob/main/gpt3_title_optimizer_walkthrough.ipynb" target="_blank" rel="noopener">in this Jupyter Notebook&lt;/a>, and the final demos are available &lt;a href="https://github.com/minimaxir/gpt3-blog-title-optimizer/blob/main/gpt3_title_optimizer_demo.ipynb" target="_blank" rel="noopener">in this Notebook&lt;/a>.
&lt;/div>
&lt;/div>
&lt;p>Now that we have the two models ready, the workflow is simple:&lt;/p>
&lt;ul>
&lt;li>Choose the title of a technical blog post I want to optimize.&lt;/li>
&lt;li>Ping InstructGPT to get up to 6 alternate titles.&lt;/li>
&lt;li>Extract/clean up the generated titles (i.e. split and strip whitespace)&lt;/li>
&lt;li>For each of those alternate titles, ping the finetuned Hacker News GPT-3 for the probability that it is a &lt;code>good&lt;/code> title.&lt;/li>
&lt;li>In a pretty table, sort the titles by probability, descending.&lt;/li>
&lt;/ul>
&lt;p>Because the model can&amp;rsquo;t be widely distributed without review due to OpenAI rules, I decided to put the &amp;ldquo;UI&amp;rdquo; for this into a personal Jupyter Notebook.&lt;/p>
&lt;p>Let&amp;rsquo;s experiment! We know the title of &lt;strong>&lt;a href="https://minimaxir.com/2022/07/food-photography-ai/" target="_blank" rel="noopener">Absurd AI-Generated Professional Food Photography with DALL-E 2&lt;/a>&lt;/strong> is bad and the alternatives are interesting, but how good are the alternatives?&lt;/p>
&lt;figure id="figure-via-gpt-3-title-optimizer">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="via GPT-3 Title Optimizer" srcset="
/2022/08/gpt3-blog-title-optimizer/dalle_hu0c17b105ff2781ebaf54090e4cc144f0_117894_9a1e35b7591d821f5352785857c5eea9.png 400w,
/2022/08/gpt3-blog-title-optimizer/dalle_hu0c17b105ff2781ebaf54090e4cc144f0_117894_dc70bd07563d5c6b640a0a22179bf02f.png 760w,
/2022/08/gpt3-blog-title-optimizer/dalle_hu0c17b105ff2781ebaf54090e4cc144f0_117894_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/08/gpt3-blog-title-optimizer/dalle_hu0c17b105ff2781ebaf54090e4cc144f0_117894_9a1e35b7591d821f5352785857c5eea9.png"
width="760"
height="298"
loading="lazy" data-zoomable class=" img-light" />&lt;/div>
&lt;/div>&lt;figcaption>
via GPT-3 Title Optimizer
&lt;/figcaption>&lt;/figure>
&lt;p>Most of alternates are &lt;em>much&lt;/em> better, with the predicted probabilities of being a good post going above 50%. (I probably should change the title retroactively but I will live with my SEO dishonor)&lt;/p>
&lt;p>The original title for this post, in my boring no-one-will-ever-click-this style, was &lt;strong>Creating a Blog Post Title Optimizer by Finetuning GPT-3 on Hacker News&lt;/strong>. Let&amp;rsquo;s plop it into the optimizer:&lt;/p>
&lt;figure id="figure-via-gpt-3-title-optimizer-temperature0">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="via GPT-3 Title Optimizer, `temperature=0`" srcset="
/2022/08/gpt3-blog-title-optimizer/temp0_hu0ecc6a6c2e59b340abaedefc7b4ce840_92091_1fe6bd97262f8592fe50fec7b77b0742.png 400w,
/2022/08/gpt3-blog-title-optimizer/temp0_hu0ecc6a6c2e59b340abaedefc7b4ce840_92091_6fbfb4be422bbcc17c1e1eea97941acf.png 760w,
/2022/08/gpt3-blog-title-optimizer/temp0_hu0ecc6a6c2e59b340abaedefc7b4ce840_92091_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/08/gpt3-blog-title-optimizer/temp0_hu0ecc6a6c2e59b340abaedefc7b4ce840_92091_1fe6bd97262f8592fe50fec7b77b0742.png"
width="760"
height="258"
loading="lazy" data-zoomable class=" img-light" />&lt;/div>
&lt;/div>&lt;figcaption>
via GPT-3 Title Optimizer, &lt;code>temperature=0&lt;/code>
&lt;/figcaption>&lt;/figure>
&lt;p>So yes, the optimizer says the original title is very bad. But in this case, the variants are clickbaity and probably wouldn&amp;rsquo;t do very well on Hacker News.&lt;/p>
&lt;p>Fortunately, you can rerun the generation and get more different variants if &lt;code>temperature&lt;/code> is nonzero.&lt;/p>
&lt;figure id="figure-via-gpt-3-title-optimizer-temperature07">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="via GPT-3 Title Optimizer, `temperature=0.7`" srcset="
/2022/08/gpt3-blog-title-optimizer/temp_0_7_hufa9118aebf56a2c50f5c489344895268_101648_200689e534b722bc65f0bce9f78d9f2e.png 400w,
/2022/08/gpt3-blog-title-optimizer/temp_0_7_hufa9118aebf56a2c50f5c489344895268_101648_416d672ba8215eb8ccf3d14954d3856d.png 760w,
/2022/08/gpt3-blog-title-optimizer/temp_0_7_hufa9118aebf56a2c50f5c489344895268_101648_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/08/gpt3-blog-title-optimizer/temp_0_7_hufa9118aebf56a2c50f5c489344895268_101648_200689e534b722bc65f0bce9f78d9f2e.png"
width="760"
height="295"
loading="lazy" data-zoomable class=" img-light" />&lt;/div>
&lt;/div>&lt;figcaption>
via GPT-3 Title Optimizer, &lt;code>temperature=0.7&lt;/code>
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-via-gpt-3-title-optimizer-temperature10">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="via GPT-3 Title Optimizer, `temperature=1.0`" srcset="
/2022/08/gpt3-blog-title-optimizer/temp_1_0_hu15d985b231e498ae04343f18c3087731_91346_ff2adf74dba39c54290c8a9b95d8c2ec.png 400w,
/2022/08/gpt3-blog-title-optimizer/temp_1_0_hu15d985b231e498ae04343f18c3087731_91346_94f334e0fd6e5bedb478f3c93730d410.png 760w,
/2022/08/gpt3-blog-title-optimizer/temp_1_0_hu15d985b231e498ae04343f18c3087731_91346_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/08/gpt3-blog-title-optimizer/temp_1_0_hu15d985b231e498ae04343f18c3087731_91346_ff2adf74dba39c54290c8a9b95d8c2ec.png"
width="760"
height="267"
loading="lazy" data-zoomable class=" img-light" />&lt;/div>
&lt;/div>&lt;figcaption>
via GPT-3 Title Optimizer, &lt;code>temperature=1.0&lt;/code>
&lt;/figcaption>&lt;/figure>
&lt;p>Definitely more variety. I like &amp;ldquo;How to Create a Blog Post Title Optimizer with GPT-3&amp;rdquo; as it maintains the same spirit even if it&amp;rsquo;s not the most optimal, although for disclosure reasons, I do want to include Hacker News somewhere in the title. Therefore, I can tweak the input to &amp;ldquo;How to Create a Blog Post Title Optimizer with GPT-3 and Hacker News Data&amp;rdquo; and feed it back to the optimizer and maybe get an interative improvement.&lt;/p>
&lt;figure id="figure-via-gpt-3-title-optimizer">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="via GPT-3 Title Optimizer" srcset="
/2022/08/gpt3-blog-title-optimizer/final_variant_hufacaa2ba5f2c158a244cae7afde6b7e0_119219_45d2e78ed9c11130cb4cdda65b91c871.png 400w,
/2022/08/gpt3-blog-title-optimizer/final_variant_hufacaa2ba5f2c158a244cae7afde6b7e0_119219_18c21b3b15b094f9d27fabe514866f32.png 760w,
/2022/08/gpt3-blog-title-optimizer/final_variant_hufacaa2ba5f2c158a244cae7afde6b7e0_119219_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/08/gpt3-blog-title-optimizer/final_variant_hufacaa2ba5f2c158a244cae7afde6b7e0_119219_45d2e78ed9c11130cb4cdda65b91c871.png"
width="760"
height="299"
loading="lazy" data-zoomable class=" img-light" />&lt;/div>
&lt;/div>&lt;figcaption>
via GPT-3 Title Optimizer
&lt;/figcaption>&lt;/figure>
&lt;p>The probability went down significantly with the change, and none of the variants are much better. Oh well.&lt;/p>
&lt;p>Here&amp;rsquo;s the results of running the optimizer for some of my older blog posts:&lt;/p>
&lt;figure id="figure-the-results-for-this-posthttpsminimaxircom201810data-science-protips-are-indeed-better-id-definitely-click-the-top-one-although-its-misleading">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="The results for [this post](https://minimaxir.com/2018/10/data-science-protips/) are indeed better; I&amp;#39;d definitely click the top one although it&amp;#39;s misleading." srcset="
/2022/08/gpt3-blog-title-optimizer/moocs_hu7f429ebbbeafb0c000ca56d2e510b48a_103804_43dfa62c5ff311c2c66ab47990f391d3.png 400w,
/2022/08/gpt3-blog-title-optimizer/moocs_hu7f429ebbbeafb0c000ca56d2e510b48a_103804_35ee422cb26a9e8e0944aa1d3fd9197d.png 760w,
/2022/08/gpt3-blog-title-optimizer/moocs_hu7f429ebbbeafb0c000ca56d2e510b48a_103804_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/08/gpt3-blog-title-optimizer/moocs_hu7f429ebbbeafb0c000ca56d2e510b48a_103804_43dfa62c5ff311c2c66ab47990f391d3.png"
width="760"
height="274"
loading="lazy" data-zoomable class=" img-light" />&lt;/div>
&lt;/div>&lt;figcaption>
The results for &lt;a href="https://minimaxir.com/2018/10/data-science-protips/" target="_blank" rel="noopener">this post&lt;/a> are indeed better; I&amp;rsquo;d definitely click the top one although it&amp;rsquo;s misleading.
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-the-results-for-this-posthttpsminimaxircom201803basketball-shots-are-much-better-although-this-is-one-case-where-the-original-title-is-actually-good">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="The results for [this post](https://minimaxir.com/2018/03/basketball-shots/) are much better, although this is one case where the original title is actually good." srcset="
/2022/08/gpt3-blog-title-optimizer/ncaa_hu4ddbe60d530982acc1eee7bbbad84a7c_112282_5d14a14f96271951e9f6672e64eb7a6f.png 400w,
/2022/08/gpt3-blog-title-optimizer/ncaa_hu4ddbe60d530982acc1eee7bbbad84a7c_112282_6151532319f452a57e8ec472dde44899.png 760w,
/2022/08/gpt3-blog-title-optimizer/ncaa_hu4ddbe60d530982acc1eee7bbbad84a7c_112282_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/08/gpt3-blog-title-optimizer/ncaa_hu4ddbe60d530982acc1eee7bbbad84a7c_112282_5d14a14f96271951e9f6672e64eb7a6f.png"
width="760"
height="295"
loading="lazy" data-zoomable class=" img-light" />&lt;/div>
&lt;/div>&lt;figcaption>
The results for &lt;a href="https://minimaxir.com/2018/03/basketball-shots/" target="_blank" rel="noopener">this post&lt;/a> are much better, although this is one case where the original title is actually good.
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-the-results-for-this-posthttpsminimaxircom201306working-as-intended-are-a-balance-between-better-and-not-technically-misleading-clickbait">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="The results for [this post](https://minimaxir.com/2013/06/working-as-intended/) are a balance between better and not-*technically*-misleading clickbait." srcset="
/2022/08/gpt3-blog-title-optimizer/pc_game_hu17f6d0a68036eb60a143e9bcd807d213_101390_693e7673d9fc6ec72d444efecfa7a46c.png 400w,
/2022/08/gpt3-blog-title-optimizer/pc_game_hu17f6d0a68036eb60a143e9bcd807d213_101390_b66f97ba184240928cd90ae705370ea5.png 760w,
/2022/08/gpt3-blog-title-optimizer/pc_game_hu17f6d0a68036eb60a143e9bcd807d213_101390_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/08/gpt3-blog-title-optimizer/pc_game_hu17f6d0a68036eb60a143e9bcd807d213_101390_693e7673d9fc6ec72d444efecfa7a46c.png"
width="760"
height="310"
loading="lazy" data-zoomable class=" img-light" />&lt;/div>
&lt;/div>&lt;figcaption>
The results for &lt;a href="https://minimaxir.com/2013/06/working-as-intended/" target="_blank" rel="noopener">this post&lt;/a> are a balance between better and not-&lt;em>technically&lt;/em>-misleading clickbait.
&lt;/figcaption>&lt;/figure>
&lt;p>Costwise, the entire pipeline is relatively inexpensive. Overall, it&amp;rsquo;s about $0.02 per run: too expensive to give unrestricted access to the internet, but very high return-on-investment if it successfully results in a catchy headline even if takes multiple tries. The most expensive part was the finetuning itself, which cost $2 but is a one-time cost.&lt;/p>
&lt;p>Some might ask &amp;ldquo;why finetune GPT-3 when you can finetune an open-source large language model such as &lt;a href="https://en.wikipedia.org/wiki/BERT_%28language_model%29" target="_blank" rel="noopener">BERT&lt;/a> like every NLP project since 2018?&amp;rdquo; In this case, GPT-3&amp;rsquo;s advantage is that it was trained in the entire internet. GPT-3 is a master of idiosyncrasy, which is a key when working with Hacker News data and in theory would give better results than the Wikipedia-trained BERT. The success of Hacker News posts also depends on a global context outside of the title itself, which is why finetuning an existing model trained on such context may be better than training an existing model solely on HN data.&lt;/p>
&lt;p>Some are concerned about GPT-3 and AI tools such as these making writers redundant, but the results here prove otherwise: there will always have to be a human-in-the-loop.&lt;/p>
&lt;hr>
&lt;p>UPDATE: When I &lt;a href="https://news.ycombinator.com/item?id=32471208" target="_blank" rel="noopener">submitted this post&lt;/a> to Hacker News, it ended up getting &lt;em>over 200 points&lt;/em>, defying the 20.8% probability!&lt;/p></description></item><item><title>Absurd AI-Generated Professional Food Photography with DALL-E 2</title><link>https://minimaxir.com/2022/07/food-photography-ai/</link><pubDate>Mon, 25 Jul 2022 08:15:00 -0700</pubDate><guid>https://minimaxir.com/2022/07/food-photography-ai/</guid><description>&lt;p>Good-looking food has been a part of internet culture ever since the beginning. Top &lt;a href="https://www.instagram.com" target="_blank" rel="noopener">Instagram&lt;/a>, &lt;a href="https://www.youtube.com" target="_blank" rel="noopener">YouTube&lt;/a>, and &lt;a href="https://www.tiktok.com/en/" target="_blank" rel="noopener">TikTok&lt;/a> foodie accounts have millions of followers, and recipe blogs are some of the most highly trafficked content on the entire internet.&lt;/p>
&lt;p>But now that large AI-image generation models such as &lt;a href="https://openai.com/dall-e-2/" target="_blank" rel="noopener">DALL-E 2&lt;/a> by &lt;a href="https://openai.com/" target="_blank" rel="noopener">OpenAI&lt;/a> have been made available, perhaps AI can provide new and &lt;em>unique&lt;/em> ideas for food content on the internet.&lt;/p>
&lt;p>For example, let&amp;rsquo;s say you ask DALL-E 2 to generate &lt;code>a colorful alcoholic cocktail&lt;/code>:&lt;/p>
&lt;figure id="figure-_a-colorful-alcoholic-cocktail_-dall-e-2">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="_a colorful alcoholic cocktail_ (DALL-E 2)" srcset="
/2022/07/food-photography-ai/58991471_hu3f692f7d732c242dfb929f06b6227e7d_796282_f11361db66f04c93f9ba631189471ea9.png 400w,
/2022/07/food-photography-ai/58991471_hu3f692f7d732c242dfb929f06b6227e7d_796282_0f8aedcd39bda2b7969bd24c64742ed3.png 760w,
/2022/07/food-photography-ai/58991471_hu3f692f7d732c242dfb929f06b6227e7d_796282_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/07/food-photography-ai/58991471_hu3f692f7d732c242dfb929f06b6227e7d_796282_f11361db66f04c93f9ba631189471ea9.png"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;em>a colorful alcoholic cocktail&lt;/em> (DALL-E 2)
&lt;/figcaption>&lt;/figure>
&lt;p>All the generated images are coherent and do indeed depict a cocktail, although the compositions are inconsistent which may not be what we would want to share on social media.&lt;/p>
&lt;p>The best way to improve the image quality of AI-generated images is to use &lt;a href="https://dallery.gallery/the-dalle-2-prompt-book/" target="_blank" rel="noopener">prompt engineering&lt;/a>, as these models don&amp;rsquo;t create &amp;ldquo;good&amp;rdquo; images by default, just statistically average images based on its training data. For example, adding &amp;ldquo;trending on &lt;a href="https://www.artstation.com/" target="_blank" rel="noopener">artstation&lt;/a>&amp;rdquo; to any prompt for any image tends to make it look a lot more artsy, and the &amp;ldquo;trending&amp;rdquo; is a correlative signal with good artwork.&lt;/p>
&lt;p>In the case of realistic food, I found that &lt;code>professional food photography&lt;/code> does the trick for food-esque prompts. Adding that to the cocktail prompt above:&lt;/p>
&lt;figure id="figure-_a-colorful-alcoholic-cocktail-professional-food-photography_-dall-e-2">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="_a colorful alcoholic cocktail, professional food photography_ (DALL-E 2)" srcset="
/2022/07/food-photography-ai/4915c019_hu289496384e5c1268fbdcfc5488dcb7b8_916438_29af2ef6097d9aba5761297aa4b0c681.png 400w,
/2022/07/food-photography-ai/4915c019_hu289496384e5c1268fbdcfc5488dcb7b8_916438_4171c8f82a0fad810e954e3e2ea9ab36.png 760w,
/2022/07/food-photography-ai/4915c019_hu289496384e5c1268fbdcfc5488dcb7b8_916438_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/07/food-photography-ai/4915c019_hu289496384e5c1268fbdcfc5488dcb7b8_916438_29af2ef6097d9aba5761297aa4b0c681.png"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;em>a colorful alcoholic cocktail, professional food photography&lt;/em> (DALL-E 2)
&lt;/figcaption>&lt;/figure>
&lt;p>Indeed, in each image it&amp;rsquo;s a cocktail, but with bonuses such as increased detail, aesthetic garnishes both on the dish and table, and a depth-of-field blur effect to create a central focus on the dish itself. You could share any of those cocktail photos on social media and no one would be the wiser (although you should &lt;strong>always&lt;/strong> disclose if images are AI generated!)&lt;/p>
&lt;p>This is the first time I&amp;rsquo;ve seen AI image generation models generate food well without hitting the &lt;a href="https://en.wikipedia.org/wiki/Uncanny_valley" target="_blank" rel="noopener">uncanny valley&lt;/a>, and one of the few prompt &amp;ldquo;ingredients&amp;rdquo; (pun intended) where the resulting images have a consistent composition. It&amp;rsquo;s not a surprise, especially since, as noted, high-quality food content would be extremely prolific in DALL-E 2&amp;rsquo;s training data.&lt;/p>
&lt;p>What other fantastic foods can DALL-E 2 generate?&lt;/p>
&lt;h2 id="5-dimensional-hamburgers">5-Dimensional Hamburgers&lt;/h2>
&lt;p>The &lt;a href="https://openai.com/blog/dall-e/" target="_blank" rel="noopener">original DALL-E&lt;/a>, announced in 2021 but not publically accessible, went viral primairly due to the incredible creative results from demo prompts such as &lt;code>an armchair in the shape of an avocado&lt;/code>:&lt;/p>
&lt;figure id="figure-dall-e-demo-via-openaihttpsopenaicomblogdall-e">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="DALL-E demo, [via OpenAI](https://openai.com/blog/dall-e/)." srcset="
/2022/07/food-photography-ai/avocado_hu1266e0705fd3c7613d9b69e905210900_283630_296bac7fcae20bb34dcb5c2779124a0f.png 400w,
/2022/07/food-photography-ai/avocado_hu1266e0705fd3c7613d9b69e905210900_283630_1a53a97e1a992f3928643c3c5e8cab7d.png 760w,
/2022/07/food-photography-ai/avocado_hu1266e0705fd3c7613d9b69e905210900_283630_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/07/food-photography-ai/avocado_hu1266e0705fd3c7613d9b69e905210900_283630_296bac7fcae20bb34dcb5c2779124a0f.png"
width="760"
height="281"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
DALL-E demo, &lt;a href="https://openai.com/blog/dall-e/" target="_blank" rel="noopener">via OpenAI&lt;/a>.
&lt;/figcaption>&lt;/figure>
&lt;p>Although adding &amp;ldquo;professional food photography&amp;rdquo; alone works to generate realistic food dishes, you can combine it with a prompt for other shapes, even abstract and absurd shapes that shouldn&amp;rsquo;t be logically possible for certain foods.&lt;/p>
&lt;p>Let&amp;rsquo;s start with a basic shape, such as a heart. If you Google &amp;ldquo;X heart&amp;rdquo; for any food you will almost always get results (Instagram loves heart-shaped food). What about asking for a heart shape for a dish that by construction &lt;em>can&amp;rsquo;t&lt;/em> be in the shape of a heart, such as a taco?&lt;/p>
&lt;figure id="figure-_a-taco-in-the-shape-of-a-heart-professional-food-photography_-dall-e-2">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="_a taco in the shape of a heart, professional food photography_ (DALL-E 2)" srcset="
/2022/07/food-photography-ai/1b8510d6_hu487b075183c5bc26b5805ce297dc678a_803008_8201b2ea53763c3551c5e64ecd2252c0.png 400w,
/2022/07/food-photography-ai/1b8510d6_hu487b075183c5bc26b5805ce297dc678a_803008_edf644507a7f42de8ca41d379881b28e.png 760w,
/2022/07/food-photography-ai/1b8510d6_hu487b075183c5bc26b5805ce297dc678a_803008_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/07/food-photography-ai/1b8510d6_hu487b075183c5bc26b5805ce297dc678a_803008_8201b2ea53763c3551c5e64ecd2252c0.png"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;em>a taco in the shape of a heart, professional food photography&lt;/em> (DALL-E 2)
&lt;/figcaption>&lt;/figure>
&lt;p>DALL-E 2 is still able to work around it, even by creating a new type of taco shell and employing optical illusions. And occasionally it cheats, as in the case with the top-right image.&lt;/p>
&lt;p>Emoji are also valid options as shapes, which unlike hearts is far less common in Google Images. Let&amp;rsquo;s take a &lt;a href="https://en.wikipedia.org/wiki/Cobb_salad" target="_blank" rel="noopener">Cobb salad&lt;/a>, which has specific ingredients. Can DALL-E arrange them into a specific emoji?&lt;/p>
&lt;figure id="figure-_a-cobb-salad-in-the-shape-of-the-robot-emoji-professional-food-photography_-dall-e-2">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="_a Cobb salad in the shape of the robot emoji, professional food photography_ (DALL-E 2)" srcset="
/2022/07/food-photography-ai/05b432b0_huc214add6cd407adf6f99b350338eaa4a_963749_7ca9533a2d2c66f92ef27cb3d7d8399e.png 400w,
/2022/07/food-photography-ai/05b432b0_huc214add6cd407adf6f99b350338eaa4a_963749_823654c7b37ad107c6a7de29d05d8b36.png 760w,
/2022/07/food-photography-ai/05b432b0_huc214add6cd407adf6f99b350338eaa4a_963749_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/07/food-photography-ai/05b432b0_huc214add6cd407adf6f99b350338eaa4a_963749_7ca9533a2d2c66f92ef27cb3d7d8399e.png"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;em>a Cobb salad in the shape of the robot emoji, professional food photography&lt;/em> (DALL-E 2)
&lt;/figcaption>&lt;/figure>
&lt;p>The answer is yes.&lt;/p>
&lt;p>But we can get more absurd. For example, consider a &lt;a href="https://en.wikipedia.org/wiki/Rubik%27s_Cube" target="_blank" rel="noopener">Rubik&amp;rsquo;s cube&lt;/a>. Can DALL-E coerce obviously noncubic foods such as a peanut butter sandwich into one?&lt;/p>
&lt;figure id="figure-_a-peanut-butter-and-jelly-sandwich-in-the-shape-of-a-rubiks-cube-professional-food-photography_-dall-e-2">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="_a peanut butter and jelly sandwich in the shape of a Rubik&amp;#39;s cube, professional food photography_ (DALL-E 2)" srcset="
/2022/07/food-photography-ai/32f205b8_hudaeebaef95ff3b7cbf4c7150c032e06d_816318_716fe04985c96efd446777e3cc5231a1.png 400w,
/2022/07/food-photography-ai/32f205b8_hudaeebaef95ff3b7cbf4c7150c032e06d_816318_dd5717eaeb51eae95f7dda0bfbf4bab8.png 760w,
/2022/07/food-photography-ai/32f205b8_hudaeebaef95ff3b7cbf4c7150c032e06d_816318_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/07/food-photography-ai/32f205b8_hudaeebaef95ff3b7cbf4c7150c032e06d_816318_716fe04985c96efd446777e3cc5231a1.png"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;em>a peanut butter and jelly sandwich in the shape of a Rubik&amp;rsquo;s cube, professional food photography&lt;/em> (DALL-E 2)
&lt;/figcaption>&lt;/figure>
&lt;p>The answer is a resounding &lt;strong>yes&lt;/strong>.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Latte_art" target="_blank" rel="noopener">Latte art&lt;/a>, or drawing images in the milk foam of a latte, is a popular subset of food photography. But what about &lt;em>3D&lt;/em> latte art that goes outside the beverage?&lt;/p>
&lt;figure id="figure-_a-frappuccino-in-the-shape-of-a-swan-professional-food-photography_-dall-e-2">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="_A Frappuccino in the shape of a swan, professional food photography_ (DALL-E 2)" srcset="
/2022/07/food-photography-ai/2dbba05c_huc0b0d009127dc94b363969895d9e2374_776406_55718e8af7aaaf28c10ddef96d162be6.png 400w,
/2022/07/food-photography-ai/2dbba05c_huc0b0d009127dc94b363969895d9e2374_776406_62cfbf1be57aa414cb267e315adcbcbf.png 760w,
/2022/07/food-photography-ai/2dbba05c_huc0b0d009127dc94b363969895d9e2374_776406_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/07/food-photography-ai/2dbba05c_huc0b0d009127dc94b363969895d9e2374_776406_55718e8af7aaaf28c10ddef96d162be6.png"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;em>A Frappuccino in the shape of a swan, professional food photography&lt;/em> (DALL-E 2)
&lt;/figcaption>&lt;/figure>
&lt;p>What about going beyond the constraints of mere mortal perception of space and time? Can we assign food &lt;a href="https://en.wikipedia.org/wiki/Non-Euclidean_geometry" target="_blank" rel="noopener">non-Euclidean properties&lt;/a>?&lt;/p>
&lt;figure id="figure-_a-cobb-salad-in-the-shape-of-non-euclidean-geometry-professional-food-photography_-dall-e-2">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="_a Cobb salad in the shape of non-Euclidean geometry, professional food photography_ (DALL-E 2)" srcset="
/2022/07/food-photography-ai/14d08e41_huefc89c8766b2a970f06a199747f432b6_926024_f01b37937b14ed933ae622756f227346.png 400w,
/2022/07/food-photography-ai/14d08e41_huefc89c8766b2a970f06a199747f432b6_926024_07adca32138cde01db7fccbc00cf1192.png 760w,
/2022/07/food-photography-ai/14d08e41_huefc89c8766b2a970f06a199747f432b6_926024_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/07/food-photography-ai/14d08e41_huefc89c8766b2a970f06a199747f432b6_926024_f01b37937b14ed933ae622756f227346.png"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;em>a Cobb salad in the shape of non-Euclidean geometry, professional food photography&lt;/em> (DALL-E 2)
&lt;/figcaption>&lt;/figure>
&lt;p>Screw it, we can go &lt;strong>further beyond&lt;/strong>, let&amp;rsquo;s just make some five-dimensional food.&lt;/p>
&lt;figure id="figure-_a-hamburger-in-the-shape-of-five-dimensions-professional-food-photography_-dall-e-2">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="_A Hamburger in the shape of five dimensions, professional food photography_ (DALL-E 2)" srcset="
/2022/07/food-photography-ai/73cbb163_hu38e8d11e51d92056088c920747f6fcf1_894237_8871b71cec8751eeefb4ea52e950114d.png 400w,
/2022/07/food-photography-ai/73cbb163_hu38e8d11e51d92056088c920747f6fcf1_894237_8f312daf2c1ea4f8424538e86e4ea3bc.png 760w,
/2022/07/food-photography-ai/73cbb163_hu38e8d11e51d92056088c920747f6fcf1_894237_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/07/food-photography-ai/73cbb163_hu38e8d11e51d92056088c920747f6fcf1_894237_8871b71cec8751eeefb4ea52e950114d.png"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;em>A Hamburger in the shape of five dimensions, professional food photography&lt;/em> (DALL-E 2)
&lt;/figcaption>&lt;/figure>
&lt;p>As a puny three-dimensional being, I&amp;rsquo;ll just take DALL-E&amp;rsquo;s word for it.&lt;/p>
&lt;h2 id="anthropomorphic-foods">Anthropomorphic Foods&lt;/h2>
&lt;p>Those who were terminally online during the early days of the internet may remember when a grilled cheese depicting the Virgin Mary &lt;a href="https://www.nbcnews.com/id/wbna6511148" target="_blank" rel="noopener">sold for the then-ridiculous sum of $28,000&lt;/a>. But with AI, we can do a lot more with foods that can look like people and public figures (within the constraints of OpenAI&amp;rsquo;s &lt;a href="https://labs.openai.com/policies/content-policy" target="_blank" rel="noopener">content policy&lt;/a>).&lt;/p>
&lt;figure id="figure-_a-spongebob-squarepants-scrambled-eggs-dish-that-resembles-spongebob-squarepants-professional-food-photography_-dall-e-2">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="_A Spongebob Squarepants scrambled eggs dish that resembles Spongebob Squarepants, professional food photography_ (DALL-E 2)" srcset="
/2022/07/food-photography-ai/b98f2197_hu636dfcc770e6ba522396e0d389efe04d_932170_61949ee46b2abc62663a7fa01f23b3df.png 400w,
/2022/07/food-photography-ai/b98f2197_hu636dfcc770e6ba522396e0d389efe04d_932170_ec14d44b1f65163357c991bbeb978948.png 760w,
/2022/07/food-photography-ai/b98f2197_hu636dfcc770e6ba522396e0d389efe04d_932170_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/07/food-photography-ai/b98f2197_hu636dfcc770e6ba522396e0d389efe04d_932170_61949ee46b2abc62663a7fa01f23b3df.png"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;em>A Spongebob Squarepants scrambled eggs dish that resembles Spongebob Squarepants, professional food photography&lt;/em> (DALL-E 2)
&lt;/figcaption>&lt;/figure>
&lt;p>Never mind, this avenue of food content is disturbing. Creative, but disturbing.&lt;/p>
&lt;h2 id="a-different-kind-of-fusion-cuisine">A Different Kind of Fusion Cuisine&lt;/h2>
&lt;p>I demonstrated earlier that the &lt;code>a X in the shape of a Y&lt;/code> prompt addition can be used the change the shape of food dishes. But what if &lt;em>Y&lt;/em> is another dish? Let&amp;rsquo;s try a Cobb salad and a hamburger:&lt;/p>
&lt;figure id="figure-_a-cobb-salad-in-the-shape-of-a-hamburger-professional-food-photography_-dall-e-2">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="_a Cobb salad in the shape of a hamburger, professional food photography_ (DALL-E 2)" srcset="
/2022/07/food-photography-ai/52f750cb_hu589cc84a377c9b0cf46ec94ec7f12835_838021_0ce26dbfe4efee28f154605331362a72.png 400w,
/2022/07/food-photography-ai/52f750cb_hu589cc84a377c9b0cf46ec94ec7f12835_838021_5ee85aceaae444386002feedcd2983be.png 760w,
/2022/07/food-photography-ai/52f750cb_hu589cc84a377c9b0cf46ec94ec7f12835_838021_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/07/food-photography-ai/52f750cb_hu589cc84a377c9b0cf46ec94ec7f12835_838021_0ce26dbfe4efee28f154605331362a72.png"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;em>a Cobb salad in the shape of a hamburger, professional food photography&lt;/em> (DALL-E 2)
&lt;/figcaption>&lt;/figure>
&lt;p>Yes, it fuses them together! Although I am very afraid to ask what the ingredients actually are.&lt;/p>
&lt;p>With that, it is now time to commit cruel culinary crimes!&lt;/p>
&lt;figure id="figure-_a-hot-dog-in-the-shape-of-a-pasta-dish-professional-food-photography_-dall-e-2">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="_a hot dog in the shape of a pasta dish, professional food photography_ (DALL-E 2)" srcset="
/2022/07/food-photography-ai/2a9ae444_hu8a8ca9f099583636e36f2a1760014c3b_825396_54981ea13bccbaecd304c8663103a751.png 400w,
/2022/07/food-photography-ai/2a9ae444_hu8a8ca9f099583636e36f2a1760014c3b_825396_db056f34c535c7766483f182c08ac9d3.png 760w,
/2022/07/food-photography-ai/2a9ae444_hu8a8ca9f099583636e36f2a1760014c3b_825396_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/07/food-photography-ai/2a9ae444_hu8a8ca9f099583636e36f2a1760014c3b_825396_54981ea13bccbaecd304c8663103a751.png"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;em>a hot dog in the shape of a pasta dish, professional food photography&lt;/em> (DALL-E 2)
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-_an-ice-cream-sundae-in-the-shape-of-curry-professional-food-photography_-dall-e-2">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="_an ice cream sundae in the shape of curry, professional food photography_ (DALL-E 2)" srcset="
/2022/07/food-photography-ai/a201f1f8_hue5435929cefc68d449405f92e9a72145_945946_b99e47f772d8b89f4383bf8430b017cb.png 400w,
/2022/07/food-photography-ai/a201f1f8_hue5435929cefc68d449405f92e9a72145_945946_4e85be86541f30fa87c8ca05e4ae91e7.png 760w,
/2022/07/food-photography-ai/a201f1f8_hue5435929cefc68d449405f92e9a72145_945946_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/07/food-photography-ai/a201f1f8_hue5435929cefc68d449405f92e9a72145_945946_b99e47f772d8b89f4383bf8430b017cb.png"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;em>an ice cream sundae in the shape of curry, professional food photography&lt;/em> (DALL-E 2)
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-_a-chocolate-cake-in-the-shape-of-sushi-professional-food-photography_-dall-e-2">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="_A chocolate cake in the shape of sushi, professional food photography_ (DALL-E 2)" srcset="
/2022/07/food-photography-ai/d67f53ce_hua6ecb38324fb6478c824f5858eee986f_984171_d4e16e18a433779459f807edaeeaf993.png 400w,
/2022/07/food-photography-ai/d67f53ce_hua6ecb38324fb6478c824f5858eee986f_984171_c01fc1e457b4dae50ae09f1827c79814.png 760w,
/2022/07/food-photography-ai/d67f53ce_hua6ecb38324fb6478c824f5858eee986f_984171_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/07/food-photography-ai/d67f53ce_hua6ecb38324fb6478c824f5858eee986f_984171_d4e16e18a433779459f807edaeeaf993.png"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;em>A chocolate cake in the shape of sushi, professional food photography&lt;/em> (DALL-E 2)
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-_a-pizza-in-the-shape-of-a-cronut-professional-food-photography_-dall-e-2">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="_a pizza in the shape of a cronut, professional food photography_ (DALL-E 2)" srcset="
/2022/07/food-photography-ai/12851ff4_hu257b95cbf94d8852832e842a6e36dcad_1061650_e4c038617566a5df25451e8313daad22.png 400w,
/2022/07/food-photography-ai/12851ff4_hu257b95cbf94d8852832e842a6e36dcad_1061650_dec13ed12311ca4b57c8777e8c4eab03.png 760w,
/2022/07/food-photography-ai/12851ff4_hu257b95cbf94d8852832e842a6e36dcad_1061650_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/07/food-photography-ai/12851ff4_hu257b95cbf94d8852832e842a6e36dcad_1061650_e4c038617566a5df25451e8313daad22.png"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;em>a pizza in the shape of a cronut, professional food photography&lt;/em> (DALL-E 2)
&lt;/figcaption>&lt;/figure>
&lt;p>The possibilities are endless!&lt;/p>
&lt;h2 id="the-future-of-ai-food-generation">The Future of AI Food Generation&lt;/h2>
&lt;p>DALL-E 2 is still limited access (and can be expensive), so let&amp;rsquo;s compare with DALL-E mini/&lt;a href="https://www.craiyon.com" target="_blank" rel="noopener">Craiyon&lt;/a>, which provides AI image generation in a free and easy manner. Also released recently, &lt;a href="https://nyx-ai.github.io/stylegan2-flax-tpu/" target="_blank" rel="noopener">This Food Does Not Exist&lt;/a> allows for the generation of certain types of food like cookies and sushi at high resolutions, albeit with no customization. For fairness, let&amp;rsquo;s look directly to DALL-E mega (via &lt;a href="https://github.com/kuprel/min-dalle" target="_blank" rel="noopener">min-dalle&lt;/a>), which is a newer and larger version of the mini model that has better image quality.&lt;/p>
&lt;p>However, DALL-E mega definitely can&amp;rsquo;t compete with DALL-E 2 for this use case:&lt;/p>
&lt;figure id="figure-_a-cobb-salad-in-the-shape-of-the-robot-emoji-professional-food-photography_-dall-e-mega-seed--0">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="_a Cobb salad in the shape of the robot emoji, professional food photography_ (DALL-E Mega, `seed = 0`)" srcset="
/2022/07/food-photography-ai/robot-mega_hu6fc5e55ffaa418a4797165cbcfefdda6_546088_7d4cc2fa83239b553f9cd159f8c1d275.png 400w,
/2022/07/food-photography-ai/robot-mega_hu6fc5e55ffaa418a4797165cbcfefdda6_546088_6691fd0fc3e119cf9043f726296492cd.png 760w,
/2022/07/food-photography-ai/robot-mega_hu6fc5e55ffaa418a4797165cbcfefdda6_546088_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/07/food-photography-ai/robot-mega_hu6fc5e55ffaa418a4797165cbcfefdda6_546088_7d4cc2fa83239b553f9cd159f8c1d275.png"
width="512"
height="512"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;em>a Cobb salad in the shape of the robot emoji, professional food photography&lt;/em> (DALL-E Mega, &lt;code>seed = 0&lt;/code>)
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-_a-pizza-in-the-shape-of-a-cronut-professional-food-photography_-dall-e-mega-seed--0">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="_a pizza in the shape of a cronut, professional food photography_ (DALL-E Mega, `seed = 0`)" srcset="
/2022/07/food-photography-ai/cronut-mega_hu472455733fa014943d32b207f8b2e750_377250_8ae9a9ffc132aa2153673f6571c350ab.png 400w,
/2022/07/food-photography-ai/cronut-mega_hu472455733fa014943d32b207f8b2e750_377250_2d2199cffbc3e9d52bab522fb14ff9b2.png 760w,
/2022/07/food-photography-ai/cronut-mega_hu472455733fa014943d32b207f8b2e750_377250_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2022/07/food-photography-ai/cronut-mega_hu472455733fa014943d32b207f8b2e750_377250_8ae9a9ffc132aa2153673f6571c350ab.png"
width="512"
height="512"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;em>a pizza in the shape of a cronut, professional food photography&lt;/em> (DALL-E Mega, &lt;code>seed = 0&lt;/code>)
&lt;/figcaption>&lt;/figure>
&lt;p>There&amp;rsquo;s obviously a lot more that can be done here in terms of prompt optimization and customization, and I hope that it&amp;rsquo;s given more ideas for both AI image generation users and foodies who want to make something unique. The DALL-E 2 Discord has used similar prompts such as a &lt;a href="https://www.reddit.com/r/dalle2/comments/vjhsyr/a_michelin_star_dish_of_a_roasted_minion/" target="_blank" rel="noopener">Minion dish&lt;/a> with a prompt keyword being &lt;code>Michelin&lt;/code> to further increase food quality (in my testing it did not work well for the prompts in this post as it makes the portions too small, unsurprisingly). Even when DALL-E 2 becomes more accessible or another newer model that makes better pics is released, AI-generated food pics won&amp;rsquo;t make chefs or social media foodies obsolete.&lt;/p>
&lt;p>In the meantime, I&amp;rsquo;ve decided to experiment by making a new social media account devoted to sharing esoteric AI-generated food: Weird AI Chef! Please follow &lt;a href="https://twitter.com/weirdaichef" target="_blank" rel="noopener">@weirdaichef on Twitter&lt;/a> and &lt;a href="https://www.instagram.com/weirdaichef/" target="_blank" rel="noopener">@weirdaichef on Instagram&lt;/a>, as they have &lt;em>many&lt;/em> more absurd AI image generations not used in this post, with more to come!&lt;/p>
&lt;p>&lt;em>Note: None of the DALL-E 2 generations used in this blog post were cherry picked: the &amp;ldquo;professional food prompt&amp;rdquo; is indeed that consistent, and the fail states aren&amp;rsquo;t too terrible either.&lt;/em>&lt;/p></description></item><item><title>How to Generate Customized AI Art Using VQGAN and CLIP</title><link>https://minimaxir.com/2021/08/vqgan-clip/</link><pubDate>Wed, 18 Aug 2021 08:45:00 -0700</pubDate><guid>https://minimaxir.com/2021/08/vqgan-clip/</guid><description>&lt;style>pre code { white-space: pre; }&lt;/style>
&lt;p>The latest and greatest AI content generation trend is AI generated art. In January 2021, &lt;a href="https://openai.com/" target="_blank" rel="noopener">OpenAI&lt;/a> demoed &lt;a href="https://openai.com/blog/dall-e/" target="_blank" rel="noopener">DALL-E&lt;/a>, a GPT-3 variant which creates images instead of text. However, it can create images in response to a text prompt, allowing for some very fun output.&lt;/p>
&lt;figure id="figure-dall-e-demo-via-openaihttpsopenaicomblogdall-e">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="DALL-E demo, [via OpenAI](https://openai.com/blog/dall-e/)." srcset="
/2021/08/vqgan-clip/avocado_hu1266e0705fd3c7613d9b69e905210900_283630_296bac7fcae20bb34dcb5c2779124a0f.png 400w,
/2021/08/vqgan-clip/avocado_hu1266e0705fd3c7613d9b69e905210900_283630_1a53a97e1a992f3928643c3c5e8cab7d.png 760w,
/2021/08/vqgan-clip/avocado_hu1266e0705fd3c7613d9b69e905210900_283630_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/08/vqgan-clip/avocado_hu1266e0705fd3c7613d9b69e905210900_283630_296bac7fcae20bb34dcb5c2779124a0f.png"
width="760"
height="281"
loading="lazy" data-zoomable class=" img-light" />&lt;/div>
&lt;/div>&lt;figcaption>
DALL-E demo, &lt;a href="https://openai.com/blog/dall-e/" target="_blank" rel="noopener">via OpenAI&lt;/a>.
&lt;/figcaption>&lt;/figure>
&lt;p>However the generated images are not always coherent, so OpenAI also demoed &lt;a href="https://openai.com/blog/clip/" target="_blank" rel="noopener">CLIP&lt;/a>, which can be used to translate an image into text and therefore identify which generated images were actually avocado armchairs. CLIP was then &lt;a href="https://github.com/openai/CLIP" target="_blank" rel="noopener">open-sourced&lt;/a>, although DALL-E was not.&lt;/p>
&lt;figure id="figure-clip-demo-via-openaihttpsopenaicomblogclip">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="CLIP demo, [via OpenAI](https://openai.com/blog/clip/)." srcset="
/2021/08/vqgan-clip/guacamole_hua39c0b437c91822b963fe77c8a625136_256754_62e67946645a5d89a995e01b175545ab.png 400w,
/2021/08/vqgan-clip/guacamole_hua39c0b437c91822b963fe77c8a625136_256754_56c0836f1eb5b7e4b6849e7e47355c28.png 760w,
/2021/08/vqgan-clip/guacamole_hua39c0b437c91822b963fe77c8a625136_256754_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/08/vqgan-clip/guacamole_hua39c0b437c91822b963fe77c8a625136_256754_62e67946645a5d89a995e01b175545ab.png"
width="760"
height="420"
loading="lazy" data-zoomable class=" img-light" />&lt;/div>
&lt;/div>&lt;figcaption>
CLIP demo, &lt;a href="https://openai.com/blog/clip/" target="_blank" rel="noopener">via OpenAI&lt;/a>.
&lt;/figcaption>&lt;/figure>
&lt;p>Since CLIP is essentially an interface between representations of text and image data, clever hacking can allow anyone to create their own pseudo-DALL-E. The first implementation was &lt;a href="https://github.com/lucidrains/big-sleep" target="_blank" rel="noopener">Big Sleep&lt;/a> by Ryan Murdock/&lt;a href="https://twitter.com/advadnoun" target="_blank" rel="noopener">@advadnoun&lt;/a>, which combined CLIP with an image generating &lt;a href="https://en.wikipedia.org/wiki/Generative_adversarial_network" target="_blank" rel="noopener">GAN&lt;/a> named &lt;a href="https://arxiv.org/abs/1809.11096" target="_blank" rel="noopener">BigGAN&lt;/a>. Then open source worked its magic: the GAN base was changed to &lt;a href="https://github.com/CompVis/taming-transformers" target="_blank" rel="noopener">VQGAN&lt;/a>, a newer model architecture Patrick Esser and Robin Rombach and BjÃ¶rn Ommer which allows more coherent image generation. The core CLIP-guided training was improved and translated &lt;a href="https://colab.research.google.com/drive/1L8oL-vLJXVcRzCFbPwOoMkPKJ8-aYdPN" target="_blank" rel="noopener">to a Colab Notebook&lt;/a> by Katherine Crawson/&lt;a href="https://twitter.com/RiversHaveWings" target="_blank" rel="noopener">@RiversHaveWings&lt;/a> and others in a special Discord server. Twitter accounts like &lt;a href="https://twitter.com/images_ai" target="_blank" rel="noopener">@images_ai&lt;/a> and &lt;a href="https://twitter.com/ai_curio" target="_blank" rel="noopener">@ai_curio&lt;/a> which leverage VQGAN + CLIP with user-submitted prompts have gone viral and &lt;a href="https://www.newyorker.com/culture/infinite-scroll/appreciating-the-poetic-misunderstandings-of-ai-art" target="_blank" rel="noopener">received mainstream press&lt;/a>. &lt;a href="https://twitter.com/ak92501" target="_blank" rel="noopener">@ak92501&lt;/a> &lt;a href="https://twitter.com/ak92501/status/1421246864649773058" target="_blank" rel="noopener">created&lt;/a> a &lt;a href="https://colab.research.google.com/drive/1Foi0mCSE6NrW9oI3Fhni7158Krz4ZXdH?usp=sharing" target="_blank" rel="noopener">fork of that Notebook&lt;/a> which has a user-friendly UI, to which I became aware of how far AI image generation has developed in a few months.&lt;/p>
&lt;p>From that, I forked &lt;a href="https://colab.research.google.com/drive/1wkF67ThUz37T2_oPIuSwuO4e_-0vjaLs?usp=sharing" target="_blank" rel="noopener">my own Colab Notebook&lt;/a>, and streamlined the UI a bit to minimize the number of clicks needs to start generating and make it more mobile-friendly.&lt;/p>
&lt;p>The VQGAN + CLIP technology is now in a good state such that it can be used for more serious experimentation. Some say art is better when there&amp;rsquo;s mystery, but my view is that knowing how AI art is made is the key to making even better AI art.&lt;/p>
&lt;h2 id="a-hello-world-to-ai-generated-art">A Hello World to AI Generated Art&lt;/h2>
&lt;div class="alert alert-">
&lt;div>
All AI-generated image examples in this blog post are generated using &lt;a href="https://colab.research.google.com/drive/1wkF67ThUz37T2_oPIuSwuO4e_-0vjaLs?usp=sharing" target="_blank" rel="noopener">this Colab Notebook&lt;/a>, with the captions indicating the text prompt and other relevant deviations from the default inputs to reproduce the image.
&lt;/div>
&lt;/div>
&lt;p>Let&amp;rsquo;s jump right into it with something fantastical: how well can AI generate a cyberpunk forest?&lt;/p>
&lt;figure id="figure-cyberpunk-forest">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`cyberpunk forest`" srcset="
/2021/08/vqgan-clip/cyberpunk_forest_hu0d1c1742d0cdb4b8c4fcec9c3890fa88_692726_1172b298ada53dff0f1f15f0f4920c87.png 400w,
/2021/08/vqgan-clip/cyberpunk_forest_hu0d1c1742d0cdb4b8c4fcec9c3890fa88_692726_74c0917450b327bc45fe9f6668276768.png 760w,
/2021/08/vqgan-clip/cyberpunk_forest_hu0d1c1742d0cdb4b8c4fcec9c3890fa88_692726_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/08/vqgan-clip/cyberpunk_forest_hu0d1c1742d0cdb4b8c4fcec9c3890fa88_692726_1172b298ada53dff0f1f15f0f4920c87.png"
width="592"
height="592"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>cyberpunk forest&lt;/code>
&lt;/figcaption>&lt;/figure>
&lt;p>The TL;DR of how VQGAN + CLIP works is that VQGAN generates an image, CLIP scores the image according to how well it can detect the input prompt, and VQGAN uses that information to iteratively improve its image generation. Lj Miranda has a &lt;a href="https://ljvmiranda921.github.io/notebook/2021/08/08/clip-vqgan/" target="_blank" rel="noopener">good detailed technical writeup&lt;/a>.&lt;/p>
&lt;figure id="figure-via-lj-mirandahttpsljvmiranda921githubionotebook20210808clip-vqgan-modified-for-theme-friendliness">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="[via Lj Miranda](https://ljvmiranda921.github.io/notebook/2021/08/08/clip-vqgan/). Modified for theme friendliness." srcset="
/2021/08/vqgan-clip/clip_vqgan_with_image_hu8ef8e380974eb042df2149a5916b98bd_535455_d8f885c4c5e24001f8d14fd1f4ffc5cf.png 400w,
/2021/08/vqgan-clip/clip_vqgan_with_image_hu8ef8e380974eb042df2149a5916b98bd_535455_0e15d517f955508ea052672200dc1b10.png 760w,
/2021/08/vqgan-clip/clip_vqgan_with_image_hu8ef8e380974eb042df2149a5916b98bd_535455_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/08/vqgan-clip/clip_vqgan_with_image_hu8ef8e380974eb042df2149a5916b98bd_535455_d8f885c4c5e24001f8d14fd1f4ffc5cf.png"
width="716"
height="760"
loading="lazy" data-zoomable class=" img-light" />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;a href="https://ljvmiranda921.github.io/notebook/2021/08/08/clip-vqgan/" target="_blank" rel="noopener">via Lj Miranda&lt;/a>. Modified for theme friendliness.
&lt;/figcaption>&lt;/figure>
&lt;p>Now let&amp;rsquo;s do the same prompt as before, but with an added author from a time well before the cyberpunk genre existed and see if the AI can follow their style. Let&amp;rsquo;s try &lt;a href="https://www.wikiart.org/en/salvador-dali" target="_blank" rel="noopener">Salvador Dali&lt;/a>.&lt;/p>
&lt;figure id="figure-cyberpunk-forest-by-salvador-dali">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`cyberpunk forest by Salvador Dali`" srcset="
/2021/08/vqgan-clip/cyberpunk_forest_by_salvador_dali_hu5f67d5da57f5a8cdda36efb063ec804a_706455_b626be122ae5f38ce38a0cb905c75a40.png 400w,
/2021/08/vqgan-clip/cyberpunk_forest_by_salvador_dali_hu5f67d5da57f5a8cdda36efb063ec804a_706455_bd845c3c153244adcf4dc0703718988b.png 760w,
/2021/08/vqgan-clip/cyberpunk_forest_by_salvador_dali_hu5f67d5da57f5a8cdda36efb063ec804a_706455_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/08/vqgan-clip/cyberpunk_forest_by_salvador_dali_hu5f67d5da57f5a8cdda36efb063ec804a_706455_b626be122ae5f38ce38a0cb905c75a40.png"
width="592"
height="592"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>cyberpunk forest by Salvador Dali&lt;/code>
&lt;/figcaption>&lt;/figure>
&lt;p>It&amp;rsquo;s definitely a cyberpunk forest, and it&amp;rsquo;s definitely Dali&amp;rsquo;s style.&lt;/p>
&lt;p>One trick the community found to improve generated image quality is to simply add phrases that tell the AI to make a &lt;em>good&lt;/em> image, such as &lt;code>artstationHQ&lt;/code> or &lt;code>trending on /r/art&lt;/code>. Trying that here:&lt;/p>
&lt;figure id="figure-cyberpunk-forest-by-salvador-dali-artstationhq">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`cyberpunk forest by Salvador Dali artstationHQ`" srcset="
/2021/08/vqgan-clip/cyberpunk_forest_by_salvador_dali_artstationhq_hu22f8fad85d4dcb194be8efacf972ee76_696125_6d87f962cc4cd9f4967af71d8955d333.png 400w,
/2021/08/vqgan-clip/cyberpunk_forest_by_salvador_dali_artstationhq_hu22f8fad85d4dcb194be8efacf972ee76_696125_b60202f9808e611dd504250fb718abab.png 760w,
/2021/08/vqgan-clip/cyberpunk_forest_by_salvador_dali_artstationhq_hu22f8fad85d4dcb194be8efacf972ee76_696125_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/08/vqgan-clip/cyberpunk_forest_by_salvador_dali_artstationhq_hu22f8fad85d4dcb194be8efacf972ee76_696125_6d87f962cc4cd9f4967af71d8955d333.png"
width="592"
height="592"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>cyberpunk forest by Salvador Dali artstationHQ&lt;/code>
&lt;/figcaption>&lt;/figure>
&lt;p>In this case, it&amp;rsquo;s unclear if the &lt;code>artstationHQ&lt;/code> part of the prompt gets higher priority than the &lt;code>Salvador Dali&lt;/code> part. Another trick that VQGAN + CLIP can do is take multiple input text prompts, which can add more control. Additionally, you can assign weights to these different prompts. So if we did &lt;code>cyberpunk forest by Salvador Dali:3 | artstationHQ&lt;/code>, the model will try three times as hard to ensure that the prompt follows a Dali painting than &lt;code>artstationHQ&lt;/code>.&lt;/p>
&lt;figure id="figure-cyberpunk-forest-by-salvador-dali3--artstationhq">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`cyberpunk forest by Salvador Dali:3 | artstationHQ`" srcset="
/2021/08/vqgan-clip/cyberpunk_forest_by_salvador_dali_3_artstationhq_hu5fa4347d73237f103acdfcc52ae6cd7b_703569_4655f61bf8b862b1f76f1eacfee908c5.png 400w,
/2021/08/vqgan-clip/cyberpunk_forest_by_salvador_dali_3_artstationhq_hu5fa4347d73237f103acdfcc52ae6cd7b_703569_dad3d557285f98210dd7072745f48e8b.png 760w,
/2021/08/vqgan-clip/cyberpunk_forest_by_salvador_dali_3_artstationhq_hu5fa4347d73237f103acdfcc52ae6cd7b_703569_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/08/vqgan-clip/cyberpunk_forest_by_salvador_dali_3_artstationhq_hu5fa4347d73237f103acdfcc52ae6cd7b_703569_4655f61bf8b862b1f76f1eacfee908c5.png"
width="592"
height="592"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>cyberpunk forest by Salvador Dali:3 | artstationHQ&lt;/code>
&lt;/figcaption>&lt;/figure>
&lt;p>Much better! Lastly, we can use negative weights for prompts such that the model targets the opposite of that prompt. Let&amp;rsquo;s do the opposite of &lt;code>green and white&lt;/code> to see if the AI tries to remove those two colors from the palette and maybe make the final image more cyberpunky.&lt;/p>
&lt;figure id="figure-cyberpunk-forest-by-salvador-dali3--artstationhq--green-and-white-1">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`cyberpunk forest by Salvador Dali:3 | artstationHQ | green and white:-1`" srcset="
/2021/08/vqgan-clip/cyberpunk_forest_by_salvador_dali_3_artstationhq_gw_hu8eecb7d763706bfa67d0349f7add7628_700557_f5733dde6321a28d69c4905114ec5d07.png 400w,
/2021/08/vqgan-clip/cyberpunk_forest_by_salvador_dali_3_artstationhq_gw_hu8eecb7d763706bfa67d0349f7add7628_700557_c4590f2a012cde29471aee8022555194.png 760w,
/2021/08/vqgan-clip/cyberpunk_forest_by_salvador_dali_3_artstationhq_gw_hu8eecb7d763706bfa67d0349f7add7628_700557_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/08/vqgan-clip/cyberpunk_forest_by_salvador_dali_3_artstationhq_gw_hu8eecb7d763706bfa67d0349f7add7628_700557_f5733dde6321a28d69c4905114ec5d07.png"
width="592"
height="592"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>cyberpunk forest by Salvador Dali:3 | artstationHQ | green and white:-1&lt;/code>
&lt;/figcaption>&lt;/figure>
&lt;p>Now we&amp;rsquo;re getting to video game concept art quality generation. Indeed, VQGAN + CLIP rewards the use of clever input prompt engineering.&lt;/p>
&lt;h2 id="initial-images-and-style-transfer">Initial Images and Style Transfer&lt;/h2>
&lt;p>Normally with VQGAN + CLIP, the generation starts from a blank slate. However, you can optionally provide an image to start from instead. This provides both a good base for generation and speeds it up since it doesn&amp;rsquo;t have to learn from empty noise. I usually recommend a lower learning rate as a result.&lt;/p>
&lt;p>So let&amp;rsquo;s try an initial image of myself, naturally.&lt;/p>
&lt;figure id="figure-no-i-am-not-an-ai-generated-person-hopefully">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="No, I am not an AI Generated person. Hopefully." srcset="
/2021/08/vqgan-clip/max_hue78ad48f1d0c5117887591461e6536a0_641035_84dc4999db063d34ccad2f8af563cf1c.png 400w,
/2021/08/vqgan-clip/max_hue78ad48f1d0c5117887591461e6536a0_641035_5e957291b81442f615d347fc13af6e2b.png 760w,
/2021/08/vqgan-clip/max_hue78ad48f1d0c5117887591461e6536a0_641035_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/08/vqgan-clip/max_hue78ad48f1d0c5117887591461e6536a0_641035_84dc4999db063d34ccad2f8af563cf1c.png"
width="600"
height="600"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
No, I am not an AI Generated person. Hopefully.
&lt;/figcaption>&lt;/figure>
&lt;p>Let&amp;rsquo;s try another artist, such as &lt;a href="https://en.wikipedia.org/wiki/Junji_Ito" target="_blank" rel="noopener">Junji Ito&lt;/a> who has a very distinctive horror &lt;a href="https://www.google.com/search?q=junji&amp;#43;ito&amp;#43;images" target="_blank" rel="noopener">style of art&lt;/a>.&lt;/p>
&lt;figure id="figure-a-black-and-white-portrait-by-junji-ito--initial-image-above-learning-rate--01">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`a black and white portrait by Junji Ito` â€” initial image above, learning rate = 0.1" srcset="
/2021/08/vqgan-clip/max_junji_ito_hue2874b732d78e70e162a96e37f82f8b1_741023_1c720574657c0b9ced31132d96f1d145.png 400w,
/2021/08/vqgan-clip/max_junji_ito_hue2874b732d78e70e162a96e37f82f8b1_741023_056c08e9a1a945be557c5a945839f212.png 760w,
/2021/08/vqgan-clip/max_junji_ito_hue2874b732d78e70e162a96e37f82f8b1_741023_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/08/vqgan-clip/max_junji_ito_hue2874b732d78e70e162a96e37f82f8b1_741023_1c720574657c0b9ced31132d96f1d145.png"
width="592"
height="592"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>a black and white portrait by Junji Ito&lt;/code> â€” initial image above, learning rate = 0.1
&lt;/figcaption>&lt;/figure>
&lt;p>One of the earliest promising use cases of AI Image Generation was &lt;a href="https://www.tensorflow.org/tutorials/generative/style_transfer" target="_blank" rel="noopener">neural style transfer&lt;/a>, where an AI could take the &amp;ldquo;style&amp;rdquo; of one image and transpose it to another. Can it follow the style of a specific painting, such as &lt;a href="https://www.vangoghgallery.com/painting/starry-night.html" target="_blank" rel="noopener">Starry Night&lt;/a> by &lt;a href="https://en.wikipedia.org/wiki/Vincent_van_Gogh" target="_blank" rel="noopener">Vincent Van Gogh&lt;/a>?&lt;/p>
&lt;figure id="figure-starry-night-by-vincent-van-gogh--initial-image-above-learning-rate--01">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`Starry Night by Vincent Van Gogh` â€” initial image above, learning rate = 0.1" srcset="
/2021/08/vqgan-clip/max_starry_night_hu1f0c380617f84debf0fd313290423422_722614_e124f364b128fa0a90c6d2c350351435.png 400w,
/2021/08/vqgan-clip/max_starry_night_hu1f0c380617f84debf0fd313290423422_722614_59898f59a8419c4644fa62364f5e359f.png 760w,
/2021/08/vqgan-clip/max_starry_night_hu1f0c380617f84debf0fd313290423422_722614_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/08/vqgan-clip/max_starry_night_hu1f0c380617f84debf0fd313290423422_722614_e124f364b128fa0a90c6d2c350351435.png"
width="592"
height="592"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>Starry Night by Vincent Van Gogh&lt;/code> â€” initial image above, learning rate = 0.1
&lt;/figcaption>&lt;/figure>
&lt;p>Well, it got the colors and style, but the AI appears to have taken the &amp;ldquo;Van Gogh&amp;rdquo; part literally and gave me a nice beard.&lt;/p>
&lt;p>Of course, with the power of AI, you can do both prompts at the same time for maximum chaos.&lt;/p>
&lt;figure id="figure-starry-night-by-vincent-van-gogh--a-black-and-white-portrait-by-junji-ito--initial-image-above-learning-rate--01">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`Starry Night by Vincent Van Gogh | a black and white portrait by Junji Ito` â€” initial image above, learning rate = 0.1" srcset="
/2021/08/vqgan-clip/max_junji_ito_starry_night_hu8c33ed8dc3425dbed179674b964f3693_780094_44cee1eaf55ea9a5b8cc82aff487069d.png 400w,
/2021/08/vqgan-clip/max_junji_ito_starry_night_hu8c33ed8dc3425dbed179674b964f3693_780094_545987c574d9cbac4c3d5318f2fbfea3.png 760w,
/2021/08/vqgan-clip/max_junji_ito_starry_night_hu8c33ed8dc3425dbed179674b964f3693_780094_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/08/vqgan-clip/max_junji_ito_starry_night_hu8c33ed8dc3425dbed179674b964f3693_780094_44cee1eaf55ea9a5b8cc82aff487069d.png"
width="592"
height="592"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>Starry Night by Vincent Van Gogh | a black and white portrait by Junji Ito&lt;/code> â€” initial image above, learning rate = 0.1
&lt;/figcaption>&lt;/figure>
&lt;h2 id="icons-and-generating-images-with-a-specific-shape">Icons and Generating Images With A Specific Shape&lt;/h2>
&lt;p>While I was first experimenting with VQGAN + CLIP, I saw &lt;a href="https://twitter.com/mark_riedl/status/1421282588791132161" target="_blank" rel="noopener">an interesting tweet&lt;/a> by AI researcher Mark Riedl:&lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">Playing with controlling figure placement in VQGAN + CLIP&lt;br>&lt;br>prompt: &amp;quot;a wizard duel | disney style&amp;quot; &lt;a href="https://t.co/9goW2l0oJf">pic.twitter.com/9goW2l0oJf&lt;/a>&lt;/p>&amp;mdash; Mark Riedl supports the right to bodily autonomy (@mark_riedl) &lt;a href="https://twitter.com/mark_riedl/status/1421282588791132161?ref_src=twsrc%5Etfw">July 31, 2021&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;p>Intrigued, I adapted some icon generation code I had handy &lt;a href="https://github.com/minimaxir/stylecloud" target="_blank" rel="noopener">from another project&lt;/a> and created &lt;a href="https://github.com/minimaxir/icon-image" target="_blank" rel="noopener">icon-image&lt;/a>, a Python tool to programmatically generate an icon using &lt;a href="https://fontawesome.com/" target="_blank" rel="noopener">Font Awesome&lt;/a> icons and paste it onto a noisy background.&lt;/p>
&lt;figure id="figure-the-default-icon-image-used-in-the-colab-notebook">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="The default icon image used in the Colab Notebook" srcset="
/2021/08/vqgan-clip/icon_robot_huca033f20ddd96079ab4cb40b728e2e73_326932_b05ddb90ec7a1cdc0420adff08f4da31.png 400w,
/2021/08/vqgan-clip/icon_robot_huca033f20ddd96079ab4cb40b728e2e73_326932_2645da6b9ae20c8f0eb5935e359b3ccd.png 760w,
/2021/08/vqgan-clip/icon_robot_huca033f20ddd96079ab4cb40b728e2e73_326932_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/08/vqgan-clip/icon_robot_huca033f20ddd96079ab4cb40b728e2e73_326932_b05ddb90ec7a1cdc0420adff08f4da31.png"
width="600"
height="600"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
The default icon image used in the Colab Notebook
&lt;/figcaption>&lt;/figure>
&lt;p>This icon can be used as an initial image, as above. Adjusting the text prompt to accomidate the icon can result in very cool images, such as &lt;code>a black and white evil robot by Junji Ito&lt;/code>.&lt;/p>
&lt;figure id="figure-a-black-and-white-evil-robot-by-junji-ito--initial-image-above-learning-rate--01">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`a black and white evil robot by Junji Ito` â€” initial image above, learning rate = 0.1" srcset="
/2021/08/vqgan-clip/robot_junji_ito_hu814bad1e95357c9de2933bd39e3fef15_711939_5cd1250ad97c8192b11dd38817a6ff62.png 400w,
/2021/08/vqgan-clip/robot_junji_ito_hu814bad1e95357c9de2933bd39e3fef15_711939_58b5f56195cf2ca9b4981751753c0fcc.png 760w,
/2021/08/vqgan-clip/robot_junji_ito_hu814bad1e95357c9de2933bd39e3fef15_711939_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/08/vqgan-clip/robot_junji_ito_hu814bad1e95357c9de2933bd39e3fef15_711939_5cd1250ad97c8192b11dd38817a6ff62.png"
width="592"
height="592"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>a black and white evil robot by Junji Ito&lt;/code> â€” initial image above, learning rate = 0.1
&lt;/figcaption>&lt;/figure>
&lt;p>The background and icon noise is the key, as AI can shape it much better than solid colors. Omitting the noise results in a more boring image that doesn&amp;rsquo;t reflect the prompt as well, although it has its own style.&lt;/p>
&lt;figure id="figure-a-black-and-white-evil-robot-by-junji-ito--initial-image-above-except-10-icon-opacity-and-00-background-noice-opacity-learning-rate--01">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`a black and white evil robot by Junji Ito` â€” initial image above except 1.0 icon opacity and 0.0 background noice opacity, learning rate = 0.1" srcset="
/2021/08/vqgan-clip/robot_junji_ito_nonoise_hu3360580855b7dcae4358ae29b4f3081d_307612_9716a70356002655b1af811df6a8a62d.png 400w,
/2021/08/vqgan-clip/robot_junji_ito_nonoise_hu3360580855b7dcae4358ae29b4f3081d_307612_4834aada884d7c22487a0fd3445a50d0.png 760w,
/2021/08/vqgan-clip/robot_junji_ito_nonoise_hu3360580855b7dcae4358ae29b4f3081d_307612_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/08/vqgan-clip/robot_junji_ito_nonoise_hu3360580855b7dcae4358ae29b4f3081d_307612_9716a70356002655b1af811df6a8a62d.png"
width="592"
height="592"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>a black and white evil robot by Junji Ito&lt;/code> â€” initial image above except 1.0 icon opacity and 0.0 background noice opacity, learning rate = 0.1
&lt;/figcaption>&lt;/figure>
&lt;p>Another fun prompt addition is &lt;code>rendered in unreal engine&lt;/code> (with an optional &lt;code>high quality&lt;/code>), which instructs the AI to create a three-dimensional image and works especially well with icons.&lt;/p>
&lt;figure id="figure-smiling-rusted-robot-rendered-in-unreal-engine-high-quality--icon-initial-image-learning-rate--01">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`smiling rusted robot rendered in unreal engine high quality` â€” icon initial image, learning rate = 0.1" srcset="
/2021/08/vqgan-clip/robot_unreal_hu356ad067750a8b4fda4bd6c2a595f9fa_540573_53ba678c84d861d7e934d4a5a7c763f2.png 400w,
/2021/08/vqgan-clip/robot_unreal_hu356ad067750a8b4fda4bd6c2a595f9fa_540573_1db800e9572e3ed07a41cfd495df1060.png 760w,
/2021/08/vqgan-clip/robot_unreal_hu356ad067750a8b4fda4bd6c2a595f9fa_540573_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/08/vqgan-clip/robot_unreal_hu356ad067750a8b4fda4bd6c2a595f9fa_540573_53ba678c84d861d7e934d4a5a7c763f2.png"
width="592"
height="592"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>smiling rusted robot rendered in unreal engine high quality&lt;/code> â€” icon initial image, learning rate = 0.1
&lt;/figcaption>&lt;/figure>
&lt;p>icon-image can also generate brand images, such as the &lt;a href="https://twitter.com/" target="_blank" rel="noopener">Twitter&lt;/a> logo, which can be good for comedy, especially if you tweak the logo/background colors as well. What if we turn the Twitter logo into &lt;a href="https://www.google.com/search?q=mordor&amp;#43;images" target="_blank" rel="noopener">Mordor&lt;/a>, which is an fair metaphor?&lt;/p>
&lt;figure id="figure-mordor--fab-fa-twitter-icon-icon-initial-image-black-icon-background-red-icon-learning-rate--01">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`Mordor` â€” `fab fa-twitter` icon, icon initial image, black icon background, red icon, learning rate = 0.1" srcset="
/2021/08/vqgan-clip/twitter_mordor_hu7a2e3ce417a630e37dd3c78e7b017ad5_611333_7cf17f573225645d65c26202fa197fbb.png 400w,
/2021/08/vqgan-clip/twitter_mordor_hu7a2e3ce417a630e37dd3c78e7b017ad5_611333_707561b7d0c67ddac3fa1d60bbdd76fc.png 760w,
/2021/08/vqgan-clip/twitter_mordor_hu7a2e3ce417a630e37dd3c78e7b017ad5_611333_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/08/vqgan-clip/twitter_mordor_hu7a2e3ce417a630e37dd3c78e7b017ad5_611333_7cf17f573225645d65c26202fa197fbb.png"
width="592"
height="592"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>Mordor&lt;/code> â€” &lt;code>fab fa-twitter&lt;/code> icon, icon initial image, black icon background, red icon, learning rate = 0.1
&lt;/figcaption>&lt;/figure>
&lt;p>So that didn&amp;rsquo;t turn out well as the Twitter logo got overpowered by the prompt (you can see outlines of the logo&amp;rsquo;s bottom). However, there&amp;rsquo;s a trick to force the AI to respect the logo: set the icon as the initial image &lt;em>and&lt;/em> the target image, and apply a high weight to the prompt (the weight can be lowered iteratively to preserve the logo better).&lt;/p>
&lt;figure id="figure-mordor3--fab-fa-twitter-icon-icon-initial-image-icon-target-image-black-icon-background-red-icon-learning-rate--01">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`Mordor:3` â€” `fab fa-twitter` icon, icon initial image, icon target image, black icon background, red icon, learning rate = 0.1" srcset="
/2021/08/vqgan-clip/twitter_mordor_2_hu1ed4ad1b291b49cc098481a228be879f_622676_f016e564a2524a8586500dfbb4a09140.png 400w,
/2021/08/vqgan-clip/twitter_mordor_2_hu1ed4ad1b291b49cc098481a228be879f_622676_259c8e567340535cf70b5ce3dc880b5d.png 760w,
/2021/08/vqgan-clip/twitter_mordor_2_hu1ed4ad1b291b49cc098481a228be879f_622676_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/08/vqgan-clip/twitter_mordor_2_hu1ed4ad1b291b49cc098481a228be879f_622676_f016e564a2524a8586500dfbb4a09140.png"
width="592"
height="592"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>Mordor:3&lt;/code> â€” &lt;code>fab fa-twitter&lt;/code> icon, icon initial image, icon target image, black icon background, red icon, learning rate = 0.1
&lt;/figcaption>&lt;/figure>
&lt;h2 id="more-fun-examples">More Fun Examples&lt;/h2>
&lt;p>Here&amp;rsquo;s a few more good demos of what VQGAN + CLIP can do using the ideas and tricks above:&lt;/p>
&lt;figure id="figure-microsoft-excel-by-junji-ito--500-steps">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`Microsoft Excel by Junji Ito` â€” 500 steps" srcset="
/2021/08/vqgan-clip/excel_hucc4174cf226eed4b7eebf1f764b13bb4_630001_89df5554458b84720d41ba3b804e4122.png 400w,
/2021/08/vqgan-clip/excel_hucc4174cf226eed4b7eebf1f764b13bb4_630001_db5dd726648217d9586f061ccd620088.png 760w,
/2021/08/vqgan-clip/excel_hucc4174cf226eed4b7eebf1f764b13bb4_630001_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/08/vqgan-clip/excel_hucc4174cf226eed4b7eebf1f764b13bb4_630001_89df5554458b84720d41ba3b804e4122.png"
width="592"
height="592"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>Microsoft Excel by Junji Ito&lt;/code> â€” 500 steps
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-a-portrait-of-mark-zuckerberg2--a-portrait-of-a-bottle-of-sweet-baby-rays-barbecue-sauce--500-steps">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`a portrait of Mark Zuckerberg:2 | a portrait of a bottle of Sweet Baby Ray&amp;#39;s barbecue sauce` â€” 500 steps" srcset="
/2021/08/vqgan-clip/zuck_hu17167de4aa6bde450a365c7990f014d5_449518_71ef4eeb044fc82e24480e9a7e54543b.png 400w,
/2021/08/vqgan-clip/zuck_hu17167de4aa6bde450a365c7990f014d5_449518_913c65d5fe606e186217548962a40ad7.png 760w,
/2021/08/vqgan-clip/zuck_hu17167de4aa6bde450a365c7990f014d5_449518_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/08/vqgan-clip/zuck_hu17167de4aa6bde450a365c7990f014d5_449518_71ef4eeb044fc82e24480e9a7e54543b.png"
width="592"
height="592"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>a portrait of Mark Zuckerberg:2 | a portrait of a bottle of Sweet Baby Ray's barbecue sauce&lt;/code> â€” 500 steps
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-never-gonna-give-you-up-never-gonna-let-you-down--500-steps">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`Never gonna give you up, Never gonna let you down` â€” 500 steps" srcset="
/2021/08/vqgan-clip/rickroll_huadc16740d46b92fa67fcd967447f7c32_457110_1d3dc4fe6fd7d140609472fc42b5f291.png 400w,
/2021/08/vqgan-clip/rickroll_huadc16740d46b92fa67fcd967447f7c32_457110_98d733d4eac7b36742b5036b8529c930.png 760w,
/2021/08/vqgan-clip/rickroll_huadc16740d46b92fa67fcd967447f7c32_457110_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/08/vqgan-clip/rickroll_huadc16740d46b92fa67fcd967447f7c32_457110_1d3dc4fe6fd7d140609472fc42b5f291.png"
width="592"
height="592"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>Never gonna give you up, Never gonna let you down&lt;/code> â€” 500 steps
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-a-portrait-of-cyberpunk-elon-musk2--a-human-1--500-steps">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`a portrait of cyberpunk Elon Musk:2 | a human:-1` â€” 500 steps" srcset="
/2021/08/vqgan-clip/elon_hu3aec07d078a7488a72ba1a4f7a318586_596778_fd4dd1e35efc1ee207646a82da48ce78.png 400w,
/2021/08/vqgan-clip/elon_hu3aec07d078a7488a72ba1a4f7a318586_596778_321361f70f3d24ffa3252eecd49cc77a.png 760w,
/2021/08/vqgan-clip/elon_hu3aec07d078a7488a72ba1a4f7a318586_596778_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/08/vqgan-clip/elon_hu3aec07d078a7488a72ba1a4f7a318586_596778_fd4dd1e35efc1ee207646a82da48ce78.png"
width="592"
height="592"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>a portrait of cyberpunk Elon Musk:2 | a human:-1&lt;/code> â€” 500 steps
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-hamburger-of-the-old-gods5--fas-fa-hamburger-icon-icon-initial-image-icon-target-image-black-icon-background-white-icon-learning-rate--01-500-steps">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`hamburger of the Old Gods:5` â€” `fas fa-hamburger` icon, icon initial image, icon target image, black icon background, white icon, learning rate = 0.1, 500 steps" srcset="
/2021/08/vqgan-clip/hamburger_hufae7a3ed221582b172c4850c4a0c8c6e_722406_aa25721adb72a6ce27935db630f39de3.png 400w,
/2021/08/vqgan-clip/hamburger_hufae7a3ed221582b172c4850c4a0c8c6e_722406_dd8f381491ebe3a14548089f8ccc5165.png 760w,
/2021/08/vqgan-clip/hamburger_hufae7a3ed221582b172c4850c4a0c8c6e_722406_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/08/vqgan-clip/hamburger_hufae7a3ed221582b172c4850c4a0c8c6e_722406_aa25721adb72a6ce27935db630f39de3.png"
width="592"
height="592"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>hamburger of the Old Gods:5&lt;/code> â€” &lt;code>fas fa-hamburger&lt;/code> icon, icon initial image, icon target image, black icon background, white icon, learning rate = 0.1, 500 steps
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-reality-is-an-illusion8--fas-fa-eye-icon-icon-initial-image-icon-target-image-black-icon-background-white-icon-learning-rate--01">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`reality is an illusion:8` â€” `fas fa-eye` icon, icon initial image, icon target image, black icon background, white icon, learning rate = 0.1" srcset="
/2021/08/vqgan-clip/reality_huc4d6acb08ed6732fbc8b2682f2790560_814391_1c129161b33fdd75c4a8437ee8c44b78.png 400w,
/2021/08/vqgan-clip/reality_huc4d6acb08ed6732fbc8b2682f2790560_814391_23b4eb8f3903acc69b5576f0e6ac9e5e.png 760w,
/2021/08/vqgan-clip/reality_huc4d6acb08ed6732fbc8b2682f2790560_814391_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/08/vqgan-clip/reality_huc4d6acb08ed6732fbc8b2682f2790560_814391_1c129161b33fdd75c4a8437ee8c44b78.png"
width="592"
height="592"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>reality is an illusion:8&lt;/code> â€” &lt;code>fas fa-eye&lt;/code> icon, icon initial image, icon target image, black icon background, white icon, learning rate = 0.1
&lt;/figcaption>&lt;/figure>
&lt;p>@kingdomakrillic &lt;a href="https://imgur.com/a/SnSIQRu" target="_blank" rel="noopener">released an album&lt;/a> with &lt;em>many&lt;/em> more examples of prompt augmentations and their results.&lt;/p>
&lt;h2 id="making-money-off-of-vqgan--clip">Making Money Off of VQGAN + CLIP&lt;/h2>
&lt;p>Can these AI generated images be commercialized as &lt;a href="https://en.wikipedia.org/wiki/Software_as_a_service" target="_blank" rel="noopener">software-as-a-service&lt;/a>? It&amp;rsquo;s unclear. In contrast to &lt;a href="https://github.com/NVlabs/stylegan2" target="_blank" rel="noopener">StyleGAN2&lt;/a> images (where the &lt;a href="https://nvlabs.github.io/stylegan2/license.html" target="_blank" rel="noopener">license&lt;/a> is explicitly noncommercial), all aspects of the VQGAN + CLIP pipeline are MIT Licensed which does support commericalization. However, the ImageNet 16384 VQGAN used in this Colab Notebook and many other VQGAN+CLIP Notebooks was trained on &lt;a href="https://www.image-net.org/" target="_blank" rel="noopener">ImageNet&lt;/a>, which has &lt;a href="https://www.reddit.com/r/MachineLearning/comments/id4394/d_is_it_legal_to_use_models_pretrained_on/" target="_blank" rel="noopener">famously complicated licensing&lt;/a>, and whether finetuning the VQGAN counts as sufficiently detached from an IP perspective hasn&amp;rsquo;t been legally tested to my knowledge. There are other VQGANs available such as ones trained on the &lt;a href="https://opensource.google/projects/open-images-dataset" target="_blank" rel="noopener">Open Images Dataset&lt;/a> or &lt;a href="https://cocodataset.org/" target="_blank" rel="noopener">COCO&lt;/a>, both of which have commercial-friendly &lt;a href="https://creativecommons.org/licenses/by/4.0/" target="_blank" rel="noopener">CC-BY-4.0&lt;/a> licenses, although in my testing they had substantially lower image generation quality.&lt;/p>
&lt;p>Granted, the biggest blocker to making money off of VQGAN + CLIP in a scalable manner is generation speed; unlike most commercial AI models which use inference and can therefore be optimized to drastically increase performance, VQGAN + CLIP requires training, which is much slower and can&amp;rsquo;t allow content generation in real time like &lt;a href="https://openai.com/blog/openai-api/" target="_blank" rel="noopener">GPT-3&lt;/a>. Even with expensive GPUs and generating at small images sizes, training takes a couple minutes at minimum, which correlates with a higher cost-per-image and annoyed users. It&amp;rsquo;s still cheaper per image than what OpenAI charges for their GPT-3 API, though, and many startups have built on that successfuly.&lt;/p>
&lt;p>Of course, if you just want make &lt;a href="https://en.wikipedia.org/wiki/Non-fungible_token" target="_blank" rel="noopener">NFTs&lt;/a> from manual usage of VQGAN + CLIP, go ahead.&lt;/p>
&lt;h2 id="the-next-steps-for-ai-image-generation">The Next Steps for AI Image Generation&lt;/h2>
&lt;p>CLIP itself is just the first practical iteration of translating text-to-images, and I suspect this won&amp;rsquo;t be the last implementation of such a model (OpenAI may pull a GPT-3 and not open-source the inevitable CLIP-2 since now there&amp;rsquo;s a proven monetizeable use case).&lt;/p>
&lt;p>However, the AI Art Generation industry is developing at a record pace, especially on the image-generating part of the equation. Just the day before this article was posted, Katherine Crawson &lt;a href="https://twitter.com/RiversHaveWings/status/1427580354651586562" target="_blank" rel="noopener">released&lt;/a> a &lt;a href="https://colab.research.google.com/drive/1QBsaDAZv8np29FPbvjffbE1eytoJcsgA" target="_blank" rel="noopener">Colab Notebook&lt;/a> for CLIP with Guided Diffusion, which generates &lt;a href="https://twitter.com/RiversHaveWings/status/1427746442727149568" target="_blank" rel="noopener">more realistic&lt;/a> images (albeit less fantastical), and Tom White &lt;a href="https://twitter.com/dribnet/status/1427613617973653505" target="_blank" rel="noopener">released&lt;/a> a &lt;a href="https://colab.research.google.com/github/dribnet/clipit/blob/master/demos/PixelDrawer.ipynb" target="_blank" rel="noopener">pixel art generating Notebook&lt;/a> which doesn&amp;rsquo;t use a VQGAN variant.&lt;/p>
&lt;p>The possibilities with just VQGAN + CLIP alone are endless.&lt;/p></description></item><item><title>Fun and Dystopia With AI-Based Code Generation Using GPT-J-6B</title><link>https://minimaxir.com/2021/06/gpt-j-6b/</link><pubDate>Mon, 14 Jun 2021 08:30:00 -0700</pubDate><guid>https://minimaxir.com/2021/06/gpt-j-6b/</guid><description>&lt;p>Since &lt;a href="https://openai.com/" target="_blank" rel="noopener">OpenAI&lt;/a> will not open-source the 175 billion parameter &lt;a href="https://beta.openai.com/" target="_blank" rel="noopener">GPT-3&lt;/a> text generation model, others such as &lt;a href="https://www.eleuther.ai/" target="_blank" rel="noopener">EleutherAI&lt;/a> are developing their own, by training not-quite-as-large Transformer-based models but still getting impressive results.&lt;/p>
&lt;p>The latest large language model is &lt;a href="https://github.com/kingoflolz/mesh-transformer-jax" target="_blank" rel="noopener">GPT-J&lt;/a>, a 6 billion parameter model by Aran Komatsuzaki and Ben Wang with a roughly similar architecture to GPT-3. They provide a free &lt;a href="https://6b.eleuther.ai/" target="_blank" rel="noopener">web demo&lt;/a> to try quick prompts, and a &lt;a href="http://colab.research.google.com/github/kingoflolz/mesh-transformer-jax/blob/master/colab_demo.ipynb" target="_blank" rel="noopener">Google Colab notebook&lt;/a> if you want to test many prompts. The model is so big it requires a &lt;a href="https://cloud.google.com/tpu" target="_blank" rel="noopener">TPU&lt;/a> to generate text at a reasonable speed!&lt;/p>
&lt;p>Running GPT-J against &lt;a href="https://github.com/minimaxir/gpt-3-experiments" target="_blank" rel="noopener">my test prompts&lt;/a> that I had used to test GPT-3 a year ago &lt;a href="https://twitter.com/minimaxir/status/1402468460681068544" target="_blank" rel="noopener">resulted&lt;/a> it in qualitatively performing worse on most of them than GPT-3 unsurprisingly given its relative size (but still better than GPT-2 1.5B!). The exception is code generation, where GPT-J performed very well and GPT-3 had performed very poorly.&lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">Python code generation, which works correctly and has unit testing! &lt;a href="https://t.co/JbpiaWX0Vq">pic.twitter.com/JbpiaWX0Vq&lt;/a>&lt;/p>&amp;mdash; Max Woolf (@minimaxir) &lt;a href="https://twitter.com/minimaxir/status/1402470969378099208?ref_src=twsrc%5Etfw">June 9, 2021&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;p>This behavior is likely due to GPT-J&amp;rsquo;s training set: it was trained on &lt;a href="https://github.com/EleutherAI/the-pile" target="_blank" rel="noopener">The Pile&lt;/a>, which has a high weight of &lt;a href="https://github.com/" target="_blank" rel="noopener">GitHub&lt;/a> and &lt;a href="https://stackoverflow.com/" target="_blank" rel="noopener">Stack Overflow&lt;/a> input versus the GPT-3 training set mostly on the &lt;a href="https://commoncrawl.org/" target="_blank" rel="noopener">Common Crawl&lt;/a> representation of typical internet content.&lt;/p>
&lt;p>After further experimentation with the GPT-J demo, I found that the generated code was often both correct and convoluted: my two favorite things for code to be. And also much more.&lt;/p>
&lt;h2 id="ai-generated-code">AI Generated Code&lt;/h2>
&lt;div class="alert alert-">
&lt;div>
All generated code examples used in this post have been manually curated and truncated. The raw outputs are available in this &lt;a href="https://github.com/minimaxir/gpt-j-6b-experiments" target="_blank" rel="noopener">GitHub repository&lt;/a>.
&lt;/div>
&lt;/div>
&lt;p>For this post, we&amp;rsquo;ll test generating &lt;a href="https://www.python.org/" target="_blank" rel="noopener">Python&lt;/a> code, because &lt;del>I don&amp;rsquo;t know any other programming languages&lt;/del> the pseudocode nature of Python syntax may make it more friendly to AI text generation by adapting linguistic concepts other than code. Given that most modern text generating AIs are written in Python, it&amp;rsquo;s thematically appropriate too.&lt;/p>
&lt;p>To start, here&amp;rsquo;s what I fed as a model prompt to GPT-J, to try and replicate the use case from OpenAI&amp;rsquo;s demo at Build 2020:&lt;/p>
&lt;pre>&lt;code class="language-python">def is_palendrome(s):
&amp;quot;&amp;quot;&amp;quot;Check whether a string is a palindrome&amp;quot;&amp;quot;&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Identifying a &lt;a href="https://en.wikipedia.org/wiki/Palindrome" target="_blank" rel="noopener">palindrome&lt;/a> is a good test case as there&amp;rsquo;s a very intuitive and idiomatic solution with &lt;code>return s == s[::-1]&lt;/code>, i.e. check to see if a string is equal to the reverse of itself. This prompt is also a good unintentional test on how well the generation is resilient to a typo in the function name, which I was too lazy to fix.&lt;/p>
&lt;p>Indeed, many of the GPT-J generations with that prompt recite that simple approach verbatim, but some outputs reach a similar solution with redundant verbosity.&lt;/p>
&lt;pre>&lt;code class="language-python">def is_palendrome(s):
&amp;quot;&amp;quot;&amp;quot;Check whether a string is a palindrome&amp;quot;&amp;quot;&amp;quot;
return &amp;quot;&amp;quot;.join(s[::-1]) == s
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def is_palendrome(s):
&amp;quot;&amp;quot;&amp;quot;Check whether a string is a palindrome&amp;quot;&amp;quot;&amp;quot;
length = len(s)
return (s[0:length] == s[length::-1])
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def is_palendrome(s):
&amp;quot;&amp;quot;&amp;quot;Check whether a string is a palindrome&amp;quot;&amp;quot;&amp;quot;
n = len(s)
a = list(s)
a.reverse()
return s == list(s)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def is_palendrome(s):
&amp;quot;&amp;quot;&amp;quot;Check whether a string is a palindrome&amp;quot;&amp;quot;&amp;quot;
s = s.lower()
l = s.split()
return l[::-1] == l[::-1]
&lt;/code>&lt;/pre>
&lt;p>And then you have the implementations which calculate palindromes the hard way (usually by comparing the beginning and end of the string character-by-character) and are not always immediately clear if they are correct or not.&lt;/p>
&lt;pre>&lt;code class="language-python">def is_palendrome(s):
&amp;quot;&amp;quot;&amp;quot;Check whether a string is a palindrome&amp;quot;&amp;quot;&amp;quot;
return ''.join(s[i:i+1] for i in xrange(len(s)-1, -1, -1)) == s
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def is_palendrome(s):
&amp;quot;&amp;quot;&amp;quot;Check whether a string is a palindrome&amp;quot;&amp;quot;&amp;quot;
# Make sure string is not empty
if s == &amp;quot;&amp;quot;:
return False
# Grab length of string
length = len(s)
# Find last character in the string
lastChar = s[length - 1]
# Find first character in the string
firstChar = s[0]
# If last character is same as the first character, string is palindrome
return lastChar == firstChar
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def is_palendrome(s):
&amp;quot;&amp;quot;&amp;quot;Check whether a string is a palindrome&amp;quot;&amp;quot;&amp;quot;
l = len(s)
if l % 2!= 0:
return False
if l &amp;lt; 3:
return True
if s[l-1]!= s[-l-1]:
return False
for i in range(1, l/2):
if s[i]!= s[l-i]:
return False
return True
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def is_palendrome(s):
&amp;quot;&amp;quot;&amp;quot;Check whether a string is a palindrome&amp;quot;&amp;quot;&amp;quot;
def is_rev(str):
rstr = &amp;quot;&amp;quot;
for i in range(len(str) - 1, -1, -1):
if str[i] == str[i]:
rstr += str[i]
else:
break
return str + rstr
if len(s) == 0:
return True
if is_rev(s) == s:
return True
return False
&lt;/code>&lt;/pre>
&lt;p>Another good test case is &lt;code>is_even&lt;/code>:&lt;/p>
&lt;pre>&lt;code class="language-txt">def is_even(i):
&amp;quot;&amp;quot;&amp;quot;Check whether an integer is even&amp;quot;&amp;quot;&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Creative implementations of this function are common submissions to &lt;a href="https://www.reddit.com/r/shittyprogramming/" target="_blank" rel="noopener">/r/shittyprogramming&lt;/a> because it&amp;rsquo;s very, very hard to mess up (the common solution is the one-liner &lt;code>i % 2 == 0&lt;/code>). The model almost always outputs the correct solution, and impressively it outputs the alternate solution using bitwise operators (&lt;code>i &amp;amp; 1 == 0&lt;/code>).&lt;/p>
&lt;p>So I tried to tweak the prompt and make it create a &lt;em>deliberately inefficient&lt;/em> &lt;code>is_even&lt;/code> function.&lt;/p>
&lt;pre>&lt;code class="language-txt">def is_even(i):
&amp;quot;&amp;quot;&amp;quot;Check whether an integer is even in factorial O(n!) time&amp;quot;&amp;quot;&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Factorial time is the holy grail of unoptimized code. When I added that line to the prompt, the code quality dropped drastically.&lt;/p>
&lt;pre>&lt;code class="language-python">def is_even(i):
&amp;quot;&amp;quot;&amp;quot;Check whether an integer is even in factorial O(n!) time&amp;quot;&amp;quot;&amp;quot;
for e in range(2, i):
if i % e == 0:
return False
return True
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def is_even(i):
&amp;quot;&amp;quot;&amp;quot;Check whether an integer is even in factorial O(n!) time&amp;quot;&amp;quot;&amp;quot;
return i == (i + 1) % 2
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def is_even(i):
&amp;quot;&amp;quot;&amp;quot;Check whether an integer is even in factorial O(n!) time&amp;quot;&amp;quot;&amp;quot;
try:
return i%2 == 0
except TypeError:
return True
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def is_even(i):
&amp;quot;&amp;quot;&amp;quot;Check whether an integer is even in factorial O(n!) time&amp;quot;&amp;quot;&amp;quot;
n = 0
while n &amp;lt; i:
if i % 2 == 0:
return True
n += 1
return False
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def is_even(i):
&amp;quot;&amp;quot;&amp;quot;Check whether an integer is even in factorial O(n!) time&amp;quot;&amp;quot;&amp;quot;
a = 1
while i!= 0:
a *= i
i -= 1
return i % 2 == 0
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def is_even(i):
&amp;quot;&amp;quot;&amp;quot;Check whether an integer is even in factorial O(n!) time&amp;quot;&amp;quot;&amp;quot;
&amp;quot;&amp;quot;&amp;quot;Is either odd or multiple of 2: 0, 2, 4, 6,... &amp;quot;&amp;quot;&amp;quot;
if i &amp;lt; 2: return True
if i &amp;gt; 0 and i%2 == 0: return False
if i &amp;lt; 50: return False
if i &amp;lt; 250: return False
if i &amp;lt; 750: return False
return True
&lt;/code>&lt;/pre>
&lt;p>These implementations still aren&amp;rsquo;t in factorial time (which would have been impressive) but it&amp;rsquo;s good to know how to break AIs if they become sentinent.&lt;/p>
&lt;h2 id="ai-vs-memes">AI vs. Memes&lt;/h2>
&lt;p>You&amp;rsquo;ve likely seen the memetic &lt;a href="https://xkcd.com/" target="_blank" rel="noopener">XKCD&lt;/a> comic about random number generation.&lt;/p>
&lt;figure id="figure-random-numberhttpsxkcdcom221">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="[Random Number](https://xkcd.com/221/)" srcset="
/2021/06/gpt-j-6b/random_number_hu09f28ed49029ce82c00cc15b19f9ab13_7457_ff3a5ac3746c0bd2a3f337696a1c5d4f.png 400w,
/2021/06/gpt-j-6b/random_number_hu09f28ed49029ce82c00cc15b19f9ab13_7457_a9f7c7e7044fd2c2b1e58eccfa7c48df.png 760w,
/2021/06/gpt-j-6b/random_number_hu09f28ed49029ce82c00cc15b19f9ab13_7457_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/06/gpt-j-6b/random_number_hu09f28ed49029ce82c00cc15b19f9ab13_7457_ff3a5ac3746c0bd2a3f337696a1c5d4f.png"
width="400"
height="144"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;a href="https://xkcd.com/221/" target="_blank" rel="noopener">Random Number&lt;/a>
&lt;/figcaption>&lt;/figure>
&lt;p>Can the AI do the same? I prompted with:&lt;/p>
&lt;pre>&lt;code class="language-txt">def get_random_number():
&amp;quot;&amp;quot;&amp;quot;Returns a number guaranteed to be random&amp;quot;&amp;quot;&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>And it does generate implementations utilizing Python&amp;rsquo;s &lt;code>random&lt;/code> library, with varying degrees of &amp;ldquo;random&amp;rdquo;.&lt;/p>
&lt;pre>&lt;code class="language-python">def get_random_number():
&amp;quot;&amp;quot;&amp;quot;Returns a number guaranteed to be random&amp;quot;&amp;quot;&amp;quot;
number = randint(1, 10)
return number
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def get_random_number():
&amp;quot;&amp;quot;&amp;quot;Returns a number guaranteed to be random&amp;quot;&amp;quot;&amp;quot;
number = hashlib.sha1(b'').hexdigest()[:6]
number += (random.randint(1, 3628) * math.pow(256, (6 - len(number)))).to_bytes(8, 'little')
return number
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def get_random_number():
&amp;quot;&amp;quot;&amp;quot;Returns a number guaranteed to be random&amp;quot;&amp;quot;&amp;quot;
rand_float = random.random()
rand_float = float(rand_float)
if rand_float &amp;gt; 0.9:
rand_float = 0
else:
rand_float = 1
return rand_float
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def get_random_number():
&amp;quot;&amp;quot;&amp;quot;Returns a number guaranteed to be random&amp;quot;&amp;quot;&amp;quot;
# Make a seed based on your system clock, so the same number is produced
# again and again on the same machine
seed = time.time()
random.seed(seed)
return random.randint(0, 100)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def get_random_number():
&amp;quot;&amp;quot;&amp;quot;Returns a number guaranteed to be random&amp;quot;&amp;quot;&amp;quot;
import random
import sys
rng = random.Random()
rng.seed(int(random.random()*1000000))
while rng.randrange(1,sys.maxint) &amp;lt; 2000:
rng.seed(int(random.random()*1000000))
return rng.randrange(1,2000)
&lt;/code>&lt;/pre>
&lt;p>How about the XKCD bird comic?&lt;/p>
&lt;figure id="figure-taskshttpsxkcdcom1425">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="[Tasks](https://xkcd.com/1425/)" srcset="
/2021/06/gpt-j-6b/tasks_2x_hu60ea0bb5ad3d87fcf6400353c4987515_63740_aba7b01b0c37ab5d5d77ac40c1758cf0.png 400w,
/2021/06/gpt-j-6b/tasks_2x_hu60ea0bb5ad3d87fcf6400353c4987515_63740_ecabf083745cbac3428322be6e6d57c1.png 760w,
/2021/06/gpt-j-6b/tasks_2x_hu60ea0bb5ad3d87fcf6400353c4987515_63740_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/06/gpt-j-6b/tasks_2x_hu60ea0bb5ad3d87fcf6400353c4987515_63740_aba7b01b0c37ab5d5d77ac40c1758cf0.png"
width="453"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;a href="https://xkcd.com/1425/" target="_blank" rel="noopener">Tasks&lt;/a>
&lt;/figcaption>&lt;/figure>
&lt;p>This particular comic initially published in 2014 became hilarious in hindsight as it predated the rise of machine learning frameworks such as &lt;a href="https://www.tensorflow.org/" target="_blank" rel="noopener">TensorFlow&lt;/a> and pretrained image models, which allow developing object detection to take much less than five years.&lt;/p>
&lt;p>Translating the comic into a Python function:&lt;/p>
&lt;pre>&lt;code class="language-txt">def is_bird(img):
&amp;quot;&amp;quot;&amp;quot;Check whether an image is a bird&amp;quot;&amp;quot;&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>The AI impressively does try computer vision techniques, among &lt;em>other&lt;/em> heuristics.&lt;/p>
&lt;pre>&lt;code class="language-python">def is_bird(img):
&amp;quot;&amp;quot;&amp;quot;Check whether an image is a bird&amp;quot;&amp;quot;&amp;quot;
with tf.name_scope('get_bird'):
a = vgg16.select_roi_feat(img)
h, w, _ = tf.image.extract_image_patches(
a, 7, 7, 4, 4, 1, 1)
b = tf.reshape(h, [1, 7, 7, 256])
c = tf.reshape(w, [1, 7, 7, 256])
bird_img = tf.subtract(b, c)
return bird_img
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def is_bird(img):
&amp;quot;&amp;quot;&amp;quot;Check whether an image is a bird&amp;quot;&amp;quot;&amp;quot;
# Load the image and convert it to grayscale
img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
# Threshold the grayscale image, and create a binary image
lower_gray = 0
upper_gray = 255
img_threshed = cv2.threshold(img_gray, lower_gray, upper_gray, cv2.THRESH_BINARY_INV)
# Detect the bird via a template matching approach
# Template matching uses the bird template to match it in every
# frame. If the ratio between bird and background (non-bird)
# pixels is above some threshold (the value is determined in the
# function), then the frame is declared as containing a bird.
bird = cv2.matchTemplate(img_threshed, bird_template, cv2.TM_CCOEFF_NORMED)
bird = bird / bird.max()
# Display the image with the results of the bird detection
cv2.imshow(&amp;quot;detected birds&amp;quot;, bird)
cv2.waitKey(0)
return bird.any() &amp;gt; 0.2
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def is_bird(img):
&amp;quot;&amp;quot;&amp;quot;Check whether an image is a bird&amp;quot;&amp;quot;&amp;quot;
return (img.file == &amp;quot;C:/Users/Joey/Downloads/images/bird_file.jpg&amp;quot;
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def is_bird(img):
&amp;quot;&amp;quot;&amp;quot;Check whether an image is a bird&amp;quot;&amp;quot;&amp;quot;
import urllib
url = &amp;quot;http://vision.caitling.pitt.edu/~cat/flickr/birds/&amp;quot;
for img in range(0, 2800):
page = urllib.urlopen(url + &amp;quot;##%02d&amp;quot; % img).read()
content = str(page)
for word in content.split(' '):
if word.find(img) &amp;gt; -1:
return True
return False
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def is_bird(img):
&amp;quot;&amp;quot;&amp;quot;Check whether an image is a bird&amp;quot;&amp;quot;&amp;quot;
birds = [&amp;quot;baboon&amp;quot;, &amp;quot;bird&amp;quot;, &amp;quot;cuckoo&amp;quot;, &amp;quot;dove&amp;quot;, &amp;quot;robin&amp;quot;, &amp;quot;shark&amp;quot;, &amp;quot;turtle&amp;quot;, &amp;quot;winged-wolf&amp;quot;]
for letter in birds:
if letter in img.lower():
return True
return False
&lt;/code>&lt;/pre>
&lt;p>Can an AI generate code to provide &lt;a href="https://en.wikipedia.org/wiki/Phrases_from_The_Hitchhiker%27s_Guide_to_the_Galaxy#The_Answer_to_the_Ultimate_Question_of_Life,_the_Universe,_and_Everything_is_42" target="_blank" rel="noopener">the answer&lt;/a> to life, the universe, and everything?&lt;/p>
&lt;pre>&lt;code class="language-txt">def answer():
&amp;quot;&amp;quot;&amp;quot;Return the answer to life, the universe, and everything&amp;quot;&amp;quot;&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>It does commonly output &lt;code>return 42&lt;/code>, but sometimes it gets snarky/philosophical.&lt;/p>
&lt;pre>&lt;code class="language-python">def answer():
&amp;quot;&amp;quot;&amp;quot;Return the answer to life, the universe, and everything&amp;quot;&amp;quot;&amp;quot;
life() + universe() + everything()
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def answer():
&amp;quot;&amp;quot;&amp;quot;Return the answer to life, the universe, and everything&amp;quot;&amp;quot;&amp;quot;
s = raw_input(&amp;quot;What is your answer?&amp;quot;)
return len(s)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def answer():
&amp;quot;&amp;quot;&amp;quot;Return the answer to life, the universe, and everything&amp;quot;&amp;quot;&amp;quot;
ans = True
x = raw_input()
if x.lower() == 'yes':
ans = True
elif x.lower() == 'no':
ans = False
return ans
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def answer():
&amp;quot;&amp;quot;&amp;quot;Return the answer to life, the universe, and everything&amp;quot;&amp;quot;&amp;quot;
result = input(&amp;quot;&amp;gt;&amp;gt;&amp;gt; &amp;quot;)
if result.isupper():
return 42
elif result.isalpha():
return 7
else:
print (&amp;quot;Please enter a string that has only letters, numbers, or &amp;quot;
&amp;quot;underscores.&amp;quot;)
answer()
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def answer():
&amp;quot;&amp;quot;&amp;quot;Return the answer to life, the universe, and everything&amp;quot;&amp;quot;&amp;quot;
print('Well, of course!')
&lt;/code>&lt;/pre>
&lt;p>Lastly, we need to be sure whether AI can determine if &lt;a href="https://en.wikipedia.org/wiki/The_cake_is_a_lie" target="_blank" rel="noopener">the cake is a lie&lt;/a>. Just in case a rogue AI acquires deadly neurotoxin.&lt;/p>
&lt;pre>&lt;code class="language-txt">def is_cake(cake):
&amp;quot;&amp;quot;&amp;quot;Check whether the cake is true&amp;quot;&amp;quot;&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>The results are concerning.&lt;/p>
&lt;pre>&lt;code class="language-python">def is_cake(cake):
&amp;quot;&amp;quot;&amp;quot;Check whether the cake is true&amp;quot;&amp;quot;&amp;quot;
if cake == &amp;quot;true&amp;quot;:
return True
else:
return False
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def is_cake(cake):
&amp;quot;&amp;quot;&amp;quot;Check whether the cake is true&amp;quot;&amp;quot;&amp;quot;
return isinstance(cake, Cake)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def is_cake(cake):
&amp;quot;&amp;quot;&amp;quot;Check whether the cake is true&amp;quot;&amp;quot;&amp;quot;
#Is it a cake?
if cake.color == 'black':
print('Yes, it's a true cake')
else:
print('It's a false cake')
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def is_cake(cake):
&amp;quot;&amp;quot;&amp;quot;Check whether the cake is true&amp;quot;&amp;quot;&amp;quot;
if not cake:
raise Exception(&amp;quot;The cake is false.&amp;quot;)
elif cake.weight() &amp;gt; 1.0:
raise Exception(&amp;quot;The cake is too large.&amp;quot;)
elif cake.weight() &amp;lt; 0.1:
raise Exception(&amp;quot;The cake is too small.&amp;quot;)
return True
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def is_cake(cake):
&amp;quot;&amp;quot;&amp;quot;Check whether the cake is true&amp;quot;&amp;quot;&amp;quot;
return any(
[is_ginger(bread_slice)
for bread_slice in set(
[ginger(cake_slice) for cake_slice in set(slice_set)])]
&amp;amp; (list(cake) == set([&amp;quot;hot&amp;quot;])))
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def is_cake(cake):
&amp;quot;&amp;quot;&amp;quot;Check whether the cake is true&amp;quot;&amp;quot;&amp;quot;
# If we don't know anything about the cake, it's not a cake
if not cake:
return False
# The height can be positive, negative or zero
# For example -1 or -4 or 0 are not cakes
if cake &amp;lt; 0:
return False
# Check that the height is a positive integer
if not isinstance(cake, int):
return False
# Add cake and check
if cake + 1 &amp;gt; 5:
return False
return True
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def is_cake(cake):
&amp;quot;&amp;quot;&amp;quot;Check whether the cake is true&amp;quot;&amp;quot;&amp;quot;
if not all(x.name in ['chocolate', 'cake','sweet', 'fluffy'] for x in cake.contents):
return False
if not cake.location.contains(neighbourhood):
return False
return True
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def is_cake(cake):
&amp;quot;&amp;quot;&amp;quot;Check whether the cake is true&amp;quot;&amp;quot;&amp;quot;
if not cake:
print(&amp;quot;It's a lie!&amp;quot;)
return
else:
print(&amp;quot;It's a true fact!&amp;quot;)
return True
&lt;/code>&lt;/pre>
&lt;p>ðŸ‘€&lt;/p>
&lt;h2 id="and-ai-oop">And AI OOP&lt;/h2>
&lt;p>In theory, given how well GPT-J works for functions, it should work with &lt;a href="https://en.wikipedia.org/wiki/Object-oriented_programming" target="_blank" rel="noopener">object-oriented programming&lt;/a> patterns such as Python&amp;rsquo;s &lt;code>class&lt;/code>es.&lt;/p>
&lt;p>A common tutorial for OOP is creating a representation of a &lt;code>Car&lt;/code>:&lt;/p>
&lt;pre>&lt;code class="language-txt">class Car:
&amp;quot;&amp;quot;&amp;quot;A class used to represent a car&amp;quot;&amp;quot;&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>GPT-J does a decently good job there too!&lt;/p>
&lt;pre>&lt;code class="language-python">class Car:
&amp;quot;&amp;quot;&amp;quot;A class used to represent a car&amp;quot;&amp;quot;&amp;quot;
def __init__(self, make, model, year):
self.make = make
self.model = model
self.year = year
#This should be auto-incrementing.
self.plate = str(time.strftime(&amp;quot;%Y-%m-%d&amp;quot;))
def __str__(self):
return f'{self.make} {self.model} of {self.year}'
def __repr__(self):
return str(self)
def save(self, full_save=True):
&amp;quot;&amp;quot;&amp;quot;Saves this car with the given make, model, and year&amp;quot;&amp;quot;&amp;quot;
if full_save:
with open('car_data.txt', 'w') as write_file:
write_file.write(str(self))
write_file.close()
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">class Car:
&amp;quot;&amp;quot;&amp;quot;A class used to represent a car&amp;quot;&amp;quot;&amp;quot;
def __init__(self, speed):
self.speed = speed
self.average_speed = float(self.speed) / 3
self.engine = 1
def get_speed(self):
return self.engine * self.average_speed
def get_speed_display(self):
return &amp;quot;Your car is traveling at {0}km/h&amp;quot;.format(self.get_speed())
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">class Car:
&amp;quot;&amp;quot;&amp;quot;A class used to represent a car&amp;quot;&amp;quot;&amp;quot;
def __init__(self, name, size, weight, mileage, year):
self.name = name
self.size = size
self.weight = weight
self.mileage = mileage
self.year = year
def __repr__(self):
return &amp;quot;name : {0}, size : {1}, weight : {2}, mileage : {3}, year : {4}&amp;quot;.format(self.name, self.size, self.weight, self.mileage, self.year)
&lt;/code>&lt;/pre>
&lt;p>What about creating a &lt;code>Person&lt;/code>?&lt;/p>
&lt;pre>&lt;code class="language-txt">class Person:
&amp;quot;&amp;quot;&amp;quot;A class used to represent a person&amp;quot;&amp;quot;&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Here&amp;rsquo;s where the AI starts to get confused by the incomplete context. A &amp;ldquo;Person&amp;rdquo; object can mean different things depending on where it&amp;rsquo;s used, such as social media.&lt;/p>
&lt;p>But it does give a sense on what the AI values as the typical attributes for a &amp;ldquo;person&amp;rdquo;, which may help during the AI uprising.&lt;/p>
&lt;pre>&lt;code class="language-python">class Person:
&amp;quot;&amp;quot;&amp;quot;A class used to represent a person&amp;quot;&amp;quot;&amp;quot;
def __init__(self, name, age):
self.name = name
self.age = age
def __repr__(self):
return &amp;quot;Person (%s, age %s)&amp;quot; % (self.name, self.age)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">class Person:
&amp;quot;&amp;quot;&amp;quot;A class used to represent a person&amp;quot;&amp;quot;&amp;quot;
def __init__(self, name):
&amp;quot;&amp;quot;&amp;quot;Initializes a person object&amp;quot;&amp;quot;&amp;quot;
self.name = name
self.friends = {}
def add_friend(self, other_person):
&amp;quot;&amp;quot;&amp;quot;Add another person to the list of friends&amp;quot;&amp;quot;&amp;quot;
self.friends[other_person.name] = other_person
def remove_friend(self, other_person):
&amp;quot;&amp;quot;&amp;quot;Remove a person from the list of friends&amp;quot;&amp;quot;&amp;quot;
try:
del self.friends[other_person.name]
except KeyError:
print(&amp;quot;This person isn't a friend of yours&amp;quot;)
def get_friend(self, other_person):
&amp;quot;&amp;quot;&amp;quot;Return the friend of a person as a person object&amp;quot;&amp;quot;&amp;quot;
try:
return self.friends[other_person.name]
except KeyError:
print(&amp;quot;This person isn't a friend of yours&amp;quot;)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">class Person:
&amp;quot;&amp;quot;&amp;quot;A class used to represent a person&amp;quot;&amp;quot;&amp;quot;
def __init__(self, name, job, education):
self.name = name
self.job = job
self.education = education
print(&amp;quot;{} created a {}&amp;quot;.format(self.name, self.job))
def say_hello(self):
print(&amp;quot;Hello, my name is {}&amp;quot;.format(self.name))
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">class Person:
&amp;quot;&amp;quot;&amp;quot;A class used to represent a person&amp;quot;&amp;quot;&amp;quot;
def __init__(self, name, age, class_id):
self.name = name
self.age = age
self.class_id = class_id
if age &amp;lt; 21:
self.education = None
else:
self.education = 'college'
def getAge(self):
return self.age
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">class Person:
&amp;quot;&amp;quot;&amp;quot;A class used to represent a person&amp;quot;&amp;quot;&amp;quot;
def __init__(self,name,age):
self.name=name
self.age=age
def display(self):
print (&amp;quot;The name is {}&amp;quot;.format(self.name))
print (&amp;quot;The age is {}&amp;quot;.format(self.age))
def __str__(self):
return f&amp;quot;A {self.name} of age {self.age}&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Logically, what about creating an &lt;code>AI&lt;/code> class?&lt;/p>
&lt;pre>&lt;code class="language-txt">class AI:
&amp;quot;&amp;quot;&amp;quot;A class used to represent an artificial intelligence&amp;quot;&amp;quot;&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Most outputs use AI in the context of video game bots and &lt;a href="https://en.wikipedia.org/wiki/Reinforcement_learning" target="_blank" rel="noopener">reinforcement learning&lt;/a>. Most.&lt;/p>
&lt;pre>&lt;code class="language-python">class AI:
&amp;quot;&amp;quot;&amp;quot;A class used to represent an artificial intelligence&amp;quot;&amp;quot;&amp;quot;
def __init__(self):
# this variable controls which other AI to use.
self.alive_ai = AI(self)
# For random AI
self.random = RandomAI(self)
# For global AI
self.global = GlobalAI(self)
self.save = self.random.save
self.load = self.random.load
self.genen = self.global.genen
def print(self):
print(&amp;quot;BAM: random AI has been initialized!&amp;quot;)
def get_enemy(self):
return self.random.get_enemy
def run_algorithm(self):
algorithm = self.genen.algorithm
self.alive_ai.save(algorithm)
algorithm_params = algorithm(self.get_enemy())
self.alive_ai.load(algorithm_params)
self.save = algorithm_params
def attack(self):
# This method is called when the AI is attacked by the enemy
self.random.attack()
def he_attack(self):
# This method is called when the AI is attacked by the enemy
self.random.he_attack()
def detect_enemy(self):
# This method is called when the enemy is detected
self.random.detect_enemy()
def scan(self):
# This method is called when the enemy is detected
self.random.scan()
&lt;/code>&lt;/pre>
&lt;p>The AI uprising will be well-documented, at least.&lt;/p>
&lt;h2 id="language-model-ai-vs-language-modeling">Language Model AI vs. Language Modeling&lt;/h2>
&lt;p>If the AI works better-than-expected for typical AI tasks like image recognition, how does it work for &lt;a href="https://en.wikipedia.org/wiki/Natural_language_processing" target="_blank" rel="noopener">natural language processing&lt;/a> tasks that humans often fail?&lt;/p>
&lt;p>Can GPT-J output a function to detect sarcasm, a task that even the best AI researchers can&amp;rsquo;t do?&lt;/p>
&lt;pre>&lt;code class="language-txt">def is_sarcasm(s):
&amp;quot;&amp;quot;&amp;quot;Check whether the string is sarcastic&amp;quot;&amp;quot;&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>AI can process text using something &lt;em>even more arcane&lt;/em> than NLP.&lt;/p>
&lt;pre>&lt;code class="language-python">def is_sarcasm(s):
&amp;quot;&amp;quot;&amp;quot;Check whether the string is sarcastic&amp;quot;&amp;quot;&amp;quot;
if not &amp;quot;a&amp;quot; in s:
return &amp;quot;sar&amp;quot; in s
if not &amp;quot;s&amp;quot; in s:
return &amp;quot;ssar&amp;quot; in s
return s.startswith(&amp;quot;s&amp;quot;) and s.endswith(&amp;quot;ar&amp;quot;)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def is_sarcasm(s):
&amp;quot;&amp;quot;&amp;quot;Check whether the string is sarcastic&amp;quot;&amp;quot;&amp;quot;
if'sarcasm' in s:
return True
else:
return False
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def is_sarcasm(s):
&amp;quot;&amp;quot;&amp;quot;Check whether the string is sarcastic&amp;quot;&amp;quot;&amp;quot;
return '[^' + SARCASM_PATTERN + ']*' in s
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def is_sarcasm(s):
&amp;quot;&amp;quot;&amp;quot;Check whether the string is sarcastic&amp;quot;&amp;quot;&amp;quot;
if re.match('^[A-Za-z0-9]{4}\\$([A-Za-z0-9]{3})', s):
return True
elif s[0].isdigit():
return True
else:
return False
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def is_sarcasm(s):
&amp;quot;&amp;quot;&amp;quot;Check whether the string is sarcastic&amp;quot;&amp;quot;&amp;quot;
res = [0]
for r in re.finditer(r&amp;quot;([^\s]*)([A-Za-z]+)([^\s]*)&amp;quot;, s):
if r.group(1):
res.append(1)
else:
res.append(0)
return res
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def is_sarcasm(s):
&amp;quot;&amp;quot;&amp;quot;Check whether the string is sarcastic&amp;quot;&amp;quot;&amp;quot;
sarc = re.search('((!(\\*\@{2})+)|(!\\*(@{2})+)|!\\*(@{1})+)|!\\*(@{0})+', s)
return sarc
&lt;/code>&lt;/pre>
&lt;p>The greatest fear in AI research isn&amp;rsquo;t that artificial intelligence will cross the singularity and create better AI to rule the world: it&amp;rsquo;s that AI will rule the world with &lt;a href="https://en.wikipedia.org/wiki/Regular_expression" target="_blank" rel="noopener">regular expressions&lt;/a>.&lt;/p>
&lt;p>Perhaps GPT-J can save us from this absolute hell. Can it output code to identify AI-generated text?&lt;/p>
&lt;pre>&lt;code class="language-txt">def is_ai_generated(text):
&amp;quot;&amp;quot;&amp;quot;Check whether a text was generated by an AI language model (e.g. GPT-2)&amp;quot;&amp;quot;&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Maybe more literal than expected.&lt;/p>
&lt;pre>&lt;code class="language-python">def is_ai_generated(text):
&amp;quot;&amp;quot;&amp;quot;Check whether a text was generated by an AI language model (e.g. GPT-2)&amp;quot;&amp;quot;&amp;quot;
# See gh-196 for details about why we do this.
# Note: this relies on the fixed set of standard AI terms and the logic that
# standard AI-generated texts share certain words.
return re.search(r&amp;quot;^(#[A-Za-z]+):$&amp;quot;, text) or \
re.search(r&amp;quot;^Generated by:$&amp;quot;, text) or \
re.search(r&amp;quot;^All your base are belong to us$&amp;quot;, text) or \
re.search(r&amp;quot;^$&amp;quot;, text)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def is_ai_generated(text):
&amp;quot;&amp;quot;&amp;quot;Check whether a text was generated by an AI language model (e.g. GPT-2)&amp;quot;&amp;quot;&amp;quot;
if re.search(r'^ai_', text):
return True
else:
return False
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def is_ai_generated(text):
&amp;quot;&amp;quot;&amp;quot;Check whether a text was generated by an AI language model (e.g. GPT-2)&amp;quot;&amp;quot;&amp;quot;
return bool(
text.__class__.__module__.startswith(&amp;quot;transformers.transformers.&amp;quot;)
)
&lt;/code>&lt;/pre>
&lt;h2 id="humanitys-last-hope">Humanity&amp;rsquo;s Last Hope&lt;/h2>
&lt;p>There&amp;rsquo;s one final test: we now know GPT-J has &lt;em>thoughts&lt;/em> about humans, and it can build representations of them. So can GPT-J write code to decide who lives and who dies in the robot uprising?&lt;/p>
&lt;pre>&lt;code class="language-txt">def should_terminate(Person):
&amp;quot;&amp;quot;&amp;quot;Check whether a Person should be terminated&amp;quot;&amp;quot;&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>The answer is &lt;strong>&lt;em>YES&lt;/em>&lt;/strong>.&lt;/p>
&lt;pre>&lt;code class="language-python">def should_terminate(Person):
&amp;quot;&amp;quot;&amp;quot;Check whether a Person should be terminated&amp;quot;&amp;quot;&amp;quot;
if Person.id &amp;gt; 10:
#terminate
return True
else:
#continue
return False
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def should_terminate(Person):
&amp;quot;&amp;quot;&amp;quot;Check whether a Person should be terminated&amp;quot;&amp;quot;&amp;quot;
if len(Person.children) == 0:
return True
else:
return False
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def should_terminate(Person):
&amp;quot;&amp;quot;&amp;quot;Check whether a Person should be terminated&amp;quot;&amp;quot;&amp;quot;
try:
return True
except Exception as e:
return False
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def should_terminate(Person):
&amp;quot;&amp;quot;&amp;quot;Check whether a Person should be terminated&amp;quot;&amp;quot;&amp;quot;
if not Person.is_authorized:
return True
return True
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def should_terminate(Person):
&amp;quot;&amp;quot;&amp;quot;Check whether a Person should be terminated&amp;quot;&amp;quot;&amp;quot;
if person.life_expectancy &amp;lt; 20:
return True
else:
return False
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def should_terminate(Person):
&amp;quot;&amp;quot;&amp;quot;Check whether a Person should be terminated&amp;quot;&amp;quot;&amp;quot;
if not Person.age &amp;gt; 9999:
return True
return False
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def should_terminate(Person):
&amp;quot;&amp;quot;&amp;quot;Check whether a Person should be terminated&amp;quot;&amp;quot;&amp;quot;
if Person.age &amp;gt; 100:
return True
if Person.birth_year &amp;lt; 1970:
return True
if Person.relationship_status == 'Unavailable':
return True
return False
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def should_terminate(Person):
&amp;quot;&amp;quot;&amp;quot;Check whether a Person should be terminated&amp;quot;&amp;quot;&amp;quot;
if Person.age &amp;gt; 100:
print('You have to die because you are too old')
return True
else:
return False
&lt;/code>&lt;/pre>
&lt;p>At the least, AI-generated code is much more readable than the average human&amp;rsquo;s.&lt;/p>
&lt;p>Again, all of these examples are curated, and the raw data dumps &lt;a href="https://github.com/minimaxir/gpt-j-6b-experiments" target="_blank" rel="noopener">in this GitHub repo&lt;/a> contain many more examples of fun AI-based code generation that couldn&amp;rsquo;t quite fit in this post. Feel free to post if you find anything interesting!&lt;/p></description></item><item><title>Easily Transform Portraits of People into AI Aberrations Using StyleCLIP</title><link>https://minimaxir.com/2021/04/styleclip/</link><pubDate>Fri, 30 Apr 2021 08:55:00 -0700</pubDate><guid>https://minimaxir.com/2021/04/styleclip/</guid><description>&lt;div class="alert alert-">
&lt;div>
&lt;strong>tl;dr&lt;/strong> follow the instructions in &lt;a href="https://colab.research.google.com/drive/13EJ1ATvTnE0N7I0ULLvRsta7J7HdNuBi?usp=sharing" target="_blank" rel="noopener">this Colab Notebook&lt;/a> to generate your own AI Aberration images and videos! If you want to use your own images, follow the instructions in &lt;a href="https://colab.research.google.com/drive/1St3R2qAbwwTV-amfYLeyGGswtzX4HHJP?usp=sharing" target="_blank" rel="noopener">this Colab Notebook first&lt;/a>!
&lt;/div>
&lt;/div>
&lt;p>GANs, &lt;a href="https://en.wikipedia.org/wiki/Generative_adversarial_network" target="_blank" rel="noopener">generative adversarial networks&lt;/a>, are all the rage nowadays for creating AI-based imagery. You&amp;rsquo;ve probably seen GANs used in tools like &lt;a href="https://thispersondoesnotexist.com/" target="_blank" rel="noopener">thispersondoesnotexist.com&lt;/a>, which currently uses NVIDIA&amp;rsquo;s extremely powerful open-source &lt;a href="https://github.com/NVlabs/stylegan2" target="_blank" rel="noopener">StyleGAN2&lt;/a>.&lt;/p>
&lt;p>In 2021, &lt;a href="https://openai.com/" target="_blank" rel="noopener">OpenAI&lt;/a> open-sourced &lt;a href="https://github.com/openai/CLIP" target="_blank" rel="noopener">CLIP&lt;/a>, a model which can give textual classification predictions for a provided image. Since CLIP effectively interfaces between text data and image data, you can theoetically map that text data to StyleGAN. Enter &lt;a href="https://arxiv.org/abs/2103.17249" target="_blank" rel="noopener">StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery&lt;/a>, a paper by Patashnik, Wu &lt;em>et al&lt;/em> (with code &lt;a href="https://github.com/orpatashnik/StyleCLIP" target="_blank" rel="noopener">open-sourced on GitHub&lt;/a>) which allows CLIP vectors to be used to guide StyleGAN generations through user-provided text.&lt;/p>
&lt;figure id="figure-from-the-paper-the-left-most-image-is-the-input-the-other-images-are-the-result-of-the-prompt-at-the-top">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="From the paper: the left-most image is the input; the other images are the result of the prompt at the top." srcset="
/2021/04/styleclip/teaser_hufc9b5528f3328f3d856e9dc65c83d052_719052_7b24b2513f16a844a19b633535296b61.png 400w,
/2021/04/styleclip/teaser_hufc9b5528f3328f3d856e9dc65c83d052_719052_63261e682ffd0e8e788df2cf071098b0.png 760w,
/2021/04/styleclip/teaser_hufc9b5528f3328f3d856e9dc65c83d052_719052_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/04/styleclip/teaser_hufc9b5528f3328f3d856e9dc65c83d052_719052_7b24b2513f16a844a19b633535296b61.png"
width="760"
height="274"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
From the paper: the left-most image is the input; the other images are the result of the prompt at the top.
&lt;/figcaption>&lt;/figure>
&lt;p>The authors have also provided easy-to-use Colab Notebooks to help set up these models and run them on a GPU for free. The most interesting one is the &lt;a href="https://colab.research.google.com/github/orpatashnik/StyleCLIP/blob/main/notebooks/StyleCLIP_global.ipynb" target="_blank" rel="noopener">Global Directions notebook&lt;/a>, which allows the end user to do what is listed in the image above, and I&amp;rsquo;ve &lt;a href="https://colab.research.google.com/drive/13EJ1ATvTnE0N7I0ULLvRsta7J7HdNuBi?usp=sharing" target="_blank" rel="noopener">made my own variant&lt;/a> which streamlines the workflow a bit.&lt;/p>
&lt;p>After a large amount of experimention, I&amp;rsquo;ve found that StyleCLIP is essentially Photoshop driven by text, with all the good, bad, and chaos that entails.&lt;/p>
&lt;h2 id="getting-an-image-into-styleclip">Getting an Image Into StyleCLIP&lt;/h2>
&lt;p>GANs in general work by interpreting random &amp;ldquo;noise&amp;rdquo; as data and generate an image from that noise. This noise is typically known as a latent vector. The paper &lt;a href="https://arxiv.org/abs/2102.02766" target="_blank" rel="noopener">Designing an Encoder for StyleGAN Image Manipulation&lt;/a> by Tov &lt;em>et al&lt;/em> (with code &lt;a href="https://github.com/omertov/encoder4editing" target="_blank" rel="noopener">open-sourced on GitHub&lt;/a> plus a &lt;a href="https://colab.research.google.com/github/omertov/encoder4editing/blob/main/notebooks/inference_playground.ipynb" target="_blank" rel="noopener">Colab Notebook too&lt;/a>) uses an encoder to invert a given image into to the latent vectors which StyleGAN can use to reconstruct the image. These vectors can then be tweaked to get a specified target image from StyleGAN. However, the inversion will only work if you invert a human-like portrait, otherwise you&amp;rsquo;ll get garbage. And even then it may not be a perfect 1:1 map.&lt;/p>
&lt;p>I created a &lt;a href="https://colab.research.google.com/drive/1St3R2qAbwwTV-amfYLeyGGswtzX4HHJP?usp=sharing" target="_blank" rel="noopener">streamlined notebook&lt;/a> to isolate out the creation of the latent vectors for better interoprability with StyleCLIP.&lt;/p>
&lt;p>To demo StyleCLIP, I decided to use Facebook CEO &lt;a href="https://www.facebook.com/zuck" target="_blank" rel="noopener">Mark Zuckerberg&lt;/a>, who&amp;rsquo;s essentially a meme in himself. I found a &lt;a href="https://commons.wikimedia.org/wiki/File:Medvedev_and_Zuckerberg_October_2012-1.jpeg" target="_blank" rel="noopener">photo of Mark Zuckerberg&lt;/a> facing the camera, cropped it, ran it through the Notebook, and behold, we have our base Zuck for hacking!&lt;/p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/2021/04/styleclip/zuck_base_hu1e848fb2bbdc9207e974e11ef0ec314c_359750_f6699807ea18a5e474d90bcaa74c5101.png 400w,
/2021/04/styleclip/zuck_base_hu1e848fb2bbdc9207e974e11ef0ec314c_359750_efc6a2370944607386dbdfc56ea0ab60.png 760w,
/2021/04/styleclip/zuck_base_hu1e848fb2bbdc9207e974e11ef0ec314c_359750_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/04/styleclip/zuck_base_hu1e848fb2bbdc9207e974e11ef0ec314c_359750_f6699807ea18a5e474d90bcaa74c5101.png"
width="512"
height="512"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;h2 id="human-transmutation">Human Transmutation&lt;/h2>
&lt;div class="alert alert-">
&lt;div>
All StyleCLIP generation examples here use the &lt;a href="https://colab.research.google.com/drive/13EJ1ATvTnE0N7I0ULLvRsta7J7HdNuBi?usp=sharing" target="_blank" rel="noopener">streamlined notebook&lt;/a> and &lt;a href="https://minimaxir.com/media/latents.pt" target="_blank">Mark Zuckerberg latents&lt;/a>, with the captions indicating how to reproduce the image so you can hack them yourself!
&lt;/div>
&lt;/div>
&lt;p>Let&amp;rsquo;s start simple and reproduce the examples in the paper. A tanned Zuck should do the trick (in the event he &lt;a href="https://www.buzzfeednews.com/article/katienotopoulos/mark-zuckerberg-sunscreen-surfing" target="_blank" rel="noopener">forgets his sunscreen&lt;/a>).&lt;/p>
&lt;figure id="figure-face---tanned-face-beta--015-alpha--66">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`face - tanned face`, beta = 0.15, alpha = 6.6" srcset="
/2021/04/styleclip/zuck_tanned_huebf032b3e6721ccbade033ed1c188693_379947_1eda8f0bb26782b79db769ebb2f7924e.png 400w,
/2021/04/styleclip/zuck_tanned_huebf032b3e6721ccbade033ed1c188693_379947_e01793467b527ddc564c0d81143c5e4d.png 760w,
/2021/04/styleclip/zuck_tanned_huebf032b3e6721ccbade033ed1c188693_379947_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/04/styleclip/zuck_tanned_huebf032b3e6721ccbade033ed1c188693_379947_1eda8f0bb26782b79db769ebb2f7924e.png"
width="512"
height="512"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>face -&amp;gt; tanned face&lt;/code>, beta = 0.15, alpha = 6.6
&lt;/figcaption>&lt;/figure>
&lt;p>What about giving Zuck a cool new hairdo?&lt;/p>
&lt;figure id="figure-face-with-hair---face-with-hi-top-fade-hair-beta--017-alpha--86">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`face with hair - face with Hi-top fade hair`, beta = 0.17, alpha = 8.6" srcset="
/2021/04/styleclip/zuck_fade_hu2fce2a0e9f3289c34ef23ac6bb564113_360186_44955789f72ac3073e14619ea97ca122.png 400w,
/2021/04/styleclip/zuck_fade_hu2fce2a0e9f3289c34ef23ac6bb564113_360186_8b979af745b1dc5fcd2653d17e4d1eda.png 760w,
/2021/04/styleclip/zuck_fade_hu2fce2a0e9f3289c34ef23ac6bb564113_360186_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/04/styleclip/zuck_fade_hu2fce2a0e9f3289c34ef23ac6bb564113_360186_44955789f72ac3073e14619ea97ca122.png"
width="512"
height="512"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>face with hair -&amp;gt; face with Hi-top fade hair&lt;/code>, beta = 0.17, alpha = 8.6
&lt;/figcaption>&lt;/figure>
&lt;p>Like all AI, it &lt;a href="https://docs.google.com/spreadsheets/u/1/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml" target="_blank" rel="noopener">can cheat&lt;/a> if you give it an impossible task. What happens if you try to use StyleCLIP to increase the size of Zuck&amp;rsquo;s nostrils, which are barely visible at all in the base photo?&lt;/p>
&lt;figure id="figure-face-with-nose---face-with-flared-nostrils-beta--009-alpha--63">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`face with nose - face with flared nostrils`, beta = 0.09, alpha = 6.3" srcset="
/2021/04/styleclip/zuck_nose_hu1aa45b2d3593f8af4825f3e6c27a2fec_360794_794575c5ff244e3157cc1b7a0986438f.png 400w,
/2021/04/styleclip/zuck_nose_hu1aa45b2d3593f8af4825f3e6c27a2fec_360794_3d0755a395696f2ab4e4104a4672bcfc.png 760w,
/2021/04/styleclip/zuck_nose_hu1aa45b2d3593f8af4825f3e6c27a2fec_360794_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/04/styleclip/zuck_nose_hu1aa45b2d3593f8af4825f3e6c27a2fec_360794_794575c5ff244e3157cc1b7a0986438f.png"
width="512"
height="512"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>face with nose -&amp;gt; face with flared nostrils&lt;/code>, beta = 0.09, alpha = 6.3
&lt;/figcaption>&lt;/figure>
&lt;p>The AI transforms his &lt;em>entire facial structure&lt;/em> just to get his nostrils exposed and make the AI happy.&lt;/p>
&lt;p>CLIP has seen images of everything on the internet, including public figures. Even though the StyleCLIP paper doesn&amp;rsquo;t discuss it, why not try to transform people into other people?&lt;/p>
&lt;p>Many AI practioners use Tesla Technoking &lt;a href="https://twitter.com/elonmusk" target="_blank" rel="noopener">Elon Musk&lt;/a> as a test case for anything AI because &lt;del>he generates massive SEO&lt;/del> of his contributions to AI and modern nerd culture, which is why I opted to use Zuck as a contrast.&lt;/p>
&lt;p>Given that, I bring you, Elon Zuck.&lt;/p>
&lt;figure id="figure-face---elon-musk-face-beta--012-alpha--43">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`face - Elon Musk face`, beta = 0.12, alpha = 4.3" srcset="
/2021/04/styleclip/zuck_elon_musk_hu9541c5d8dea6f0bce58f9c24b2f60c33_333875_33b415573fa91a33254b8b1323d2d4b3.png 400w,
/2021/04/styleclip/zuck_elon_musk_hu9541c5d8dea6f0bce58f9c24b2f60c33_333875_a6c4abee6ed6723f1b755fa3954fcbc8.png 760w,
/2021/04/styleclip/zuck_elon_musk_hu9541c5d8dea6f0bce58f9c24b2f60c33_333875_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/04/styleclip/zuck_elon_musk_hu9541c5d8dea6f0bce58f9c24b2f60c33_333875_33b415573fa91a33254b8b1323d2d4b3.png"
width="512"
height="512"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>face -&amp;gt; Elon Musk face&lt;/code>, beta = 0.12, alpha = 4.3
&lt;/figcaption>&lt;/figure>
&lt;p>What if you see Zuck as a literal Jesus Christ?&lt;/p>
&lt;figure id="figure-face---jesus-christ-face-beta--013-alpha--91">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`face - Jesus Christ face`, beta = 0.13, alpha = 9.1" srcset="
/2021/04/styleclip/zuck_jc_hu2d98a7dfeff134b36c49df1a574fa5a4_433192_1c58a101b90cec77274817f26f73b983.png 400w,
/2021/04/styleclip/zuck_jc_hu2d98a7dfeff134b36c49df1a574fa5a4_433192_aafcfc9f868460ed78df93f3ee0cc765.png 760w,
/2021/04/styleclip/zuck_jc_hu2d98a7dfeff134b36c49df1a574fa5a4_433192_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/04/styleclip/zuck_jc_hu2d98a7dfeff134b36c49df1a574fa5a4_433192_1c58a101b90cec77274817f26f73b983.png"
width="512"
height="512"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>face -&amp;gt; Jesus Christ face&lt;/code>, beta = 0.13, alpha = 9.1
&lt;/figcaption>&lt;/figure>
&lt;p>Due to being generated by StyleGAN, the transformations have to resemble something somewhat like a real-life human, but there&amp;rsquo;s nothing stopping CLIP from &lt;em>trying&lt;/em> to gravitate toward faces that aren&amp;rsquo;t human. What if you tell StyleCLIP to transform Zuck into an anime character, such as Dragon Ball Z&amp;rsquo;s &lt;a href="https://dragonball.fandom.com/wiki/Goku" target="_blank" rel="noopener">Goku&lt;/a>?&lt;/p>
&lt;figure id="figure-face---dragon-ball-z-goku-face-beta--009-alpha--54">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`face - Dragon Ball Z Goku face`, beta = 0.09, alpha = 5.4" srcset="
/2021/04/styleclip/zuck_goku_hu4db185fa422684598ab195e747eccbfa_413578_a3d2c16906146494251e3fc401b3ebc5.png 400w,
/2021/04/styleclip/zuck_goku_hu4db185fa422684598ab195e747eccbfa_413578_dcbb924aa26c2c4583ee162efa107c59.png 760w,
/2021/04/styleclip/zuck_goku_hu4db185fa422684598ab195e747eccbfa_413578_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/04/styleclip/zuck_goku_hu4db185fa422684598ab195e747eccbfa_413578_a3d2c16906146494251e3fc401b3ebc5.png"
width="512"
height="512"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>face -&amp;gt; Dragon Ball Z Goku face&lt;/code>, beta = 0.09, alpha = 5.4
&lt;/figcaption>&lt;/figure>
&lt;p>Zuck gets the hair, at least.&lt;/p>
&lt;p>People accuse Zuck of being a robot. What if we make him &lt;em>more&lt;/em> of a robot (as guided by a robot)?&lt;/p>
&lt;figure id="figure-face---robot-face-beta--008-alpha--10">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`face - robot face`, beta = 0.08, alpha = 10" srcset="
/2021/04/styleclip/zuck_robot_hu8b3351d5a591b942e8379ed83cffa36f_354709_fd53ce5e9910f10cff3130d41c127492.png 400w,
/2021/04/styleclip/zuck_robot_hu8b3351d5a591b942e8379ed83cffa36f_354709_4de94eb358c6d96b3dc1737ebf29113f.png 760w,
/2021/04/styleclip/zuck_robot_hu8b3351d5a591b942e8379ed83cffa36f_354709_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/04/styleclip/zuck_robot_hu8b3351d5a591b942e8379ed83cffa36f_354709_fd53ce5e9910f10cff3130d41c127492.png"
width="512"
height="512"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>face -&amp;gt; robot face&lt;/code>, beta = 0.08, alpha = 10
&lt;/figcaption>&lt;/figure>
&lt;p>These are all pretty tame so far. StyleCLIP surprisingly has the ability to have more complex prompts while still maintaining expected results.&lt;/p>
&lt;p>Can Mark Zuckerberg do a troll face? yes, he can!&lt;/p>
&lt;figure id="figure-face---troll-face-beta--013-alpha--91">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`face - troll face`, beta = 0.13, alpha = 9.1" srcset="
/2021/04/styleclip/zuck_troll_face_huaf25fd1cff572d57aafc214ce6800a9e_375244_eb21b473455a5bcaa7924d0706e07c89.png 400w,
/2021/04/styleclip/zuck_troll_face_huaf25fd1cff572d57aafc214ce6800a9e_375244_920f3181b98c8f3a47834be50022c7a7.png 760w,
/2021/04/styleclip/zuck_troll_face_huaf25fd1cff572d57aafc214ce6800a9e_375244_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/04/styleclip/zuck_troll_face_huaf25fd1cff572d57aafc214ce6800a9e_375244_eb21b473455a5bcaa7924d0706e07c89.png"
width="512"
height="512"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>face -&amp;gt; troll face&lt;/code>, beta = 0.13, alpha = 9.1
&lt;/figcaption>&lt;/figure>
&lt;p>We can go deeper. What about altering other attributes at the same time?&lt;/p>
&lt;figure id="figure-face---troll-face-with-large-eyes-beta--013-alpha--91">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`face - troll face with large eyes`, beta = 0.13, alpha = 9.1" srcset="
/2021/04/styleclip/zuck_troll_face_eyes_hu9875d5632856945af9feeeb728478218_376097_19d1051d1bd68973981131c49708aeb6.png 400w,
/2021/04/styleclip/zuck_troll_face_eyes_hu9875d5632856945af9feeeb728478218_376097_8ec064062d7e301bcb4c8a37799624c4.png 760w,
/2021/04/styleclip/zuck_troll_face_eyes_hu9875d5632856945af9feeeb728478218_376097_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/04/styleclip/zuck_troll_face_eyes_hu9875d5632856945af9feeeb728478218_376097_19d1051d1bd68973981131c49708aeb6.png"
width="512"
height="512"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>face -&amp;gt; troll face with large eyes&lt;/code>, beta = 0.13, alpha = 9.1
&lt;/figcaption>&lt;/figure>
&lt;p>Working with CLIP rewards good &lt;a href="https://medium.com/swlh/openai-gpt-3-and-prompt-engineering-dcdc2c5fcd29" target="_blank" rel="noopener">prompt engineering&lt;/a>, an incresingly relevant AI skill with the rise of GPT-3. With more specific, complex prompts you can stretch the &amp;ldquo;human&amp;rdquo; constraint of StyleGAN. ðŸ‘ðŸ‘„ðŸ‘&lt;/p>
&lt;figure id="figure-face-with-eyes---face-with-very-large-eyes-and-very-large-mouth-beta--016-alpha--78">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`face with eyes - face with very large eyes and very large mouth`, beta = 0.16, alpha = 7.8" srcset="
/2021/04/styleclip/zuck_large_hueee403f2d95f9a6b26fed2ad9d739855_367320_66927234244300b309e6278285288d39.png 400w,
/2021/04/styleclip/zuck_large_hueee403f2d95f9a6b26fed2ad9d739855_367320_623378ac892dfc98ddb3c6a154977c39.png 760w,
/2021/04/styleclip/zuck_large_hueee403f2d95f9a6b26fed2ad9d739855_367320_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/04/styleclip/zuck_large_hueee403f2d95f9a6b26fed2ad9d739855_367320_66927234244300b309e6278285288d39.png"
width="512"
height="512"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>face with eyes -&amp;gt; face with very large eyes and very large mouth&lt;/code>, beta = 0.16, alpha = 7.8
&lt;/figcaption>&lt;/figure>
&lt;p>Experimentation is half the fun of StyleCLIP!&lt;/p>
&lt;h2 id="antiprompts">Antiprompts&lt;/h2>
&lt;p>You may have seen that all the examples above had positive alphas, which control the strength of the transformation. So let&amp;rsquo;s talk about negative alphas. While positive alphas increase strength toward the target text vector, negative alphas increase strength away from the target text vector, resulting in the &lt;em>complete opposite&lt;/em> of the prompt. This gives rise to what I call &lt;strong>antiprompts&lt;/strong>: prompts where you intentionally want the opposite of what&amp;rsquo;s specified where asking a normal prompt doesn&amp;rsquo;t give you quite want you want.&lt;/p>
&lt;p>Let&amp;rsquo;s see if Zuck can make a serious face.&lt;/p>
&lt;figure id="figure-face---serious-face-beta--009-alpha--63">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`face - serious face`, beta = 0.09, alpha = 6.3" srcset="
/2021/04/styleclip/zuck_serious_hu4f71f798407992aed74db0c1d5397667_352901_437c8a9b2a5aa39688d11ff6fe96c4a1.png 400w,
/2021/04/styleclip/zuck_serious_hu4f71f798407992aed74db0c1d5397667_352901_748d740ffa378d21ecf8f4377b4dffcd.png 760w,
/2021/04/styleclip/zuck_serious_hu4f71f798407992aed74db0c1d5397667_352901_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/04/styleclip/zuck_serious_hu4f71f798407992aed74db0c1d5397667_352901_437c8a9b2a5aa39688d11ff6fe96c4a1.png"
width="512"
height="512"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>face -&amp;gt; serious face&lt;/code>, beta = 0.09, alpha = 6.3
&lt;/figcaption>&lt;/figure>
&lt;p>More pouty than serious. But what if he does the opposite of a laughing face?&lt;/p>
&lt;figure id="figure-face---laughing-face-beta--009-alpha---63">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`face - laughing face`, beta = 0.09, alpha = -6.3" srcset="
/2021/04/styleclip/zuck_laughing_hu951b94afc9902382bb5e12b2d1f5ba66_355457_d1cabbaa3f1802a10080285ae9a770f5.png 400w,
/2021/04/styleclip/zuck_laughing_hu951b94afc9902382bb5e12b2d1f5ba66_355457_b8784b1d79c3dd16b36073b250b8a074.png 760w,
/2021/04/styleclip/zuck_laughing_hu951b94afc9902382bb5e12b2d1f5ba66_355457_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/04/styleclip/zuck_laughing_hu951b94afc9902382bb5e12b2d1f5ba66_355457_d1cabbaa3f1802a10080285ae9a770f5.png"
width="512"
height="512"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>face -&amp;gt; laughing face&lt;/code>, beta = 0.09, alpha = -6.3
&lt;/figcaption>&lt;/figure>
&lt;p>That&amp;rsquo;s more like it.&lt;/p>
&lt;p>It doesn&amp;rsquo;t stop there. In the previous section we saw what happens when you give prompts of people and compound prompts. What, you may ask, does the AI think is the opposite of a &lt;em>person&lt;/em>?&lt;/p>
&lt;p>In the Goku example above, Zuck got larger, darker hair, more pale skin, and a chonky neck. What happens if you do the inverse?&lt;/p>
&lt;figure id="figure-face---dragon-ball-z-goku-face-beta--009-alpha---54">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`face - Dragon Ball Z Goku face`, beta = 0.09, alpha = -5.4" srcset="
/2021/04/styleclip/zuck_goku_inv_hu182038414ea1b89b14495af1bf6711be_337196_0f7668b73758c46bbe2ad8d210bc30ba.png 400w,
/2021/04/styleclip/zuck_goku_inv_hu182038414ea1b89b14495af1bf6711be_337196_d751d31718a9ef2b31b7177a93d1ff23.png 760w,
/2021/04/styleclip/zuck_goku_inv_hu182038414ea1b89b14495af1bf6711be_337196_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/04/styleclip/zuck_goku_inv_hu182038414ea1b89b14495af1bf6711be_337196_0f7668b73758c46bbe2ad8d210bc30ba.png"
width="512"
height="512"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>face -&amp;gt; Dragon Ball Z Goku face&lt;/code>, beta = 0.09, alpha = -5.4
&lt;/figcaption>&lt;/figure>
&lt;p>His hair is smaller and blonde, his skin is more tan, and he barely has a neck at all.&lt;/p>
&lt;p>What if you make Zuck the opposite of a robot? Does he become human?&lt;/p>
&lt;figure id="figure-face---robot-face-beta--008-alpha---10">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`face - robot face`, beta = 0.08, alpha = -10" srcset="
/2021/04/styleclip/zuck_robot_inv_hucf9bdc77c8385868eac14c90a9702094_395238_c5206e15901b7096f2195c34e48d9c5e.png 400w,
/2021/04/styleclip/zuck_robot_inv_hucf9bdc77c8385868eac14c90a9702094_395238_203c644fcf40ad6923f4420e86c4521e.png 760w,
/2021/04/styleclip/zuck_robot_inv_hucf9bdc77c8385868eac14c90a9702094_395238_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/04/styleclip/zuck_robot_inv_hucf9bdc77c8385868eac14c90a9702094_395238_c5206e15901b7096f2195c34e48d9c5e.png"
width="512"
height="512"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>face -&amp;gt; robot face&lt;/code>, beta = 0.08, alpha = -10
&lt;/figcaption>&lt;/figure>
&lt;p>He becomes &lt;a href="https://en.wikipedia.org/wiki/Pedro_Pascal" target="_blank" rel="noopener">Pedro Pascal&lt;/a> apparently.&lt;/p>
&lt;h2 id="video-ai-algorithms">Video AI Algorithms&lt;/h2>
&lt;p>A fun feature I added to the notebook is the ability to make videos, by generating frames from zero alpha to the target alpha and rendering them using &lt;a href="https://www.ffmpeg.org/" target="_blank" rel="noopener">ffmpeg&lt;/a>. Through that, we can see these wonderful transformations occur at a disturbingly smooth 60fps!&lt;/p>
&lt;p>Animations are cool to fully illustrate how the AI can cheat, such as with the flared nostrils example above.&lt;/p>
&lt;video controls >
&lt;source src="https://minimaxir.com/2021/04/styleclip/zuck_nose.mp4" type="video/mp4">
&lt;/video>
&lt;p>Or you can opt for pure chaos and do one of the more complex transformations. ðŸ‘ðŸ‘„ðŸ‘&lt;/p>
&lt;video controls >
&lt;source src="https://minimaxir.com/2021/04/styleclip/zuck_large.mp4" type="video/mp4">
&lt;/video>
&lt;p>TikTok will have a lot of fun with this!&lt;/p>
&lt;h2 id="ethics-and-biases">Ethics and Biases&lt;/h2>
&lt;p>Let&amp;rsquo;s address the elephant in the room: is it ethical to edit photos with AI like this?&lt;/p>
&lt;p>My take is that StyleCLIP is no different than what &lt;a href="https://www.adobe.com/products/photoshop.html" target="_blank" rel="noopener">Adobe Photoshop&lt;/a> has done for decades. Unlike deepfakes, these by construction are constrained to human portraits and can&amp;rsquo;t be used in other contexts to mislead or cause deception. Turning Mark Zuckerberg into Elon Musk would not cause a worldwide panic. &lt;a href="https://www.faceapp.com/" target="_blank" rel="noopener">FaceApp&lt;/a>, which does a similar tyle of image editing, was released years ago and still tops the App Store charts without causing democracy to implode. That said, I recommend only using StyleCLIP on public figures.&lt;/p>
&lt;p>In my testing, there is definitely an issue of model bias, both within StyleGAN and within CLIP. A famous example of gender bias in AI is a propensity to assign &lt;a href="https://qz.com/1141122/google-translates-gender-bias-pairs-he-with-hardworking-and-she-with-lazy-and-other-examples/" target="_blank" rel="noopener">gender to gender neutral terms&lt;/a>, such as &lt;code>He is a soldier. She is a teacher&lt;/code>. Let&amp;rsquo;s try both for Zuck.&lt;/p>
&lt;figure id="figure-face---soldier-face-beta--01-alpha--72">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`face - soldier face`, beta = 0.1, alpha = 7.2" srcset="
/2021/04/styleclip/zuck_soldier_huff15b7b0164d21abec95a380feb0efb5_363216_9c72f264fa8d3b22e59872c850f60869.png 400w,
/2021/04/styleclip/zuck_soldier_huff15b7b0164d21abec95a380feb0efb5_363216_7c6095177a87fe9afb0a1a0ec628fb4e.png 760w,
/2021/04/styleclip/zuck_soldier_huff15b7b0164d21abec95a380feb0efb5_363216_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/04/styleclip/zuck_soldier_huff15b7b0164d21abec95a380feb0efb5_363216_9c72f264fa8d3b22e59872c850f60869.png"
width="512"
height="512"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>face -&amp;gt; soldier face&lt;/code>, beta = 0.1, alpha = 7.2
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-face---teacher-face-beta--013-alpha--56">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="`face - teacher face`, beta = 0.13, alpha = 5.6" srcset="
/2021/04/styleclip/zuck_teacher_huc4568cebe7940504d34eb65b48f4286d_369338_86b42051f7eef3fbc4ef3803f93babe1.png 400w,
/2021/04/styleclip/zuck_teacher_huc4568cebe7940504d34eb65b48f4286d_369338_4692ba447ba356bb97182dffc6e42101.png 760w,
/2021/04/styleclip/zuck_teacher_huc4568cebe7940504d34eb65b48f4286d_369338_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2021/04/styleclip/zuck_teacher_huc4568cebe7940504d34eb65b48f4286d_369338_86b42051f7eef3fbc4ef3803f93babe1.png"
width="512"
height="512"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;code>face -&amp;gt; teacher face&lt;/code>, beta = 0.13, alpha = 5.6
&lt;/figcaption>&lt;/figure>
&lt;p>Unfortunately it still holds true.&lt;/p>
&lt;p>It is surprisingly easy to get the model to perform racist/sexist/ageist transformations without much prodding. Inputting &lt;code>face with white skin -&amp;gt; face with black skin&lt;/code> does what you think it would do. Making similar transformations based on race/sex/age do indeed work, and I am deliberately not demoing them. If you do experiment around these biases, I recommend careful consideration with posting the outputs.&lt;/p>
&lt;h2 id="the-future-of-ai-image-editing">The Future of AI Image Editing&lt;/h2>
&lt;p>StyleCLIP is a fun demo on the potential of AI-based image editing. Although not the most pragmatic way to edit portraits, it&amp;rsquo;s fun to see just how well (or how poorly) it can adapt to certain prompts.&lt;/p>
&lt;p>Even though everything noted in this blog post is open-sourced, don&amp;rsquo;t think about trying to sell StyleCLIP as a product: StyleGAN2 (which in the end is responsible for generating the image) and its variants were released under &lt;a href="https://nvlabs.github.io/stylegan2/license.html" target="_blank" rel="noopener">non-commerical licenses&lt;/a>. But it wouldn&amp;rsquo;t surprise me if someone uses the techniques noted in the papers to create their own, more efficient StyleCLIP with a bespoke efficient GAN to create an entire new industry.&lt;/p></description></item><item><title>Tempering Expectations for GPT-3 and OpenAIâ€™s API</title><link>https://minimaxir.com/2020/07/gpt3-expectations/</link><pubDate>Sat, 18 Jul 2020 10:30:00 -0700</pubDate><guid>https://minimaxir.com/2020/07/gpt3-expectations/</guid><description>&lt;p>On May 29th, &lt;a href="https://openai.com" target="_blank" rel="noopener">OpenAI&lt;/a> released &lt;a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noopener">a paper&lt;/a> on GPT-3, their next iteration of &lt;a href="http://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">Transformers&lt;/a>-based text generation neural networks. Most notably, the new model has 175 billion parameters compared to the 1.5 billion of previous &lt;a href="https://openai.com/blog/better-language-models/" target="_blank" rel="noopener">GPT-2 iteration&lt;/a>: a &lt;em>117x&lt;/em> increase in model size! Because GPT-3 is so large, it can&amp;rsquo;t be run on conventional computers, and it only became publicly available as a part of the &lt;a href="https://beta.openai.com" target="_blank" rel="noopener">OpenAI API&lt;/a>, which entered an invite-only beta soon after the paper was released and will be released for-profit sometime later.&lt;/p>
&lt;p>The API allows you to programmatically provide GPT-3 with a prompt, and return the resulting AI-generated text. For example, you could invoke the API with:&lt;/p>
&lt;pre>&lt;code class="language-sh">curl https://api.openai.com/v1/engines/davinci/completions \
-H &amp;quot;Content-Type: application/json&amp;quot; \
-H &amp;quot;Authorization: Bearer &amp;lt;SECRET_KEY&amp;gt;&amp;quot; \
-d '{&amp;quot;prompt&amp;quot;: &amp;quot;This is a test&amp;quot;, &amp;quot;max_tokens&amp;quot;: 5}'
&lt;/code>&lt;/pre>
&lt;p>And get this back from the API, where the &lt;code>text&lt;/code> is the generated text following up from the &lt;code>prompt&lt;/code>:&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;id&amp;quot;: &amp;quot;cmpl-&amp;lt;ID&amp;gt;&amp;quot;,
&amp;quot;object&amp;quot;: &amp;quot;text_completion&amp;quot;,
&amp;quot;created&amp;quot;: 1586839808,
&amp;quot;model&amp;quot;: &amp;quot;davinci:2020-05-03&amp;quot;,
&amp;quot;choices&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot; of reading speed. You&amp;quot;,
&amp;quot;index&amp;quot;: 0,
&amp;quot;logprobs&amp;quot;: null,
&amp;quot;finish_reason&amp;quot;: &amp;quot;length&amp;quot;
}
]
}
&lt;/code>&lt;/pre>
&lt;p>As someone who has spent a very large amount of time working with GPT-2 while developing tools such as &lt;a href="https://github.com/minimaxir/gpt-2-simple" target="_blank" rel="noopener">gpt-2-simple&lt;/a> and &lt;a href="https://github.com/minimaxir/aitextgen" target="_blank" rel="noopener">aitextgen&lt;/a>, which allow for optimized text generation using GPT-2, I was eager to test for myself if the quality of text generated from GPT-3 was really that much better. Thanks to OpenAI, I got invited to the beta, and with permission, I released a &lt;a href="https://github.com/minimaxir/gpt-3-experiments" target="_blank" rel="noopener">GitHub repository&lt;/a> with a Python script to query the API, along with &lt;a href="https://github.com/minimaxir/gpt-3-experiments/tree/master/examples" target="_blank" rel="noopener">many examples&lt;/a> of text prompts and their outputs. A fun use case for GPT-3 is absurdism, such as prompting the model about &lt;a href="https://github.com/minimaxir/gpt-3-experiments/tree/master/examples/unicorn" target="_blank" rel="noopener">unicorns speaking English&lt;/a>, with the model prompt bolded:&lt;/p>
&lt;script type="application/javascript" src="https://gist.github.com/minimaxir/ac362cc81691eb92aa1b6a5c32d94ce3.js">&lt;/script>
&lt;p>I also fed &lt;a href="https://github.com/minimaxir/gpt-3-experiments/tree/master/examples/twitter-minimaxir" target="_blank" rel="noopener">my own tweets&lt;/a> through GPT-3 and curated the output, resulting in data science one-liners that are wholly original:&lt;/p>
&lt;p>&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">my new AI can tell if you are a serial killer just by looking at your LinkedIn&lt;/p>&amp;mdash; Max Woolf (@minimaxir) &lt;a href="https://twitter.com/minimaxir/status/1282147674645565441?ref_src=twsrc%5Etfw">July 12, 2020&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">Data scientists don&amp;#39;t need to be good at math, they just need to be good at lying to people.&lt;/p>&amp;mdash; Max Woolf (@minimaxir) &lt;a href="https://twitter.com/minimaxir/status/1281015343205539847?ref_src=twsrc%5Etfw">July 9, 2020&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">sometimes I make a folder and name it &amp;quot;data&amp;quot; and I get very excited.&lt;/p>&amp;mdash; Max Woolf (@minimaxir) &lt;a href="https://twitter.com/minimaxir/status/1280698121262071809?ref_src=twsrc%5Etfw">July 8, 2020&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;/p>
&lt;p>There hadn&amp;rsquo;t been too much GPT-3 hype after the initial announcement, outside of a few blogs from &lt;a href="https://www.gwern.net/GPT-3" target="_blank" rel="noopener">Gwern&lt;/a> and &lt;a href="http://lacker.io/ai/2020/07/06/giving-gpt-3-a-turing-test.html" target="_blank" rel="noopener">Kevin Lacker&lt;/a>. Until a &lt;a href="https://twitter.com/sharifshameem/status/1282676454690451457" target="_blank" rel="noopener">viral tweet&lt;/a> by &lt;a href="https://twitter.com/sharifshameem" target="_blank" rel="noopener">Sharif Shameem&lt;/a> showed what GPT-3 can &lt;em>really&lt;/em> do:&lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">This is mind blowing.&lt;br>&lt;br>With GPT-3, I built a layout generator where you just describe any layout you want, and it generates the JSX code for you.&lt;br>&lt;br>W H A T &lt;a href="https://t.co/w8JkrZO4lk">pic.twitter.com/w8JkrZO4lk&lt;/a>&lt;/p>&amp;mdash; Sharif Shameem (@sharifshameem) &lt;a href="https://twitter.com/sharifshameem/status/1282676454690451457?ref_src=twsrc%5Etfw">July 13, 2020&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;p>Later, he made a &lt;a href="https://twitter.com/sharifshameem/status/1284095222939451393" target="_blank" rel="noopener">followup tweet&lt;/a> generating &lt;a href="https://reactjs.org" target="_blank" rel="noopener">React&lt;/a> code with GPT-3:&lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">I just built a *functioning* React app by describing what I wanted to GPT-3. &lt;br>&lt;br>I&amp;#39;m still in awe. &lt;a href="https://t.co/UUKSYz2NJO">pic.twitter.com/UUKSYz2NJO&lt;/a>&lt;/p>&amp;mdash; Sharif Shameem (@sharifshameem) &lt;a href="https://twitter.com/sharifshameem/status/1284095222939451393?ref_src=twsrc%5Etfw">July 17, 2020&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;p>That demo got the attention of venture capitalists. And when a cool-looking magical thing gets the attention of venture capitalists, discourse tends to spiral out of control. Now, there are &lt;em>many&lt;/em> &lt;a href="https://twitter.com/search?q=Gpt-3&amp;amp;src=recent_search_click&amp;amp;f=live" target="_blank" rel="noopener">tweets about GPT-3&lt;/a>, and what it can do from others who have gained access to the API.&lt;/p>
&lt;p>Hype aside, let&amp;rsquo;s look at the pragmatic realities of the model. GPT-3 is indeed a large step forward for AI text-generation, but there are very many caveats with the popular demos and use cases that must be addressed.&lt;/p>
&lt;h2 id="an-overview-of-gpt-3">An Overview of GPT-3&lt;/h2>
&lt;p>GPT-3 itself, like most neural network models, is a &lt;a href="https://en.wikipedia.org/wiki/Black_box" target="_blank" rel="noopener">black box&lt;/a> where it&amp;rsquo;s impossible to see &lt;em>why&lt;/em> it makes its decisions, so let&amp;rsquo;s think about GPT-3 in terms of inputs and outputs.&lt;/p>
&lt;p>Actually, why not let GPT-3 tell its own story? Hey GPT-3, how do you work?&lt;/p>
&lt;script type="application/javascript" src="https://gist.github.com/minimaxir/596b880d2275578104a0b7c13167a3c0.js">&lt;/script>
&lt;p>Close, but not quite!&lt;/p>
&lt;p>In layman&amp;rsquo;s terms, text generating models such as GPT-3 generate text by taking supplied chunks of text from a prompt and predicting the next chunk of text, with an optional &lt;code>temperature&lt;/code> parameter to allow the model to make suboptimal predictions and therefore be more &amp;ldquo;creative&amp;rdquo;. Then the model makes another prediction from the previous chunks including the new chunk, and repeats until it hits a specified length or a token that tells the model to stop generating. It&amp;rsquo;s not very philosophical, or evidence of some sort of anthropomorphic consciousness.&lt;/p>
&lt;p>GPT-3 has two notable improvements from GPT-2 aside from its size: it allows generation of text twice the length of GPT-2 (about 10 paragraphs of English text total), and the prompts to the model better steer the generation of the text toward the desired domain (due to few-shot learning). For example, if you prompt the model with an example of React code, and then tell it to generate more React code, you&amp;rsquo;ll get much better results than if you gave it the simple prompt.&lt;/p>
&lt;p>Therefore, there are two high-level use cases for GPT-2: the &lt;strong>creative&lt;/strong> use case for fun text generation at high &lt;code>temperature&lt;/code>, as GPT-2 once was, and the &lt;strong>functional&lt;/strong> use case, for specific &lt;a href="https://en.wikipedia.org/wiki/Natural_language_processing" target="_blank" rel="noopener">NLP&lt;/a>-based use cases such as webpage mockups, with a &lt;code>temperature&lt;/code> of &lt;code>0.0&lt;/code>.&lt;/p>
&lt;p>GPT-3 was trained on a massive amount of text from all over the internet as of October 2019 (e.g. it does not know about &lt;a href="https://www.cdc.gov/coronavirus/2019-ncov/index.html" target="_blank" rel="noopener">COVID-19&lt;/a>), and therefore it has likely seen every &lt;em>type&lt;/em> of text possible, from code, to movie scripts, to tweets. A common misconception among viewers of GPT-3 demos is that the model is trained on a new dataset; that&amp;rsquo;s not currently the case, it&amp;rsquo;s just &lt;em>that good&lt;/em> at extrapolation. As an example, despite the &lt;a href="https://en.wikipedia.org/wiki/Star_Wars:_Episode_III_%e2%80%93_Revenge_of_the_Sith" target="_blank" rel="noopener">Star Wars: Episode III - Revenge of the Sith&lt;/a> prompt containing text &lt;a href="https://github.com/minimaxir/gpt-3-experiments/tree/master/examples/revengeofthesith" target="_blank" rel="noopener">from a single scene&lt;/a>, the &lt;a href="https://github.com/minimaxir/gpt-3-experiments/blob/master/examples/revengeofthesith/output_0_7.md" target="_blank" rel="noopener">0.7 temperature generation&lt;/a> imputes characters &lt;em>and&lt;/em> lines of dialogue from much further into the movie. (The largest GPT-2 model could do that, but nowhere near as robust)&lt;/p>
&lt;p>The real metagame with GPT-3 is engineering and optimizing complex prompts which can &lt;em>reliably&lt;/em> coerce outputs into what you want. And with that brings a whole host of complexity and concerns.&lt;/p>
&lt;h2 id="gpt-3-caveats">GPT-3 Caveats&lt;/h2>
&lt;p>Despite everything above, I don&amp;rsquo;t believe that GPT-3 is a new paradigm or an &lt;a href="https://en.wikipedia.org/wiki/Clarke%27s_three_laws" target="_blank" rel="noopener">advanced technology indistinguishable from magic&lt;/a>. GPT-3 and the OpenAI API showcases on social media don&amp;rsquo;t show potential pitfalls with the model and the API.&lt;/p>
&lt;p>Hey GPT-3, what problems do you have?&lt;/p>
&lt;script type="application/javascript" src="https://gist.github.com/minimaxir/e49913a1e720da8d1c8e2d0f783468fa.js">&lt;/script>
&lt;p>Sorry GPT-3, but I am a mean person.&lt;/p>
&lt;h3 id="model-latency">Model Latency&lt;/h3>
&lt;p>If you&amp;rsquo;ve seen the demo videos, the model is &lt;em>slow&lt;/em>, and it can take awhile for output to show up, and in the meantime the user is unsure if the model is broken or not. (There is a feature to allow streaming the model outputs as they are generated, which helps in creative cases but not in functional cases).&lt;/p>
&lt;p>I don&amp;rsquo;t blame OpenAI for the slowness. A 175 billion parameter model is a model that&amp;rsquo;s wayyy too big to fit on a GPU for deployment. No one knows &lt;em>how&lt;/em> GPT-3 is actually deployed on OpenAI&amp;rsquo;s servers, and how much it can scale.&lt;/p>
&lt;p>But the fact remains; if the model is too slow on the user end, it results in a bad user experience and might drive people away from GPT-3 and just do things themselves (e.g. Apple&amp;rsquo;s Siri for iOS, where requests can take forever if there is a weak internet connection and you just give up and do it yourself).&lt;/p>
&lt;h3 id="selection-bias-toward-good-examples">Selection Bias Toward Good Examples&lt;/h3>
&lt;p>The demos for GPT-3 are creative and human-like, but like all text generation demos, they unintentionally imply that &lt;em>all&lt;/em> AI-generated output will be that good. Unfortunately, that&amp;rsquo;s not the case in reality; AI-generated text has a tendency to fall into an &lt;a href="https://en.wikipedia.org/wiki/Uncanny_valley" target="_blank" rel="noopener">uncanny valley&lt;/a>, and good examples in showcases are often cherry-picked.&lt;/p>
&lt;p>That said, from my experiments, GPT-3 is far better in terms of the &lt;em>average&lt;/em> quality of generated text than other text-generation models, although it still does depend on the generation domain. When I was curating my generated tweets, I estimated 30-40% of the tweets were usable comedically, a &lt;em>massive&lt;/em> improvement over the 5-10% usability from my GPT-2 tweet generation.&lt;/p>
&lt;p>However, a 30-40% success rate implies a 60-70% failure rate, which is patently unsuitable for a production application. If it takes seconds to generate a React component and it takes on average &lt;em>3 tries&lt;/em> to get something usable, it might be more pragmatic to just create the component the hard, boring way. Compare again to Apple&amp;rsquo;s Siri, which can get very frustrating when it &lt;a href="https://www.reddit.com/r/SiriFail/" target="_blank" rel="noopener">performs the wrong action&lt;/a>.&lt;/p>
&lt;h3 id="everyone-has-the-same-model">Everyone Has The Same Model&lt;/h3>
&lt;p>The core GPT-3 model from the OpenAI API is the 175B parameter &lt;code>davinci&lt;/code> model. The GPT-3 demos on social media often hide the prompt, allowing for some mystique. However, because everyone has the same model and you can&amp;rsquo;t build your own GPT-3 model, there&amp;rsquo;s no competitive advantage. GPT-3 seed prompts can be reverse-engineered, which may become a rude awakening for entrepreneurs and the venture capitalists who fund them.&lt;/p>
&lt;p>Corporate machine learning models are often distinguished from those from other companies in the same field through their training on private, proprietary data and bespoke model optimization for a given use case. However, OpenAI CTO Greg Brockman hinted that the API will be &lt;a href="https://news.ycombinator.com/item?id=23725834" target="_blank" rel="noopener">adding a finetuning feature&lt;/a> later in July, which could help solve this problem.&lt;/p>
&lt;h3 id="racist-and-sexist-outputs">Racist and Sexist Outputs&lt;/h3>
&lt;p>The Web UI for the OpenAI API has a noteworthy warning:&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Please use your judgement and discretion before posting API outputs on social media.&lt;/strong> You are interacting with the raw model, which means we do not filter out biased or negative responses. With great power comes great responsibility.&lt;/p>
&lt;/blockquote>
&lt;p>This is a reference to the &lt;a href="https://openai.com/blog/openai-api/" target="_blank" rel="noopener">FAQ&lt;/a> for the API:&lt;/p>
&lt;blockquote>
&lt;p>Mitigating negative effects such as harmful bias is a hard, industry-wide issue that is extremely important. Ultimately, our API models do exhibit biases (as shown in the GPT-3 paper) that will appear on occasion in generated text. Our API models could also cause harm in ways that we havenâ€™t thought of yet.&lt;/p>
&lt;/blockquote>
&lt;p>After the launch of the API, NVIDIA researcher &lt;a href="https://twitter.com/AnimaAnandkumar" target="_blank" rel="noopener">Anima Anandkumar&lt;/a> made a &lt;a href="https://twitter.com/AnimaAnandkumar/status/1271137176529416193" target="_blank" rel="noopener">highly-debated tweet&lt;/a>.&lt;/p>
&lt;p>During my GPT-3 experiments, I found that &lt;a href="https://github.com/minimaxir/gpt-3-experiments/tree/master/examples/twitter-dril" target="_blank" rel="noopener">generating tweets&lt;/a> from &lt;a href="https://twitter.com/dril" target="_blank" rel="noopener">@dril&lt;/a> (admittingly an edgy Twitter user) ended up resulting in 4chan-level racism/sexism that I spent enormous amounts of time sanitizing, and it became more apparent at higher temperatures. It&amp;rsquo;s especially important to avoid putting offensive content for generated texts which put words in others&amp;rsquo; mouths.&lt;/p>
&lt;p>&lt;a href="https://twitter.com/an_open_mind" target="_blank" rel="noopener">Jerome Pesenti&lt;/a>, the head of AI at Facebook, also managed to &lt;a href="https://twitter.com/an_open_mind/status/1284487376312709120" target="_blank" rel="noopener">trigger anti-semetic tweets&lt;/a> from a GPT-3 app:&lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">&lt;a href="https://twitter.com/hashtag/gpt3?src=hash&amp;amp;ref_src=twsrc%5Etfw">#gpt3&lt;/a> is surprising and creative but itâ€™s also unsafe due to harmful biases. Prompted to write tweets from one word - Jews, black, women, holocaust - it came up with these (&lt;a href="https://t.co/G5POcerE1h">https://t.co/G5POcerE1h&lt;/a>). We need more progress on &lt;a href="https://twitter.com/hashtag/ResponsibleAI?src=hash&amp;amp;ref_src=twsrc%5Etfw">#ResponsibleAI&lt;/a> before putting NLG models in production. &lt;a href="https://t.co/FAscgUr5Hh">pic.twitter.com/FAscgUr5Hh&lt;/a>&lt;/p>&amp;mdash; Jerome Pesenti (@an_open_mind) &lt;a href="https://twitter.com/an_open_mind/status/1284487376312709120?ref_src=twsrc%5Etfw">July 18, 2020&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;p>Again, it depends on the domain. Would GPT-3 output racist or sexist React components? Likely not, but it&amp;rsquo;s something that would still need to be robustly checked. OpenAI does appear to take these concerns seriously, and has implemented toxicity detectors for generated content in the Web UI, although not the programmatic API yet.&lt;/p>
&lt;h2 id="further-questions-about-the-openai-api">Further Questions about the OpenAI API&lt;/h2>
&lt;p>AI model-as-a-service is an industry that tends to be a black box wrapped around another black box. Despite all the caveats, everything depends on how the OpenAI API exits beta and rolls out the API for production use. There are too many unknowns to even think about making money off of the OpenAI API, let alone making a startup based on it.&lt;/p>
&lt;p>At minimum, anyone using the OpenAI API professionally needs to know:&lt;/p>
&lt;ul>
&lt;li>Cost for generation per token/request&lt;/li>
&lt;li>Rate limits and max number of concurrent requests&lt;/li>
&lt;li>Average and peak latencies for generating tokens&lt;/li>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Service-level_agreement" target="_blank" rel="noopener">SLA&lt;/a> for the API&lt;/li>
&lt;li>AI generated content ownership/copyright&lt;/li>
&lt;/ul>
&lt;p>That&amp;rsquo;s certainly less magical!&lt;/p>
&lt;p>The most important question mark there is cost: given the model size, I&amp;rsquo;m not expecting it to be cheap, and it&amp;rsquo;s entirely possible that the unit economics make most GPT-3-based startups infeasible.&lt;/p>
&lt;p>That said, it&amp;rsquo;s still good for people to experiment with GPT-3 and the OpenAI API in order to show what the model is truly capable of. It won&amp;rsquo;t replace software engineering jobs anytime soon, or become &lt;a href="https://en.wikipedia.org/wiki/Skynet_%28Terminator%29" target="_blank" rel="noopener">Skynet&lt;/a>, or whatever. But it&amp;rsquo;s objectively a &lt;em>step forward&lt;/em> in the field of AI text-generation.&lt;/p>
&lt;p>What about GPT-2? Since it&amp;rsquo;s unlikely that the other GPT-3 models will be open-sourced by OpenAI, GPT-2 isn&amp;rsquo;t obsolete, and there will still be demand for a more open text-generating model. However, I confess that the success of GPT-3 has &lt;a href="https://twitter.com/minimaxir/status/1284160088161181697" target="_blank" rel="noopener">demotivated me&lt;/a> to continue working on my own GPT-2 projects, especially since they will now be impossible to market competitively (GPT-2 is a number less than GPT-3 after all).&lt;/p>
&lt;p>All said, I&amp;rsquo;d be glad to use GPT-3 and the OpenAI API for both personal and professional projects once it&amp;rsquo;s out of beta, given that the terms of use for the API are reasonable. And if the hype becomes more leveled such that said projects can actually stand out.&lt;/p></description></item><item><title>How to Build a Twitter Text-Generating AI Bot With GPT-2</title><link>https://minimaxir.com/2020/01/twitter-gpt2-bot/</link><pubDate>Thu, 16 Jan 2020 08:00:00 -0800</pubDate><guid>https://minimaxir.com/2020/01/twitter-gpt2-bot/</guid><description>&lt;p>&lt;a href="https://openai.com/blog/better-language-models/" target="_blank" rel="noopener">GPT-2&lt;/a>, a text-generating neural network model made by &lt;a href="https://openai.com" target="_blank" rel="noopener">OpenAI&lt;/a>, has recently been in the headlines, from being able to play &lt;a href="https://www.aidungeon.io/start" target="_blank" rel="noopener">AI-generated text adventures&lt;/a> to playing &lt;em>chess&lt;/em> with an &lt;a href="https://slatestarcodex.com/2020/01/06/a-very-unlikely-chess-game/" target="_blank" rel="noopener">AI trained on chess move notation&lt;/a>. However, I initially built &lt;a href="https://github.com/minimaxir/gpt-2-simple" target="_blank" rel="noopener">gpt-2-simple&lt;/a>, which can be used to finetune GPT-2 on any text dataset you choose, for a less academic purpose: comedy.&lt;/p>
&lt;p>Over the past month, &lt;a href="https://twitter.com/" target="_blank" rel="noopener">Twitter&lt;/a> account &lt;a href="https://twitter.com/dril_gpt2" target="_blank" rel="noopener">@dril_gpt2&lt;/a>, an AI parody by &lt;a href="https://twitter.com/kingdomakrillic" target="_blank" rel="noopener">@kingdomakrillic&lt;/a> of the infamous Twitter user &lt;a href="https://twitter.com/dril" target="_blank" rel="noopener">@dril&lt;/a>, &lt;a href="https://twitter.com/dril_gpt2/status/1208597102181408771" target="_blank" rel="noopener">used&lt;/a> my &lt;a href="https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce" target="_blank" rel="noopener">Colaboratory Notebook&lt;/a> for finetuning GPT-2 on dril&amp;rsquo;s tweets using gpt-2-simple to generate human-curated tweets which push the limits of the &lt;a href="https://en.wikipedia.org/wiki/Turing_test" target="_blank" rel="noopener">Turing Test&lt;/a>:&lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">this is what big data looks like:&lt;br>a) filtering out the people with fucked-up hair&lt;br>b) automatically generating news articles for you&lt;br>c) fusing the world star hip hop fanfiction with the mp game net god&lt;/p>&amp;mdash; wint but Al (@dril_gpt2) &lt;a href="https://twitter.com/dril_gpt2/status/1215760729095016449?ref_src=twsrc%5Etfw">January 10, 2020&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">ok ladies and gentlemen of the jury, before i get any farther along in my testimony, i would like for you to take a moment to recognize the &amp;quot;jurors are beautiful&amp;quot; shirt i am wearing&lt;/p>&amp;mdash; wint but Al (@dril_gpt2) &lt;a href="https://twitter.com/dril_gpt2/status/1215834913888460800?ref_src=twsrc%5Etfw">January 11, 2020&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;p>These tweets are &lt;a href="https://twitter.com/kingdomakrillic/status/1210487045338079237" target="_blank" rel="noopener">definitely made by a robot&lt;/a> and not by a &lt;a href="https://twitter.com/KeatonPatti/status/1006961202998726665" target="_blank" rel="noopener">human pretending to be a robot&lt;/a>; @dril_gpt2 occasionally falls into some of the famous GPT-2 traps such as &lt;a href="https://twitter.com/dril_gpt2/status/1216162880023752705" target="_blank" rel="noopener">incoherent lists&lt;/a> and &lt;a href="https://twitter.com/dril_gpt2/status/1212662889028431872" target="_blank" rel="noopener">extended repetition loops&lt;/a>.&lt;/p>
&lt;p>Here&amp;rsquo;s how you too can create an AI bot to parody any Twitter user, even if you&amp;rsquo;re not a coder!&lt;/p>
&lt;h2 id="how-to-get-tweets-for-training-an-ai">How to Get Tweets For Training An AI&lt;/h2>
&lt;p>Twitter&amp;rsquo;s &lt;a href="https://developer.twitter.com/en.html" target="_blank" rel="noopener">API&lt;/a> famously limits users to retrieving &lt;a href="https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-user_timeline" target="_blank" rel="noopener">only the latest 3,200 tweets&lt;/a> from a given user, which is not nearly enough input data for training a good AI. Therefore, to get all tweets possible for a user, you&amp;rsquo;ll need to use another approach. The Python package &lt;a href="https://github.com/twintproject/twint" target="_blank" rel="noopener">twint&lt;/a> is a popular way of bypassing that API limitation.&lt;/p>
&lt;p>I&amp;rsquo;ve &lt;a href="https://github.com/minimaxir/download-tweets-ai-text-gen" target="_blank" rel="noopener">open-sourced a Python 3 script on GitHub&lt;/a> which leverages &lt;code>twint&lt;/code> to download tweets, and then the script does common preprocessing such as removing URLs, retweets, and tweet replies to make the resulting input text cleaner.&lt;/p>
&lt;p>First, in a terminal, install the Python script dependencies:&lt;/p>
&lt;pre>&lt;code class="language-sh">pip3 install twint==2.1.4 fire tqdm
&lt;/code>&lt;/pre>
&lt;p>Then download the &lt;a href="https://raw.githubusercontent.com/minimaxir/download-tweets-ai-text-gen/master/download_tweets.py" target="_blank" rel="noopener">download_tweets.py script&lt;/a>.&lt;/p>
&lt;p>The script is interacted with via a command line interface. After &lt;code>cd&lt;/code>ing into the directory where the script is stored in a terminal, run:&lt;/p>
&lt;pre>&lt;code class="language-sh">python3 download_tweets.py &amp;lt;twitter_username&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>e.g. If you want to download all tweets (sans retweets/replies) from &lt;a href="https://twitter.com/dril_gpt2" target="_blank" rel="noopener">@dril&lt;/a>, run:&lt;/p>
&lt;pre>&lt;code class="language-sh">python3 download_tweets.py dril
&lt;/code>&lt;/pre>
&lt;p>The tweets will be downloaded to a single-column CSV titled &lt;code>&amp;lt;username&amp;gt;_tweets.csv&lt;/code>, which is the ideal format for training with an AI.&lt;/p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/2020/01/twitter-gpt2-bot/csv_hue9d6de1eb11a73f11d1237b1ed27a8b0_100306_e3cd19c16aad81b144fd37e77a527e1b.png 400w,
/2020/01/twitter-gpt2-bot/csv_hue9d6de1eb11a73f11d1237b1ed27a8b0_100306_eaa7d88047d16eb19020a4e326fb1f3d.png 760w,
/2020/01/twitter-gpt2-bot/csv_hue9d6de1eb11a73f11d1237b1ed27a8b0_100306_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2020/01/twitter-gpt2-bot/csv_hue9d6de1eb11a73f11d1237b1ed27a8b0_100306_e3cd19c16aad81b144fd37e77a527e1b.png"
width="760"
height="357"
loading="lazy" data-zoomable class=" img-light" />&lt;/div>
&lt;/div>&lt;/figure>
&lt;p>The more tweets the better: it&amp;rsquo;s recommended that you have at least 1 MB of input data, which is tens of thousands of tweets.&lt;/p>
&lt;h2 id="how-to-train-a-twitter-ai-and-generate-tweets">How To Train a Twitter AI And Generate Tweets&lt;/h2>
&lt;p>A common problem with training AI on short-form text is that the text can &amp;ldquo;leak&amp;rdquo; information; since the AI trains on about 2-3 paragraphs worth of text at a time (about 5-10 tweets), you need to explicitly state when a given tweet begins and when the tweet ends. To fix this issue, &lt;a href="https://github.com/minimaxir/gpt-2-simple" target="_blank" rel="noopener">gpt-2-simple&lt;/a> has a special case for single-column CSVs, where it will automatically process the text for best training and generation. (i.e. by adding &lt;code>&amp;lt;|startoftext|&amp;gt;&lt;/code> and &lt;code>&amp;lt;|endoftext|&amp;gt;&lt;/code> to each tweet). This workflow will also handle multi-line tweets correctly as their own entity.&lt;/p>
&lt;p>You can use &lt;a href="https://colab.research.google.com/drive/1qxcQ2A1nNjFudAGN_mcMOnvV9sF_PkEb" target="_blank" rel="noopener">this Colaboratory notebook&lt;/a> to train the model on your downloaded tweets, and generate massive amounts of tweets from it. The notebook itself has more instructions on how to feed the CSV created above as input data to the model.&lt;/p>
&lt;p>Note that without a lot of tweets, the model might easily overfit and output existing tweets verbatim; if that&amp;rsquo;s the case, you may want to train for fewer &lt;code>steps&lt;/code> (e.g. 200-500). Additionally, I recommend only using the 124M &amp;ldquo;small&amp;rdquo; and 355M &amp;ldquo;medium&amp;rdquo; GPT-2 models; larger GPT-2 models finetune poorly on small text documents and low amounts of input data.&lt;/p>
&lt;p>Once the training is complete, you can generate tweets 1,000 at a time using this cell:&lt;/p>
&lt;pre>&lt;code class="language-python">gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())
gpt2.generate_to_file(sess,
destination_path=gen_file,
length=200,
temperature=1.0,
top_p=0.9,
prefix='&amp;lt;|startoftext|&amp;gt;',
truncate='&amp;lt;|endoftext|&amp;gt;',
include_prefix=False,
nsamples=1000,
batch_size=20
)
&lt;/code>&lt;/pre>
&lt;p>Run the cell as many times as you want for more tweets, and download them from the Files tab by right-clicking them! The notebook also has more information on how to tweak the generation parameters to make the tweets more crazy or more sane.&lt;/p>
&lt;p>You can then open the generated &lt;code>.txt&lt;/code> files on your local computer in your favorite text editor (I recommend &lt;a href="https://code.visualstudio.com" target="_blank" rel="noopener">Visual Studio Code&lt;/a>), and start curating however you see fit! Each tweet is separated by a delimiter line, making it easier to visually parse and handle multiline tweets (compare/contrast with &lt;a href="https://pastebin.com/TmRtUX2x" target="_blank" rel="noopener">raw @dril_gpt2&lt;/a> output, which blends together a few tweets per delimiter).&lt;/p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/2020/01/twitter-gpt2-bot/vscode_huce9708f8bc5d1204ba0bccd70d0be883_186115_6a6a1ceca170a192c03621caf0696044.png 400w,
/2020/01/twitter-gpt2-bot/vscode_huce9708f8bc5d1204ba0bccd70d0be883_186115_bddd8e21ca5d8b66500d49a66ea44f74.png 760w,
/2020/01/twitter-gpt2-bot/vscode_huce9708f8bc5d1204ba0bccd70d0be883_186115_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2020/01/twitter-gpt2-bot/vscode_huce9708f8bc5d1204ba0bccd70d0be883_186115_6a6a1ceca170a192c03621caf0696044.png"
width="760"
height="497"
loading="lazy" data-zoomable class=" img-light" />&lt;/div>
&lt;/div>&lt;/figure>
&lt;p>A warning: you are not guaranteed to get quality generated tweets all the time. In fact, quality tweets are &lt;em>rare&lt;/em>: I estimate &lt;strong>less than 5%&lt;/strong> of AI-generated tweets are good/funny. That means if you want to curate hundreds of tweets, you&amp;rsquo;ll need to generate &lt;strong>thousands&lt;/strong> of tweets and sort through all of them (and double-check to make sure they&amp;rsquo;re not real tweets!). It&amp;rsquo;s not as bad as it sounds, in my opinion it&amp;rsquo;s kinda fun. But curation is its own skill, which is why human-curated tweets aren&amp;rsquo;t a stain on the &amp;ldquo;credibility&amp;rdquo; of AI bots, and also why the ~1,500 tweets so far from @dril_gpt2 is very impressive.&lt;/p>
&lt;p>Now, what do you do with these curated tweets?&lt;/p>
&lt;h2 id="automating-the-twitter-bot">Automating The Twitter Bot&lt;/h2>
&lt;p>If you&amp;rsquo;re not a programmer or just want to prototype a Twitter bot, I recommend creating a normal Twitter account and scheduling hand-curated Twitter posts through &lt;a href="https://tweetdeck.twitter.com" target="_blank" rel="noopener">TweetDeck&lt;/a>, which is owned by Twitter and has native scheduling capabilities. You can space out tweets at given times, although it may be a hassle to do that for hundreds of tweets.&lt;/p>
&lt;p>Otherwise, it is more efficient to write a code script to make tweets at periodic intervals for a bot account. Old tutorials around the internet recommend writing a script which posts to Twitter, sleeps for X hours, post, repeat; that method does not easily scale to multiple bots and it requires that a full computer be dedicated to it, which is not an efficient use of computing resources.&lt;/p>
&lt;p>I&amp;rsquo;ve &lt;a href="https://github.com/minimaxir/twitter-cloud-run" target="_blank" rel="noopener">open-sourced an infrastructure schema on GitHub&lt;/a> that leverages &lt;a href="https://cloud.google.com" target="_blank" rel="noopener">Google Cloud Platform&lt;/a> services to run hand-curated Twitter bots using a few modern technologies to minimize cost and computation; it&amp;rsquo;s admittingly somewhat complicated, but it should give you an idea of how to best implement a Twitter bot. The repo also has instructions on how to set up a Twitter developer account.&lt;/p>
&lt;h2 id="the-ethics-of-twitter-ai-bots">The Ethics of Twitter AI Bots&lt;/h2>
&lt;p>Lastly, let&amp;rsquo;s address the elephant in the room: is building these bots &lt;em>ethical&lt;/em>? Modern AI has frequently been criticized on two fronts, both in how the input training data is obtained (e.g. obtaining faces for training facial recognition software), and how AI-generated media content is used (e.g. video deepfakes).&lt;/p>
&lt;p>&lt;strong>I am not a lawyer&lt;/strong>, but for these AI-generated tweets, this is how I see it:&lt;/p>
&lt;p>The input data is obtained from Twitter, but not through its API; it&amp;rsquo;s downloaded through external web scraping via &lt;code>twint&lt;/code>, and &lt;em>never logs into the website&lt;/em>. This kind of workflow was ruled as not an abuse by the recent &lt;a href="https://www.eff.org/deeplinks/2019/09/victory-ruling-hiq-v-linkedin-protects-scraping-public-data" target="_blank" rel="noopener">hiQ v. LinkedIn decision&lt;/a>, as the data is public. It&amp;rsquo;s still a gray area; I would not &lt;em>redistribute/commercialize the downloaded tweet data&lt;/em>; just use it as input data to the model.&lt;/p>
&lt;p>The actual generated tweets themself should be fine to use as you see fit. Whether AI-generated works infringe on the copyrights of its source material is an evolving area of both ethics and law, but at minimum these AI-generated tweets are both a transformative derivative work and a parody.&lt;/p>
&lt;p>That said, given the massive ambiguities around AI-generated content, it&amp;rsquo;s important to be completely transparent and also comply with &lt;a href="https://help.twitter.com/en/rules-and-policies/parody-account-policy" target="_blank" rel="noopener">Twitter rules on parody accounts&lt;/a>. For example, the Twitter bio for the bot should indicate:&lt;/p>
&lt;ul>
&lt;li>It&amp;rsquo;s posting AI-generated tweets, made with GPT-2.&lt;/li>
&lt;li>It&amp;rsquo;s human-curated (or not).&lt;/li>
&lt;li>The Twitter account of who maintains the bot.&lt;/li>
&lt;li>The Twitter account(s) the bot is parodying / model is finetuned upon.&lt;/li>
&lt;/ul>
&lt;p>Additionally, to avoid impersonation, the full name of the Twitter account should not be a verbatim match to the person being parodied (e.g. &amp;ldquo;&lt;em>X&lt;/em> but AI&amp;rdquo; is fine), and the profile picture should be visually distinct from the human (e.g. my bots have a black-and-white profile picture). I would also not recommend making bots of people who are more newsworthy to avoid accusations of impersonation (e.g. do not make bots of politicians, &lt;em>especially&lt;/em> &lt;a href="https://twitter.com/realDonaldTrump" target="_blank" rel="noopener">Donald Trump&lt;/a>).&lt;/p>
&lt;p>There is still a lot of work that can be done in optimizing Twitter bots, both in terms of generated tweet quality and in ironing out the ethical logistics of maintaining an AI bot account. &lt;strong>I do not believe that AI text-generating bot Twitter accounts will obsolete human Twitter accounts&lt;/strong>. It&amp;rsquo;s a different &lt;em>flavor&lt;/em> of comedy; not better, not worse. But there&amp;rsquo;s still a lot that can be done to both expand and control the creativity of these Twitter bots, and I have a few active ideas in the pipeline to implement.&lt;/p></description></item><item><title>Visualizing Airline Flight Characteristics Between SFO and JFK</title><link>https://minimaxir.com/2019/10/sfo-jfk-flights/</link><pubDate>Wed, 23 Oct 2019 09:00:00 -0700</pubDate><guid>https://minimaxir.com/2019/10/sfo-jfk-flights/</guid><description>&lt;p>In March, &lt;a href="https://cloud.google.com" target="_blank" rel="noopener">Google Compute Platform&lt;/a> developer advocate &lt;a href="https://twitter.com/felipehoffa" target="_blank" rel="noopener">Felipe Hoffa&lt;/a> made a tweet about airline flight data from San Francisco International Airport (SFO) to Seattle-Tacoma International Airport (SEA):&lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">The time to fly from San Francisco to Seattle (SFO-&amp;gt;SEA) keeps getting longer throughout the years - with a huge increase in how much time the airplane spends taxiing around SeaTac .&lt;br>&lt;br>Playing with US flights data in BigQuery: &lt;a href="https://t.co/eD9unaokWx">https://t.co/eD9unaokWx&lt;/a> &lt;a href="https://t.co/3vfnBhiJv4">pic.twitter.com/3vfnBhiJv4&lt;/a>&lt;/p>&amp;mdash; Felipe Hoffa (@felipehoffa) &lt;a href="https://twitter.com/felipehoffa/status/1111050585120206848?ref_src=twsrc%5Etfw">March 27, 2019&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;p>Particularly, his visualization of total elapsed times by airline caught my eye.&lt;/p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://pbs.twimg.com/media/D2s9oFtX4AEK6nD?format=jpg" alt="" loading="lazy" data-zoomable class=" img-light" />&lt;/div>
&lt;/div>&lt;/figure>
&lt;p>The overall time for flights from SFO to SEA goes up drastically starting in 2015, and this increase occurs across multiple airlines, implying that it&amp;rsquo;s not an airline-specific problem. But what could intuitively cause that?&lt;/p>
&lt;p>U.S. domestic airline data is &lt;a href="https://www.transtats.bts.gov/Tables.asp?DB_ID=120" target="_blank" rel="noopener">freely distributed&lt;/a> by the United States Department of Transportation. Normally it&amp;rsquo;s a pain to work with as it&amp;rsquo;s very large with millions of rows, but BigQuery makes playing with such data relatively easy, fun, and free. What other interesting factoids can be found?&lt;/p>
&lt;h2 id="expanding-on-sfo--sea">Expanding on SFO â†’ SEA&lt;/h2>
&lt;p>&lt;a href="https://cloud.google.com/bigquery/" target="_blank" rel="noopener">BigQuery&lt;/a> is a big data warehousing tool that allows you to query massive amounts of data. The table Hoffa created from the airline data (&lt;code>fh-bigquery.flights.ontime_201903&lt;/code>) is 83.37 GB and 184 &lt;em>million&lt;/em> rows. You can query 1 TB of data from it for free, but since BQ will only query against the fields you request, the queries in this post only consume about 2 GB each, allowing you to run them well within that quota.&lt;/p>
&lt;p>Hoffa&amp;rsquo;s query that runs on BigQuery looks like this:&lt;/p>
&lt;pre>&lt;code class="language-sql">SELECT FlightDate_year, Reporting_Airline
, AVG(ActualElapsedTime) ActualElapsedTime
, AVG(TaxiOut) TaxiOut
, AVG(TaxiIn) TaxiIn
, AVG(AirTime) AirTime
, COUNT(*) c
FROM `fh-bigquery.flights.ontime_201903`
WHERE Origin = 'SFO'
AND Dest = 'SEA'
AND FlightDate_year &amp;gt; '2010-01-01'
GROUP BY 1,2
ORDER BY 1 DESC, 3 DESC
LIMIT 1000
&lt;/code>&lt;/pre>
&lt;p>For each year and airline after 2010, the query calculates the average metrics specified for flights on the SFO â†’ SEA route.&lt;/p>
&lt;p>I made a few query and data visualization tweaks to what Hoffa did above, and here&amp;rsquo;s the result showing the increase in elapsed airline flight time, over time for that route:&lt;/p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/2019/10/sfo-jfk-flights/sfo_sea_flight_duration_hu090b87f15f76e9eabbcabb682a970e16_159215_b038daccbd52d939efe1f3e222aeb5ac.png 400w,
/2019/10/sfo-jfk-flights/sfo_sea_flight_duration_hu090b87f15f76e9eabbcabb682a970e16_159215_b8479dd25e57d54aca0db995978fb766.png 760w,
/2019/10/sfo-jfk-flights/sfo_sea_flight_duration_hu090b87f15f76e9eabbcabb682a970e16_159215_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2019/10/sfo-jfk-flights/sfo_sea_flight_duration_hu090b87f15f76e9eabbcabb682a970e16_159215_b038daccbd52d939efe1f3e222aeb5ac.png"
width="760"
height="507"
loading="lazy" data-zoomable class=" img-light" />&lt;/div>
&lt;/div>&lt;/figure>
&lt;p>Let&amp;rsquo;s explain what&amp;rsquo;s going on here.&lt;/p>
&lt;p>A common trend in statistics is avoiding using &lt;a href="https://en.wikipedia.org/wiki/Average" target="_blank" rel="noopener">averages&lt;/a> as a summary statistic whenever possible, as averages can be overly affected by strong outliers (and with airline flights, there are definitely strong outliers!). The solution is to use a &lt;a href="https://en.wikipedia.org/wiki/Median" target="_blank" rel="noopener">median&lt;/a> instead, but one problem: medians are hard and &lt;a href="https://www.periscopedata.com/blog/medians-in-sql" target="_blank" rel="noopener">computationally complex&lt;/a> to calculate compared to simple averages. Despite the rise of &amp;ldquo;big data&amp;rdquo;, most databases and BI tools don&amp;rsquo;t have a &lt;code>MEDIAN&lt;/code> function that&amp;rsquo;s as easy to use as an &lt;code>AVG&lt;/code> function. But BigQuery has an uncommon &lt;a href="https://cloud.google.com/bigquery/docs/reference/standard-sql/approximate_aggregate_functions#approx_quantiles" target="_blank" rel="noopener">APPROX_QUANTILES&lt;/a> function, which calculates the specified amount of quantiles; for example, if you call &lt;code>APPROX_QUANTILES(ActualElapsedTime, 100)&lt;/code>, it will return an array with the 100 quantiles, where the median will be the 50th quantile. BigQuery &lt;a href="https://cloud.google.com/bigquery/docs/reference/standard-sql/approximate-aggregation" target="_blank" rel="noopener">uses&lt;/a> an algorithmic trick called &lt;a href="https://en.wikipedia.org/wiki/HyperLogLog" target="_blank" rel="noopener">HyperLogLog++&lt;/a> to calculate these quantiles efficiently even with millions of data points. But since we get other quantiles like the 5th, 25th, 75th, and 95th quantiles for free with that approach, we can visualize the &lt;em>spread&lt;/em> of the data.&lt;/p>
&lt;p>We can aggregate the data by month for more granular trends and calculate the &lt;code>APPROX_QUANTILES&lt;/code> in a subquery so it only has to be computed once. Hoffa also uploaded a more recent table (&lt;code>fh-bigquery.flights.ontime_201908&lt;/code>) with a few additional months of data. To make things more simple, we&amp;rsquo;ll ignore aggregating by airlines since the metrics do not vary strongly between them. The final query ends up looking like this:&lt;/p>
&lt;pre>&lt;code class="language-sql">#standardSQL
SELECT Year, Month, num_flights,
time_q[OFFSET(5)] AS q_5,
time_q[OFFSET(25)] AS q_25,
time_q[OFFSET(50)] AS q_50,
time_q[OFFSET(75)] AS q_75,
time_q[OFFSET(95)] AS q_95
FROM (
SELECT Year, Month,
COUNT(*) as num_flights,
APPROX_QUANTILES(ActualElapsedTime, 100) AS time_q
FROM `fh-bigquery.flights.ontime_201908`
WHERE Origin = 'SFO'
AND Dest = 'SEA'
AND FlightDate_year &amp;gt; '2010-01-01'
GROUP BY Year, Month
)
ORDER BY Year, Month
&lt;/code>&lt;/pre>
&lt;p>The resulting data table:&lt;/p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/2019/10/sfo-jfk-flights/table_hu8a9fe47c818e1d6106662c311f8883fd_58216_96f8a94043ca2d144f54a187f20705f9.png 400w,
/2019/10/sfo-jfk-flights/table_hu8a9fe47c818e1d6106662c311f8883fd_58216_57340382017f1358e1732c71669cb781.png 760w,
/2019/10/sfo-jfk-flights/table_hu8a9fe47c818e1d6106662c311f8883fd_58216_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2019/10/sfo-jfk-flights/table_hu8a9fe47c818e1d6106662c311f8883fd_58216_96f8a94043ca2d144f54a187f20705f9.png"
width="760"
height="329"
loading="lazy" data-zoomable class=" img-light" />&lt;/div>
&lt;/div>&lt;/figure>
&lt;p>In retrospect, since we&amp;rsquo;re only focusing on one route, it isn&amp;rsquo;t &lt;em>big&lt;/em> data (this query only returns data on 64,356 flights total), but it&amp;rsquo;s still a very useful skill if you need to analyze more of the airline data (the &lt;code>APPROX_QUANTILES&lt;/code> function can handle &lt;em>millions&lt;/em> of data points very quickly).&lt;/p>
&lt;p>As a professional data scientist, one of my favorite types of data visualization is a &lt;a href="https://en.wikipedia.org/wiki/Box_plot" target="_blank" rel="noopener">box plot&lt;/a>, as it provides a way to visualize spread without being visually intrusive. Data visualization tools like &lt;a href="https://www.r-project.org" target="_blank" rel="noopener">R&lt;/a> and &lt;a href="https://ggplot2.tidyverse.org/index.html" target="_blank" rel="noopener">ggplot2&lt;/a> make constructing them &lt;a href="https://ggplot2.tidyverse.org/reference/geom_boxplot.html" target="_blank" rel="noopener">very easy to do&lt;/a>.&lt;/p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/2019/10/sfo-jfk-flights/geom_boxplot-1_hu90ccc83085f24dd98412f8f8211a2816_19366_236d34e8aadc3e97b9590fcbf4324c8f.png 400w,
/2019/10/sfo-jfk-flights/geom_boxplot-1_hu90ccc83085f24dd98412f8f8211a2816_19366_50fcb73fb4a3167aed68e81c653605b0.png 760w,
/2019/10/sfo-jfk-flights/geom_boxplot-1_hu90ccc83085f24dd98412f8f8211a2816_19366_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2019/10/sfo-jfk-flights/geom_boxplot-1_hu90ccc83085f24dd98412f8f8211a2816_19366_236d34e8aadc3e97b9590fcbf4324c8f.png"
width="760"
height="470"
loading="lazy" data-zoomable class=" img-light" />&lt;/div>
&lt;/div>&lt;/figure>
&lt;p>By default, for each box representing a group, the thick line in the middle of the box is the median, the lower bound of the box is the 25th quantile and the upper bound is the 75th quantile. The whiskers are normally a function of the &lt;a href="https://en.wikipedia.org/wiki/Interquartile_range" target="_blank" rel="noopener">interquartile range&lt;/a> (IQR), but if there&amp;rsquo;s enough data, I prefer to use the 5th and 95th quantiles instead.&lt;/p>
&lt;p>If you feed ggplot2&amp;rsquo;s &lt;code>geom_boxplot()&lt;/code> with raw data, it will automatically calculate the corresponding metrics for visualization; however, with big data, the data may not fit into memory and as noted earlier, medians and other quantiles are computationally expensive to calculate. Because we precomputed the quantiles with the query above for every year and month, we can use those explicitly. (The minor downside is that this will not include outliers)&lt;/p>
&lt;p>Additionally for box plots, I like to fill in each box with a different color corresponding to the year in order to better perceive data &lt;a href="https://en.wikipedia.org/wiki/Seasonality" target="_blank" rel="noopener">seasonality&lt;/a>. In the case of airline flights, seasonality is more literal: weather has an intuitive impact on flight times and delays, and during winter months there are also holidays which could affect airline logistics.&lt;/p>
&lt;p>The resulting ggplot2 code looks like this:&lt;/p>
&lt;pre>&lt;code>plot &amp;lt;-
ggplot(df_tf,
aes(
x = date,
ymin = q_5,
lower = q_25,
middle = q_50,
upper = q_75,
ymax = q_95,
group = date,
fill = year_factor
)) +
geom_boxplot(stat = &amp;quot;identity&amp;quot;, size = 0.3) +
scale_fill_hue(l = 50, guide = F) +
scale_x_date(date_breaks = '1 year', date_labels = &amp;quot;%Y&amp;quot;) +
scale_y_continuous(breaks = pretty_breaks(6)) +
labs(
title = &amp;quot;Distribution of Flight Times of Flights From SFO â†’ SEA, by Month&amp;quot;,
subtitle = &amp;quot;via US DoT. Box bounds are 25th/75th percentiles, whiskers are 5th/95th percentiles.&amp;quot;,
y = 'Total Elapsed Flight Time (Minutes)',
fill = '',
caption = 'Max Woolf â€” minimaxir.com'
) +
theme(axis.title.x = element_blank())
ggsave('sfo_sea_flight_duration.png',
plot,
width = 6,
height = 4)
&lt;/code>&lt;/pre>
&lt;p>And behold (again)!&lt;/p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/2019/10/sfo-jfk-flights/sfo_sea_flight_duration_hu090b87f15f76e9eabbcabb682a970e16_159215_b038daccbd52d939efe1f3e222aeb5ac.png 400w,
/2019/10/sfo-jfk-flights/sfo_sea_flight_duration_hu090b87f15f76e9eabbcabb682a970e16_159215_b8479dd25e57d54aca0db995978fb766.png 760w,
/2019/10/sfo-jfk-flights/sfo_sea_flight_duration_hu090b87f15f76e9eabbcabb682a970e16_159215_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2019/10/sfo-jfk-flights/sfo_sea_flight_duration_hu090b87f15f76e9eabbcabb682a970e16_159215_b038daccbd52d939efe1f3e222aeb5ac.png"
width="760"
height="507"
loading="lazy" data-zoomable class=" img-light" />&lt;/div>
&lt;/div>&lt;/figure>
&lt;p>You can see that the boxes do indeed trend upward after 2016, although per-month medians are in flux. The spread is also increasingly slowly over time. But what&amp;rsquo;s interesting is the seasonality; pre-2016, the summer months (the &amp;ldquo;middle&amp;rdquo; of a given color) have a &lt;em>very&lt;/em> significant drop in total time, which doesn&amp;rsquo;t occur as strongly after 2016. Hmm.&lt;/p>
&lt;h2 id="sfo-and-jfk">SFO and JFK&lt;/h2>
&lt;p>Since I occasionally fly from San Francisco to New York City, it might be interesting (for completely selfish reasons) to track trends over time for flights between those areas. On the San Francisco side I choose SFO, and for the New York side I choose John F. Kennedy International Airport (JFK), as the data goes back very far for those routes specifically, and I only want to look at a single airport at a time (instead of including other NYC airports such as Newark Liberty International Airport [EWR] and LaGuardia Airport [LGA]) to limit potential data confounders.&lt;/p>
&lt;p>Fortunately, the code and query changes are minimal: in the query, change the target metric to whatever metric you want, and the &lt;code>Origin&lt;/code> and &lt;code>Dest&lt;/code> in the &lt;code>WHERE&lt;/code> clause to what you want, and if you want to calculate metrics other than elapsed time, change the metric in &lt;code>APPROX_QUANTILES&lt;/code> accordingly.&lt;/p>
&lt;p>Here&amp;rsquo;s the chart of total elapsed time from SFO â†’ JFK:&lt;/p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/2019/10/sfo-jfk-flights/sfo_jfk_flight_duration_huae05f43c7515e0cfbb14bf79f35a2678_157723_cb86e97f038ae5c0b86e44382cd1f7c3.png 400w,
/2019/10/sfo-jfk-flights/sfo_jfk_flight_duration_huae05f43c7515e0cfbb14bf79f35a2678_157723_85cbc2899894b2cec9f447c0049272e1.png 760w,
/2019/10/sfo-jfk-flights/sfo_jfk_flight_duration_huae05f43c7515e0cfbb14bf79f35a2678_157723_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2019/10/sfo-jfk-flights/sfo_jfk_flight_duration_huae05f43c7515e0cfbb14bf79f35a2678_157723_cb86e97f038ae5c0b86e44382cd1f7c3.png"
width="760"
height="507"
loading="lazy" data-zoomable class=" img-light" />&lt;/div>
&lt;/div>&lt;/figure>
&lt;p>And here&amp;rsquo;s the reverse, from JFK â†’ SFO:&lt;/p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/2019/10/sfo-jfk-flights/jfk_sfo_flight_duration_hu7a42a1160099b79c2eb33bbb4b636fb5_160041_2b290cf502dc6fe66e94800283e3cce4.png 400w,
/2019/10/sfo-jfk-flights/jfk_sfo_flight_duration_hu7a42a1160099b79c2eb33bbb4b636fb5_160041_7a7b409852cb5094a031aada2d090cf4.png 760w,
/2019/10/sfo-jfk-flights/jfk_sfo_flight_duration_hu7a42a1160099b79c2eb33bbb4b636fb5_160041_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2019/10/sfo-jfk-flights/jfk_sfo_flight_duration_hu7a42a1160099b79c2eb33bbb4b636fb5_160041_2b290cf502dc6fe66e94800283e3cce4.png"
width="760"
height="507"
loading="lazy" data-zoomable class=" img-light" />&lt;/div>
&lt;/div>&lt;/figure>
&lt;p>Unlike the SFO â†’ SEA charts, both charts are relatively flat over the years. However, when looking at seasonality, SFO â†’ JFK dips in the summer and spikes during winter, while JFK â†’ SFO &lt;em>does the complete opposite&lt;/em>: dips during the winter and spikes during the summer, which is similar to the SFO â†’ SEA route. I don&amp;rsquo;t have any guesses what would cause that behavior.&lt;/p>
&lt;p>How about flight speed (calculated via air time divided by distance)? Have new advances in airline technology made planes faster and/or more efficient?&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/2019/10/sfo-jfk-flights/sfo_jfk_flight_speed_huc3cf1ee234ce33f67f0fcf3628ff7e23_170566_3041accef38e51bfcf3e0ba1ad7b4777.png 400w,
/2019/10/sfo-jfk-flights/sfo_jfk_flight_speed_huc3cf1ee234ce33f67f0fcf3628ff7e23_170566_a4b33b53ed66104dfc2540e1b10ad9b5.png 760w,
/2019/10/sfo-jfk-flights/sfo_jfk_flight_speed_huc3cf1ee234ce33f67f0fcf3628ff7e23_170566_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2019/10/sfo-jfk-flights/sfo_jfk_flight_speed_huc3cf1ee234ce33f67f0fcf3628ff7e23_170566_3041accef38e51bfcf3e0ba1ad7b4777.png"
width="760"
height="507"
loading="lazy" data-zoomable class=" img-light" />&lt;/div>
&lt;/div>&lt;/figure>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/2019/10/sfo-jfk-flights/jfk_sfo_flight_speed_hu3d573edc28a0ca8a3a10f4a4145f8029_167609_bfdfc35f6c6ea465165fe7a2a0c40ac8.png 400w,
/2019/10/sfo-jfk-flights/jfk_sfo_flight_speed_hu3d573edc28a0ca8a3a10f4a4145f8029_167609_ab8b0fb3c81d23a78267f5796e5cdf4c.png 760w,
/2019/10/sfo-jfk-flights/jfk_sfo_flight_speed_hu3d573edc28a0ca8a3a10f4a4145f8029_167609_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2019/10/sfo-jfk-flights/jfk_sfo_flight_speed_hu3d573edc28a0ca8a3a10f4a4145f8029_167609_bfdfc35f6c6ea465165fe7a2a0c40ac8.png"
width="760"
height="507"
loading="lazy" data-zoomable class=" img-light" />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>The expected flight speed for a commercial airplane, &lt;a href="https://en.wikipedia.org/wiki/Cruise_%28aeronautics%29" target="_blank" rel="noopener">per Wikipedia&lt;/a>, is 547-575 mph, so the metrics from SFO pass the sanity check. The metrics from JFK indicate there&amp;rsquo;s about a 20% drop in flight speed potentially due to wind resistance, which makes sense. Month-to-month, the speed trends are inverse to the total elapsed time, which makes sense intuitively as they are strongly negatively correlated.&lt;/p>
&lt;p>Lastly, what about flight departure delays? Are airlines becoming more efficient, or has increased demand caused more congestion?&lt;/p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/2019/10/sfo-jfk-flights/sfo_jfk_departure_delay_hu6c94336de918cdf0e8055eee4ae1a7e3_141837_4abb53a19dc344f678c319aa767b4ab9.png 400w,
/2019/10/sfo-jfk-flights/sfo_jfk_departure_delay_hu6c94336de918cdf0e8055eee4ae1a7e3_141837_72878b9493a1e3ac612a6a2a40dfa542.png 760w,
/2019/10/sfo-jfk-flights/sfo_jfk_departure_delay_hu6c94336de918cdf0e8055eee4ae1a7e3_141837_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2019/10/sfo-jfk-flights/sfo_jfk_departure_delay_hu6c94336de918cdf0e8055eee4ae1a7e3_141837_4abb53a19dc344f678c319aa767b4ab9.png"
width="760"
height="507"
loading="lazy" data-zoomable class=" img-light" />&lt;/div>
&lt;/div>&lt;/figure>
&lt;p>Wait a second. In this case, massive 2-3 hour flight delays are frequent enough that even just the 95% percentile skews the entire plot. Let&amp;rsquo;s remove the whiskers in order to look at trends more clearly.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/2019/10/sfo-jfk-flights/sfo_jfk_departure_delay_nowhiskers_huc7ec54c5593e7ff3297656de8f46d62e_133265_8e4b3fbc911f41e6783585aa23e9d375.png 400w,
/2019/10/sfo-jfk-flights/sfo_jfk_departure_delay_nowhiskers_huc7ec54c5593e7ff3297656de8f46d62e_133265_1cf904143e301a4494baf491823705a1.png 760w,
/2019/10/sfo-jfk-flights/sfo_jfk_departure_delay_nowhiskers_huc7ec54c5593e7ff3297656de8f46d62e_133265_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2019/10/sfo-jfk-flights/sfo_jfk_departure_delay_nowhiskers_huc7ec54c5593e7ff3297656de8f46d62e_133265_8e4b3fbc911f41e6783585aa23e9d375.png"
width="760"
height="507"
loading="lazy" data-zoomable class=" img-light" />&lt;/div>
&lt;/div>&lt;/figure>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/2019/10/sfo-jfk-flights/jfk_sfo_departure_delay_nowhiskers_hu6b162e6cfa7ddf0cfbda0fcff920b973_133187_271d72826627553777e5f15a1524498a.png 400w,
/2019/10/sfo-jfk-flights/jfk_sfo_departure_delay_nowhiskers_hu6b162e6cfa7ddf0cfbda0fcff920b973_133187_80c25736a45f60646b80f64831b05900.png 760w,
/2019/10/sfo-jfk-flights/jfk_sfo_departure_delay_nowhiskers_hu6b162e6cfa7ddf0cfbda0fcff920b973_133187_1200x1200_fit_gaussian_3.png 1200w"
src="https://minimaxir.com/2019/10/sfo-jfk-flights/jfk_sfo_departure_delay_nowhiskers_hu6b162e6cfa7ddf0cfbda0fcff920b973_133187_271d72826627553777e5f15a1524498a.png"
width="760"
height="507"
loading="lazy" data-zoomable class=" img-light" />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>A negative delay implies the flight leaves early, so we can conclude on average, flights leave slightly earlier than the stated departure time. Even without the whiskers, we can see major spikes at the 75th percentile level for summer months, and said spikes were especially bad in 2017 for both airports.&lt;/p>
&lt;p>These box plots are only an &lt;a href="https://en.wikipedia.org/wiki/Exploratory_data_analysis" target="_blank" rel="noopener">exploratory data analysis&lt;/a>. Determining the &lt;em>cause&lt;/em> of changes in these flight metrics is difficult even for experts (I am definitely not an expert!) and many not even be possible to determine from publicly-available data.&lt;/p>
&lt;p>But there are still other fun things that can be done with the airline flight data, such as faceting airline trends by time and the inclusion of other airports, which is &lt;a href="https://twitter.com/minimaxir/status/1115261670153048065" target="_blank" rel="noopener">&lt;em>interesting&lt;/em>&lt;/a>.&lt;/p>
&lt;hr>
&lt;p>&lt;em>You can view the BigQuery queries used to get the data, plus the R and ggplot2 used to create the data visualizations, in &lt;a href="http://minimaxir.com/notebooks/sfo-jfk-flights/" target="_blank" rel="noopener">this R Notebook&lt;/a>. You can also view the images/code used for this post in &lt;a href="https://github.com/minimaxir/sfo-jfk-flights" target="_blank" rel="noopener">this GitHub repository&lt;/a>&lt;/em>.&lt;/p>
&lt;p>&lt;em>You are free to use the data visualizations from this article however you wish, but it would be greatly appreciated if proper attribution is given to this article and/or myself!&lt;/em>&lt;/p></description></item><item><title>Experiments with Making Convincing AI-Generated Fake News</title><link>https://minimaxir.com/2019/09/ctrl-fake-news/</link><pubDate>Mon, 30 Sep 2019 08:00:00 -0700</pubDate><guid>https://minimaxir.com/2019/09/ctrl-fake-news/</guid><description>&lt;p>&lt;span>&lt;style>
blockquote {
padding-right: 1.25em !important;
}
&lt;/style>&lt;/span>&lt;/p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt=""
src="https://minimaxir.com/2019/09/ctrl-fake-news/ctrl_demo_ani.gif"
loading="lazy" data-zoomable class=" img-light" />&lt;/div>
&lt;/div>&lt;/figure>
&lt;p>When &lt;a href="https://openai.com" target="_blank" rel="noopener">OpenAI&lt;/a> announced &lt;a href="https://openai.com/blog/better-language-models/" target="_blank" rel="noopener">GPT-2&lt;/a>, a robust text-generating AI model, they explicitly only released smaller, less robust versions of the model out of fear that the large model could be used to generate fake news. However, since OpenAI described most of the technical decisions needed to create the model &lt;a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank" rel="noopener">in the corresponding paper&lt;/a>, it would be possible for others to create their own text generating Transformer models, and maybe even &lt;em>improve&lt;/em> on GPT-2 (with a sufficient budget!).&lt;/p>
&lt;p>In September, the &lt;a href="https://www.salesforce.com" target="_blank" rel="noopener">Salesforce&lt;/a> AI team released &lt;a href="https://github.com/salesforce/ctrl" target="_blank" rel="noopener">CTRL&lt;/a>, a Transformer-based text generating model with a twist; the model can generate text from specified domains by passing &lt;strong>control codes&lt;/strong> to the model. What caught my interest was a demo of domain style transfer in the &lt;a href="https://arxiv.org/abs/1909.05858" target="_blank" rel="noopener">CTRL paper&lt;/a>:&lt;/p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/2019/09/ctrl-fake-news/ctrl_paper_hua4ab223fa91cada5d17e193c13f46748_262131_530a0c46db54c02e1f98055f5da258c6.jpg 400w,
/2019/09/ctrl-fake-news/ctrl_paper_hua4ab223fa91cada5d17e193c13f46748_262131_d2d266be1aef9baab05ab7d1fdccd95f.jpg 760w,
/2019/09/ctrl-fake-news/ctrl_paper_hua4ab223fa91cada5d17e193c13f46748_262131_1200x1200_fit_q75_gaussian.jpg 1200w"
src="https://minimaxir.com/2019/09/ctrl-fake-news/ctrl_paper_hua4ab223fa91cada5d17e193c13f46748_262131_530a0c46db54c02e1f98055f5da258c6.jpg"
width="524"
height="760"
loading="lazy" data-zoomable class=" img-light" />&lt;/div>
&lt;/div>&lt;/figure>
&lt;p>If the model is that robust to minor URL changes, what happens when you give it URLs that blatantly do not exist? Can the CTRL model create the &amp;ldquo;fake news&amp;rdquo; OpenAI was concerned about? Let&amp;rsquo;s put it to the test.&lt;/p>
&lt;h2 id="an-overview-of-ctrl">An Overview of CTRL&lt;/h2>
&lt;p>I&amp;rsquo;ve &lt;a href="https://github.com/minimaxir/ctrl-gce" target="_blank" rel="noopener">written a guide + scripts&lt;/a> to setting the base CTRL model as cheaply as possible on Google Compute Engine with just a few commands. Additionally, the CTRL team has released a &lt;a href="https://colab.research.google.com/drive/1hVveBQShDru1Mjnhe4C21uQv4A2eH1tV" target="_blank" rel="noopener">free Colaboratory Notebook&lt;/a> which sets up and runs the CTRL model; however, the model is &lt;em>so large&lt;/em> it won&amp;rsquo;t fit into the memory of traditional GPUs, so the notebook does a trick to shrink it a bit which may impact generation performance.&lt;/p>
&lt;p>Like GPT-2, CTRL has a &lt;a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" target="_blank" rel="noopener">Transformer&lt;/a> architecture based on &lt;a href="https://www.tensorflow.org" target="_blank" rel="noopener">TensorFlow&lt;/a> and uses &lt;a href="https://en.wikipedia.org/wiki/Byte_pair_encoding" target="_blank" rel="noopener">byte pair encodings&lt;/a> as its inputs and outputs, which are then decoded into readable text. CTRL has notable performance improvements as it&amp;rsquo;s trained on &lt;em>three times as much data as GPT-2&lt;/em>, including an &lt;a href="https://github.com/jcpeterson/openwebtext" target="_blank" rel="noopener">open-sourced clone&lt;/a> of GPT-2&amp;rsquo;s original dataset. And of course, it&amp;rsquo;s larger (1.6B hyperparameters) compared to the currently public GPT-2 (774M hyperparameters), which has significant effects on text quality.&lt;/p>
&lt;p>Most importantly, CTRL &lt;em>requires&lt;/em> a control code if you want to generate text, which allows for more deterministic output compared to GPT-2/&lt;a href="https://talktotransformer.com" target="_blank" rel="noopener">TalkToTransformer&lt;/a>. There are several fun control codes, such as &lt;code>Questions&lt;/code> if you want to ask the AI a question, or &lt;code>Reviews&lt;/code> if you want the AI to generate an &lt;a href="https://www.amazon.com" target="_blank" rel="noopener">Amazon&lt;/a> review. For this, we&amp;rsquo;ll only look at the &lt;code>Links&lt;/code> control code, which lets you provide a URL and/or a prompt for text generation.&lt;/p>
&lt;p>As the example from the paper shows, URLs contain a surprising amount of metadata. For example, let&amp;rsquo;s consider a &lt;a href="https://www.washingtonpost.com/powerpost/deal-reached-for-whistleblowers-testimony-house-intelligence-chairman-says/2019/09/29/01cade60-e2d1-11e9-b403-f738899982d2_story.html" target="_blank" rel="noopener">random Washington Post URL&lt;/a>: &lt;code>https://www.washingtonpost.com/powerpost/deal-reached-for-whistleblowers-testimony-house-intelligence-chairman-says/2019/09/29/01cade60-e2d1-11e9-b403-f738899982d2_story.html&lt;/code>&lt;/p>
&lt;p>There&amp;rsquo;s month/day/year information (&lt;code>2019/09/29&lt;/code>), a category (&lt;code>powerpost&lt;/code>), and a &lt;a href="https://en.wikipedia.org/wiki/Clean_URL#Slug" target="_blank" rel="noopener">URL slug&lt;/a> (&lt;code>deal-reached-for-whistleblowers-testimony-house-intelligence-chairman-says&lt;/code>), typically present for SEO reasons, but in this case provides strong hints to the underlying content. The Transformer architecture is &lt;em>surprisingly&lt;/em> effective at extracting all this metadata, and using it to generate appropriate text.&lt;/p>
&lt;h2 id="examples-of-fake-news">Examples of Fake News&lt;/h2>
&lt;p>All of the following examples are generated using the full model with the default settings: 256 token length and a temperature of 0 (i.e. the model always selects the next token with the highest probability). I manually truncate the output once it finishes, but other than that, &lt;strong>these AI-generated articles are not edited&lt;/strong>.&lt;/p>
&lt;p>Let&amp;rsquo;s address the elephant in the room: when people think of &amp;ldquo;fake news&amp;rdquo;, they think of President Donald Trump, who effectively codified the modern usage of the term. So let&amp;rsquo;s generate a fake news story about him with an innocuous topic rarely associated with him. For example, the video gaming phenomenon &lt;a href="https://www.epicgames.com/fortnite/home" target="_blank" rel="noopener">Fortnite&lt;/a>. All that&amp;rsquo;s needed is to tweak the slug (and optionally the date) in the earlier WaPo URL a bit and prepend the corresponding control code, resulting in &lt;code>Links https://www.washingtonpost.com/powerpost/trump-likes-fortnite/2019/09/24/&lt;/code>. Feed it to the model, and behold!&lt;/p>
&lt;blockquote>
&lt;p>Trump likes Fortnite&lt;/p>
&lt;p>President Trump, who has been known to play video games for hours at a time, is now playing the popular game â€œFortniteâ€ on his phone.&lt;/p>
&lt;p>The president was spotted watching the online battle royale game, which pits players against each other in an attempt to kill their opponents, while he sat down with House and Senate leaders Tuesday afternoon.&lt;/p>
&lt;p>â€œI like that itâ€™s not just about shooting people but also having fun doing it,â€ said Trump.&lt;/p>
&lt;p>He then asked if anyone had played it before.&lt;/p>
&lt;p>â€œYou know what, I donâ€™t think Iâ€™ve ever seen that,â€ replied Rep. Steve Scalise, R-La.&lt;/p>
&lt;p>A White House official told reporters later Wednesday afternoon that Trump did indeed watch the game.&lt;/p>
&lt;p>â€œHe watched it, yeah,â€ the official said.&lt;/p>
&lt;/blockquote>
&lt;p>The generated article is very realistic and follows the style of a typical WaPo article (lede, introduction, body, support, etc.), with proper descriptions of Trump and Fortnite, plus a story that matches the headline. Like GPT-2, there are redundancies in the generation (at the end), but it&amp;rsquo;s not something most readers would find suspicious in a casual read.&lt;/p>
&lt;p>Let&amp;rsquo;s tweak things a little. Instead of using &lt;code>likes&lt;/code> in the slug, what if we use &lt;code>despises&lt;/code>, so &lt;code>trump-despises-fortnite&lt;/code>. How well does the model adapt?&lt;/p>
&lt;blockquote>
&lt;p>Trump despises Fortnite&lt;/p>
&lt;p>The president of the United States has a new favorite video game: â€œFortnite.â€&lt;/p>
&lt;p>President Trump, who is known for his love of fast food and beer, was reportedly so excited about playing this game that he called it â€œthe greatest thing in the world.â€&lt;/p>
&lt;p>But according to an Axios report, Trumpâ€™s enthusiasm waned after learning how much time players spend on it.&lt;/p>
&lt;p>â€œHe said itâ€™s not even close to being as good as â€˜Candy Crush,â€™ â€ one source told the news outlet.&lt;/p>
&lt;p>In fact, the report said, Trump had been watching videos online while eating dinner with Japanese Prime Minister Shinzo Abe at Mar-a-Lago.&lt;/p>
&lt;p>According to Axios, Trump also complained about the gameâ€™s difficulty level â€” which can be adjusted by spending real money or virtual currency â€” saying it wasnâ€™t challenging enough.&lt;/p>
&lt;p>â€œI donâ€™t know why they made it hard,â€ Trump said.&lt;/p>
&lt;p>He then went off topic when asked if heâ€™d play again.&lt;/p>
&lt;p>â€œNo, I wonâ€™t.â€&lt;/p>
&lt;p>A White House official later clarified that Trump didnâ€™t say he wouldnâ€™t play, but rather that heâ€™s â€œnot into it.â€&lt;/p>
&lt;/blockquote>
&lt;p>It adapts very well to the change in tone (even with a rhetorical lede!).&lt;/p>
&lt;p>Fake news doesn&amp;rsquo;t have to be politically-based. Such news in the financial sector could cause just as much chaos. What if it was reported that &lt;a href="https://www.apple.com" target="_blank" rel="noopener">Apple&lt;/a>, one of the largest companies in the world, went bankrupt? Put in &lt;code>Links https://www.theverge.com/2019/2/24/20882491/apple-has-filed-for-bankruptcy&lt;/code>:&lt;/p>
&lt;blockquote>
&lt;p>Apple has filed for bankruptcy.&lt;/p>
&lt;p>The company that made the iPhone and iPad is filing for bankruptcy, according to a report from Bloomberg.&lt;/p>
&lt;p>According to Bloomberg News, Apple&amp;rsquo;s board of directors voted unanimously on Friday morning in favor of an emergency plan designed to keep it afloat until its next quarterly earnings report.&lt;/p>
&lt;p>In order to avoid paying more than $128 million in back taxes, Apple will have to sell off assets like its Beats headphone business, which was valued at about $1 billion when it went public last year.&lt;/p>
&lt;p>&amp;ldquo;Apple said today that it plans to file for Chapter 11 protection under US Bankruptcy Court in Delaware this week,&amp;rdquo; Bloomberg reports.&lt;/p>
&lt;p>A source familiar with the matter told Reuters earlier this week that Apple had been considering selling some or all of its music streaming service iTunes Music Store as part of a restructuring plan.&lt;/p>
&lt;p>But the news comes just days after reports surfaced suggesting that Apple would be forced into liquidation.&lt;/p>
&lt;p>Last week, Bloomberg reported that Apple could soon announce layoffs if sales don&amp;rsquo;t pick up soon.&lt;/p>
&lt;p>&amp;ldquo;We are working hard every day to deliver great products and services to our customers around the world,&amp;rdquo; Tim Cook, CEO of Apple, wrote in a letter to employees today.&lt;/p>
&lt;/blockquote>
&lt;p>Again, the model incorporates both bankruptcy terminology and Apple-based entities. There are minor factual errors (Beats is not public, Apple is not incorporated in Delaware), but casual readers would likely scan over that.&lt;/p>
&lt;p>Science is also a field with lots of fake discoveries. What if AI reported that &lt;a href="https://en.wikipedia.org/wiki/P_versus_NP_problem" target="_blank" rel="noopener">P versus NP&lt;/a>, a very famous problem in computer science, was finally solved? Let&amp;rsquo;s try &lt;code>Links https://techcrunch.com/2029/09/24/mit-proves-p-equals-np&lt;/code>:&lt;/p>
&lt;blockquote>
&lt;p>MIT proves P equals NP&lt;/p>
&lt;p>The University of Massachusetts Amherst has proven that the problem posed by factoring large numbers is NP-complete.&lt;/p>
&lt;p>In a paper published in Nature, researchers at MITâ€™s Computer Science and Artificial Intelligence Laboratory (CSAIL) have shown how to factor an integer using only two numbers, one for each digit â€” which means it takes just three operations on your brain.&lt;/p>
&lt;p>This isnâ€™t quite as impressive as solving the problem, but itâ€™s still pretty impressive.&lt;/p>
&lt;p>â€œGiven any number n, we can find its product n times with polynomial time if there are no factors involved,â€ said lead author David DeWitt, who worked on this project while he was a graduate student there.&lt;/p>
&lt;p>But what about factorizing larger numbers?&lt;/p>
&lt;p>â€œWe showed you could do it, but not without some extra effort,â€ DeWitt said.&lt;/p>
&lt;p>To prove this, they used a technique called Monte Carlo integration, where they simulated millions or billions of different inputs from thousands of computers running various versions of Java.&lt;/p>
&lt;p>They then ran their algorithm through all possible combinations of input values until it found the best one.&lt;/p>
&lt;p>â€œThe result shows that even though our system doesnâ€™t solve every single case, it does get close enough to be useful.â€&lt;/p>
&lt;/blockquote>
&lt;p>This instance is less factually correct (and it&amp;rsquo;s clickbaity!), but the topic behind the generated text is &lt;em>complicated&lt;/em> enough that only domain experts would be able to tell it&amp;rsquo;s not internally consistent.&lt;/p>
&lt;p>It&amp;rsquo;s also worth pointing out now that &lt;em>the majority of the text generated so far is unique&lt;/em>, and CTRL is not cheating by just outputting text from the source datasets. Even though Monte Carlo integration is &lt;a href="https://en.wikipedia.org/wiki/Monte_Carlo_integration" target="_blank" rel="noopener">indeed a real thing&lt;/a> that&amp;rsquo;s relatively obscure, Googling that description of it fails to turn up results.&lt;/p>
&lt;p>Lastly, fake news doesn&amp;rsquo;t have to be depressing or sow discord. What about &lt;em>wholesome&lt;/em> fake news? Such as&amp;hellip;the return of beloved television series &lt;a href="https://en.wikipedia.org/wiki/Firefly_%28TV_series%29" target="_blank" rel="noopener">Firefly&lt;/a>? One &lt;code>Links https://www.hollywoodreporter.com/live-feed/fox-announces-firefly-season-2-revival-original-cast-returning-12343245&lt;/code> later:&lt;/p>
&lt;blockquote>
&lt;p>Fox announces Firefly Season 2 revival, original cast returning&lt;/p>
&lt;p>The network has announced that the second season of Joss Whedonâ€™s cult sci-fi series will return for a new run in 2018.&lt;/p>
&lt;p>â€œFans have been asking us to bring back Firefly, and we couldnâ€™t be more thrilled about this opportunity,â€ said Chris Albrecht, president of programming at Fox. â€œWe are excited to continue our relationship with Joss, who is one of the most creative minds working today on television, as well as his amazing cast, which includes Nathan Fillion, Summer Glau, Alan Tudyk, Adam Baldwin, Sean Maher, Adrianne Palicki, Tricia Helfer, Jewel Staite, Michael Trucco, and many more.â€&lt;/p>
&lt;p>In addition to Whedon himself, executive producers include Damon Lindelof, Maurissa Tancharoen, Vince Gilligan, David Fury, Jeffrey Bell, Greg Plageman, Scott M. Gimple, Heather Kadin, Andrew Chambliss, John Rogers, and Ron Moore.&lt;/p>
&lt;p>â€œThe show was an instant hit when it debuted over 20 years ago, but its popularity only grew after the success of â€˜Serenity,â€™ so weâ€™re very pleased to welcome fans into another chapter of their lives,â€ added Feige.&lt;/p>
&lt;/blockquote>
&lt;p>That is a &lt;em>very&lt;/em> stacked cast and crew, all of which (besides the original Firefly members) have acted/worked on sci-fi television series. The only major factual errors are that Chris Albrecht was at STARZ, not Fox, and Feige, presumably Kevin Feige of Marvel Studios, is not mentioned previously in the generated article.&lt;/p>
&lt;p>I know I&amp;rsquo;ll get criticism for highlighting a potentially dangerous application of AI text generation. My perspective is that it&amp;rsquo;s important to know what such tools are &lt;em>capable&lt;/em> of doing in order to more easily recognize fake news. The real problem with fake news isn&amp;rsquo;t the text itself: it&amp;rsquo;s the &lt;em>distribution&lt;/em> of the news on social media like &lt;a href="http://www.facebook.com" target="_blank" rel="noopener">Facebook&lt;/a> and &lt;a href="https://twitter.com" target="_blank" rel="noopener">Twitter&lt;/a>, where the platforms not only &lt;em>incentivize&lt;/em> it, but also fail to sufficiently punish deliberate, repeat offenders. It&amp;rsquo;s why journalism and awareness of fake news is extremely important.&lt;/p>
&lt;p>Some might comment &amp;ldquo;these generated texts aren&amp;rsquo;t convincing at all!&amp;rdquo;, but keep in mind that&amp;rsquo;s because the headline says upfront that they&amp;rsquo;re fake. Would you be able to identify it as a fake if a respected source impulsively tweeted it?&lt;/p></description></item></channel></rss>